[3gH    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H    H[INFO] Using python from: /workspace/isaaclab/_isaac_sim/python.sh
Loading user config located at: '/isaac-sim/kit/data/Kit/Isaac-Sim/5.1/user.config.json'
[Info] [carb] Logging to file: /isaac-sim/kit/logs/Kit/Isaac-Sim/5.1/kit_20251205_082100.log
[0.037s] [ext: omni.kit.async_engine-0.0.3] startup
[0.316s] [ext: omni.metrics.core-0.0.3] startup
[0.316s] [ext: omni.client.lib-1.1.0] startup
[0.325s] [ext: omni.blobkey-1.1.2] startup
[0.325s] [ext: omni.stats-1.0.1] startup
[0.326s] [ext: omni.datastore-0.0.0] startup
[0.329s] [ext: omni.client-1.3.0] startup
[0.332s] [ext: omni.ujitso.default-1.0.0] startup
[0.333s] [ext: omni.hsscclient-1.1.2] startup
[0.335s] [ext: omni.gpu_foundation.shadercache.vulkan-1.0.0] startup
[0.338s] [ext: omni.assets.plugins-0.0.0] startup
[0.338s] [ext: omni.gpu_foundation-0.0.0] startup
[0.343s] [ext: carb.windowing.plugins-1.0.0] startup
2025-12-05T08:21:00Z [330ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2025-12-05T08:21:00Z [330ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
[0.343s] [ext: omni.kit.renderer.init-0.0.0] startup
2025-12-05T08:21:01Z [839ms] [Warning] [omni.platforminfo.plugin] failed to open the default display.  Can't verify X Server version.

|---------------------------------------------------------------------------------------------|
| Driver Version: 550.54.14     | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA RTX A5000                 | Yes: 0 |     | 24810   MB | 10de      | 0          |
|     |                                  |        |     |            | 2231      | 704db0bc.. |
|     |                                  |        |     |            | 1         |            |
|=============================================================================================|
| OS: 24.04.2 LTS (Noble Numbat) ubuntu, Version: 24.04.2, Kernel: 5.15.0-119-generic
| Processor: 12th Gen Intel(R) Core(TM) i9-12900KF
| Bare Metal Cores: 16 | Bare Metal Logical Cores: 32
| Available Cores:  24 
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 31900 | Free Memory: 28522
| Total Page/Swap (MB): 0 | Free Page/Swap: 0
|---------------------------------------------------------------------------------------------|
2025-12-05T08:21:01Z [1,131ms] [Warning] [gpu.foundation.plugin] CPU performance profile is set to powersave. This profile sets the CPU to the lowest frequency reducing performance.
[1.256s] [ext: omni.kit.pipapi-0.0.0] startup
[1.257s] [ext: omni.kit.pip_archive-0.0.0] startup
[1.258s] [ext: omni.pip.compute-1.6.3] startup
[1.258s] [ext: omni.pip.cloud-1.4.3] startup
[1.261s] [ext: omni.isaac.core_archive-3.0.0] startup
[1.261s] [ext: omni.materialx.libs-1.0.7] startup
[1.266s] [ext: omni.kit.telemetry-0.5.2] startup
[1.280s] [ext: omni.kit.loop-isaac-1.3.7] startup
[1.280s] [ext: omni.kit.test-2.0.1] startup
[1.319s] [ext: omni.usd.config-1.0.6] startup
[1.322s] [ext: omni.gpucompute.plugins-0.0.0] startup
[1.322s] [ext: omni.usd.libs-1.0.1] startup
[1.373s] [ext: omni.mdl-56.0.3] startup
[1.441s] [ext: omni.iray.libs-0.0.0] startup
[1.445s] [ext: omni.mdl.neuraylib-0.2.12] startup
[1.447s] [ext: omni.kit.usd.mdl-1.1.5] startup
[1.517s] [ext: omni.isaac.ml_archive-3.0.4] startup
[1.517s] [ext: omni.appwindow-1.1.10] startup
2025-12-05T08:21:01Z [1,505ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2025-12-05T08:21:01Z [1,505ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
[1.519s] [ext: omni.kit.renderer.core-1.1.0] startup
2025-12-05T08:21:01Z [1,510ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2025-12-05T08:21:01Z [1,510ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2025-12-05T08:21:01Z [1,514ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2025-12-05T08:21:01Z [1,514ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
[1.528s] [ext: omni.kit.renderer.capture-0.0.0] startup
[1.529s] [ext: omni.kit.renderer.imgui-2.0.5] startup
2025-12-05T08:21:01Z [1,518ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2025-12-05T08:21:01Z [1,518ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2025-12-05T08:21:01Z [1,519ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2025-12-05T08:21:01Z [1,519ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
2025-12-05T08:21:01Z [1,519ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2025-12-05T08:21:01Z [1,519ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
[1.586s] [ext: omni.ui-2.27.1] startup
[1.593s] [ext: omni.kit.mainwindow-1.0.3] startup
[1.594s] [ext: carb.audio-0.1.0] startup
[1.613s] [ext: omni.uiaudio-1.0.0] startup
[1.614s] [ext: omni.kit.uiapp-0.0.0] startup
[1.614s] [ext: omni.usd.schema.metrics.assembler-107.3.1] startup
[1.616s] [ext: omni.usd.schema.audio-0.0.0] startup
[1.619s] [ext: omni.usd.schema.physx-107.3.26] startup
[1.642s] [ext: omni.usd_resolver-1.0.0] startup
[1.644s] [ext: omni.usd.core-1.5.3] startup
[1.646s] [ext: omni.usd.schema.omni_lens_distortion-0.0.0] startup
[1.660s] [ext: omni.usd.schema.semantics-0.0.0] startup
[1.662s] [ext: isaacsim.robot.schema-3.6.0] startup
[1.667s] [ext: omni.usd.schema.geospatial-0.0.0] startup
[1.669s] [ext: omni.usd.schema.anim-0.0.0] startup
[1.682s] [ext: omni.kit.window.popup_dialog-2.0.24] startup
[1.686s] [ext: usdrt.scenegraph-7.6.1] startup
[1.712s] [ext: omni.timeline-1.0.14] startup
[1.713s] [ext: omni.resourcemonitor-107.0.1] startup
[1.714s] [ext: omni.activity.core-1.0.3] startup
[1.715s] [ext: omni.kit.widget.nucleus_connector-2.0.1] startup
[1.716s] [ext: omni.hydra.usdrt_delegate-7.5.1] startup
[1.720s] [ext: omni.kit.actions.core-1.0.0] startup
[1.722s] [ext: omni.graph.exec-0.9.6] startup
[1.722s] [ext: omni.kit.commands-1.4.10] startup
[1.724s] [ext: omni.kit.usd_undo-0.1.8] startup
[1.725s] [ext: omni.kit.exec.core-0.13.4] startup
[1.726s] [ext: omni.kit.audiodeviceenum-1.0.2] startup
[1.726s] [ext: omni.hydra.scene_delegate-0.3.4] startup
[1.729s] [ext: omni.usd-1.13.10] startup
[1.742s] [ext: omni.kit.asset_converter-5.0.17] startup
[1.749s] [ext: omni.usd.schema.omniscripting-1.0.0] startup
[1.752s] [ext: omni.usd.schema.omni_sensors-0.0.0] startup
[1.753s] [ext: omni.usd.schema.render_settings.rtx-0.0.0] startup
[1.753s] [ext: omni.usd.schema.omnigraph-1.0.0] startup
[1.757s] [ext: omni.kit.property.adapter.core-1.0.2] startup
[1.759s] [ext: omni.kit.property.adapter.usd-1.0.2] startup
[1.760s] [ext: omni.kit.notification_manager-1.0.10] startup
[1.761s] [ext: omni.kit.clipboard-1.0.5] startup
[1.761s] [ext: omni.kit.menu.core-1.1.2] startup
[1.762s] [ext: omni.kit.hotkeys.core-1.3.10] startup
[1.764s] [ext: omni.index.libs-380600.8087.0] startup
[1.764s] [ext: omni.kit.widget.context_menu-1.2.5] startup
[1.765s] [ext: omni.kit.widget.options_menu-1.1.6] startup
[1.767s] [ext: omni.kit.helper.file_utils-0.1.9] startup
[1.768s] [ext: omni.index-1.0.1] startup
[1.769s] [ext: omni.hydra.rtx.shadercache.vulkan-1.0.0] startup
[1.769s] [ext: omni.kit.widget.options_button-1.0.3] startup
[1.770s] [ext: omni.kit.widget.filebrowser-2.12.3] startup
[1.773s] [ext: omni.kit.widget.path_field-2.0.11] startup
[1.774s] [ext: omni.volume-0.5.2] startup
[1.775s] [ext: omni.ujitso.client-0.0.0] startup
[1.776s] [ext: omni.kit.widget.browser_bar-2.0.10] startup
[1.776s] [ext: omni.hydra.rtx-1.0.0] startup
2025-12-05T08:21:02Z [1,774ms] [Warning] [omni.log] Source: omni.hydra was already registered.
[1.796s] [ext: omni.ui.scene-1.11.5] startup
[1.799s] [ext: omni.kit.usd.layers-2.2.11] startup
[1.804s] [ext: omni.kit.window.filepicker-2.13.4] startup
[1.815s] [ext: omni.kit.widget.filter-1.1.4] startup
[1.815s] [ext: omni.kit.stage_template.core-1.1.22] startup
[1.816s] [ext: omni.kit.window.drop_support-1.0.5] startup
[1.816s] [ext: omni.kit.window.content_browser_registry-0.0.6] startup
[1.817s] [ext: omni.kit.menu.utils-2.0.5] startup
[1.820s] [ext: omni.kit.window.file_importer-1.1.18] startup
[1.821s] [ext: omni.kit.window.file_exporter-1.0.33] startup
[1.822s] [ext: omni.hydra.engine.stats-1.0.3] startup
[1.823s] [ext: omni.kit.window.file-2.0.5] startup
[1.826s] [ext: omni.kit.widget.searchable_combobox-1.0.6] startup
[1.826s] [ext: omni.kit.window.content_browser-3.1.3] startup
[1.834s] [ext: omni.kit.raycast.query-1.1.0] startup
[1.838s] [ext: omni.kit.context_menu-1.8.6] startup
[1.839s] [ext: omni.kit.hydra_texture-1.4.6] startup
[1.841s] [ext: omni.kit.viewport.registry-104.0.6] startup
[1.841s] [ext: omni.kit.viewport.legacy_gizmos-1.0.19] startup
[1.843s] [ext: omni.kit.material.library-2.0.7] startup
[1.846s] [ext: omni.kit.widget.searchfield-1.1.8] startup
[1.847s] [ext: omni.kit.viewport.scene_camera_model-1.0.6] startup
[1.850s] [ext: omni.kit.widget.stage-3.1.4] startup
[1.858s] [ext: omni.kit.widget.viewport-107.1.3] startup
[1.860s] [ext: omni.kit.property.adapter.fabric-1.0.3] startup
[1.861s] [ext: omni.kit.viewport.window-107.2.0] startup
[1.872s] [ext: omni.kit.widget.settings-1.2.6] startup
[1.873s] [ext: omni.kit.widget.highlight_label-1.0.3] startup
[1.874s] [ext: omni.kit.viewport.utility-1.1.2] startup
[1.874s] [ext: omni.kit.window.preferences-1.8.0] startup
[1.877s] [ext: omni.kit.window.property-1.12.1] startup
[1.879s] [ext: omni.kit.viewport.actions-107.0.2] startup
[1.883s] [ext: omni.kit.viewport.menubar.core-107.2.1] startup
[1.892s] [ext: omni.kit.property.usd-4.5.12] startup
[1.899s] [ext: omni.kvdb-107.3.26] startup
[1.900s] [ext: omni.kit.manipulator.selector-1.1.3] startup
[1.901s] [ext: omni.kit.viewport.menubar.display-107.0.3] startup
[1.902s] [ext: omni.kit.manipulator.selection-106.0.1] startup
[1.902s] [ext: omni.kit.manipulator.transform-107.0.0] startup
[1.905s] [ext: omni.localcache-107.3.26] startup
[1.906s] [ext: omni.kit.widget.toolbar-2.0.1] startup
[1.911s] [ext: omni.convexdecomposition-107.3.26] startup
[1.913s] [ext: omni.usdphysics-107.3.26] startup
[1.914s] [ext: omni.physx.foundation-107.3.26] startup
[1.915s] [ext: omni.kit.manipulator.tool.snap-1.5.13] startup
[1.918s] [ext: omni.kit.primitive.mesh-1.0.17] startup
[1.920s] [ext: omni.kit.manipulator.viewport-107.0.1] startup
[1.921s] [ext: omni.physx.cooking-107.3.26] startup
[1.923s] [ext: omni.kit.viewport.manipulator.transform-107.0.4] startup
[1.924s] [ext: omni.fabric.commands-1.1.6] startup
[1.926s] [ext: omni.physics-107.3.26] startup
[1.928s] [ext: omni.physx-107.3.26] startup
[1.934s] [ext: omni.kit.manipulator.prim.core-107.0.8] startup
[1.939s] [ext: omni.kit.numpy.common-0.1.3] startup
[1.940s] [ext: omni.physics.stageupdate-107.3.26] startup
[1.941s] [ext: omni.physics.physx-107.3.26] startup
2025-12-05T08:21:02Z [1,929ms] [Warning] [carb] Acquiring non optional plugin interface which is not listed as dependency: [omni::physx::IPhysxBenchmarks v1.0] (plugin: <default plugin>), by client: omni.physics.physx.plugin. Add it to CARB_PLUGIN_IMPL_DEPS() macro of a client.
[1.942s] [ext: omni.kit.manipulator.prim.fabric-107.0.4] startup
[1.943s] [ext: omni.kit.widget.prompt-1.0.7] startup
[1.943s] [ext: omni.kit.manipulator.prim.usd-107.0.3] startup
[1.944s] [ext: omni.isaac.dynamic_control-2.0.7] startup
2025-12-05T08:21:02Z [1,934ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.
[1.948s] [ext: omni.inspect-1.0.2] startup
[1.948s] [ext: omni.kit.widget.layers-1.8.6] startup
[1.957s] [ext: omni.kit.manipulator.prim-107.0.0] startup
[1.957s] [ext: omni.debugdraw-0.1.4] startup
[1.959s] [ext: omni.graph.core-2.184.5] startup
[1.960s] [ext: omni.usdphysics.ui-107.3.26] startup
[1.978s] [ext: omni.physx.commands-107.3.26] startup
[1.982s] [ext: isaacsim.core.deprecation_manager-0.2.7] startup
[1.982s] [ext: omni.usd.metrics.assembler-107.3.1] startup
[1.986s] [ext: omni.physx.ui-107.3.26] startup
[2.008s] [ext: isaacsim.core.version-2.0.6] startup
[2.008s] [ext: omni.warp.core-1.8.2] startup
[2.188s] [ext: omni.usd.metrics.assembler.physics-107.3.26] startup
[2.190s] [ext: omni.physics.tensors-107.3.26] startup
[2.193s] [ext: isaacsim.storage.native-1.5.1] startup
[2.194s] [ext: isaacsim.core.utils-3.5.1] startup
[2.196s] [ext: omni.kit.widget.text_editor-1.1.1] startup
[2.197s] [ext: omni.physx.tensors-107.3.26] startup
[2.199s] [ext: omni.kit.window.extensions-1.4.27] startup
[2.203s] [ext: isaacsim.core.simulation_manager-1.4.4] startup
[3.226s] [ext: omni.kit.property.light-1.0.12] startup
[3.258s] [ext: isaacsim.core.cloner-1.4.10] startup
[3.260s] [ext: omni.kit.property.camera-1.0.10] startup
[3.260s] [ext: omni.services.facilities.base-1.0.4] startup
[3.261s] [ext: omni.hydra.scene_api-0.1.2] startup
[3.264s] [ext: omni.kit.property.geometry-2.0.4] startup
[3.266s] [ext: omni.kit.property.material-1.11.9] startup
[3.270s] [ext: omni.services.core-1.9.0] startup
[3.481s] [ext: omni.kit.property.transform-1.5.13] startup
[3.483s] [ext: isaacsim.core.prims-0.6.1] startup
[3.505s] [ext: omni.kit.stage_templates-2.0.0] startup
[3.506s] [ext: omni.kit.livestream.core-7.5.0] startup
[3.508s] [ext: omni.services.transport.server.base-1.1.1] startup
[3.509s] [ext: omni.kit.streamsdk.plugins-7.6.3] startup
[3.509s] [ext: omni.kit.property.render-1.2.1] startup
[3.509s] [ext: isaacsim.core.api-4.8.0] startup
[3.516s] [ext: omni.kit.property.audio-1.0.16] startup
[3.517s] [ext: omni.services.transport.server.http-1.3.1] startup
[3.531s] [ext: omni.kit.livestream.webrtc-7.0.0] startup

Failed to open [/var/run/utmp]
Active user not found. Using default user [kiosk]Streaming server started.
[8.960s] [ext: omni.kit.property.bundle-1.4.1] startup
[8.961s] [ext: omni.services.livestream.nvcf-7.2.0] startup
[8.968s] [ext: omni.graph.tools-1.79.2] startup
[8.985s] [ext: omni.graph.action_core-1.1.7] startup
[8.986s] [ext: omni.graph-1.141.2] startup
[9.018s] [ext: omni.ui_query-1.1.8] startup
[9.019s] [ext: omni.graph.action_nodes-1.50.4] startup
[9.021s] [ext: omni.kit.ui_test-1.3.7] startup
[9.024s] [ext: omni.graph.visualization.nodes-2.1.3] startup
[9.035s] [ext: omni.graph.action-1.130.0] startup
[9.037s] [ext: omni.kit.selection-0.1.6] startup
[9.039s] [ext: isaacsim.gui.components-1.2.1] startup
[9.044s] [ext: isaacsim.gui.menu-2.4.4] startup
[9.056s] [ext: semantics.schema.editor-2.0.2] startup
2025-12-05T08:21:09Z [9,049ms] [Warning] [pxr.Semantics] pxr.Semantics is deprecated - please use Semantics instead
[9.074s] [ext: omni.kit.usd.collect-2.4.5] startup
[9.078s] [ext: semantics.schema.property-2.0.1] startup
[9.080s] [ext: omni.kit.usdz_export-1.0.9] startup
[9.084s] [ext: omni.kit.widget.graph-2.0.0] startup
[9.092s] [ext: omni.kit.tool.asset_importer-4.3.2] startup
[9.097s] [ext: omni.kit.graph.delegate.default-1.2.3] startup
[9.098s] [ext: isaacsim.asset.importer.mjcf-2.5.13] startup
[9.105s] [ext: omni.kit.graph.editor.core-1.5.3] startup
[9.108s] [ext: omni.graph.image.core-0.6.1] startup
[9.108s] [ext: omni.kit.widget.zoombar-1.0.6] startup
[9.109s] [ext: omni.kit.graph.usd.commands-1.3.1] startup
[9.110s] [ext: omni.kit.widget.material_preview-1.0.16] startup
[9.111s] [ext: omni.graph.image.nodes-1.3.1] startup
[9.112s] [ext: omni.kit.browser.core-2.3.13] startup
[9.115s] [ext: omni.graph.scriptnode-1.50.0] startup
[9.116s] [ext: omni.kit.window.material_graph-1.9.1] startup
[9.125s] [ext: omni.syntheticdata-0.6.13] startup
[9.145s] [ext: omni.kit.browser.folder.core-1.10.9] startup
[9.152s] [ext: omni.videoencoding-0.1.2] startup
[9.154s] [ext: omni.graph.nodes-1.170.10] startup
[9.164s] [ext: omni.graph.ui_nodes-1.50.5] startup
[9.167s] [ext: omni.kit.widget.stage_icons-1.0.8] startup
[9.169s] [ext: isaacsim.core.experimental.utils-0.3.0] startup
[9.173s] [ext: isaacsim.test.docstring-1.1.0] startup
[9.177s] [ext: omni.warp-1.8.2] startup
[9.181s] [ext: omni.kit.window.stage-2.6.1] startup
[9.187s] [ext: omni.kit.menu.create-2.0.1] startup
[9.189s] [ext: isaacsim.core.experimental.prims-0.8.1] startup
[9.234s] [ext: omni.replicator.core-1.12.27] startup
2025-12-05T08:21:09Z [9,370ms] [Warning] [omni.graph.core.plugin] Found duplicate of category 'Replicator' - was 'Annotators', adding 'Fabric Reader'
2025-12-05T08:21:09Z [9,370ms] [Warning] [omni.graph.core.plugin] Category 'Replicator' not accepted on node type 'omni.replicator.core.FabricReader' in extension 'omni.replicator.core'
2025-12-05T08:21:09Z [9,371ms] [Warning] [omni.replicator.core.scripts.extension] No material configuration file, adding configuration to material settings directly.
[9.387s] [ext: omni.kit.scripting-107.3.2] startup
[9.393s] [ext: isaacsim.robot.surface_gripper-3.3.1] startup
[9.397s] [ext: isaacsim.core.nodes-3.4.3] startup
[9.404s] [ext: omni.kit.tool.collect-2.2.18] startup
[9.409s] [ext: omni.kit.menu.stage-1.2.7] startup
[9.411s] [ext: isaacsim.replicator.behavior-1.1.16] startup
[9.413s] [ext: isaacsim.robot.manipulators-3.3.6] startup
[9.420s] [ext: isaacsim.asset.browser-1.3.23] startup
[9.584s] [ext: isaacsim.util.debug_draw-3.1.0] startup
[9.589s] [ext: omni.sensors.nv.common-3.0.0] startup
[9.601s] [ext: isaacsim.sensors.camera-1.3.6] startup
[9.619s] [ext: isaacsim.sensors.physx-2.3.2] startup
[9.629s] [ext: omni.sensors.net-1.0.0] startup
[9.635s] [ext: omni.sensors.nv.materials-2.0.0] startup
[9.640s] [ext: isaacsim.app.about-2.0.11] startup
[9.650s] [ext: omni.sensors.nv.wpm-3.0.0] startup
[9.653s] [ext: omni.kit.stagerecorder.core-107.0.3] startup
[9.683s] [ext: isaacsim.simulation_app-2.12.2] startup
[9.686s] [ext: omni.sensors.nv.ids-2.0.0] startup
[9.715s] [ext: omni.sensors.nv.lidar-3.0.0] startup
[9.726s] [ext: omni.sensors.nv.radar-3.0.0] startup
[9.728s] [ext: omni.kit.stagerecorder.ui-107.0.1] startup
[9.738s] [ext: omni.anim.curve.core-1.3.1] startup
[9.813s] [ext: omni.kit.manipulator.camera-106.0.4] startup
[9.901s] [ext: isaacsim.gui.property-1.1.3] startup
[9.936s] [ext: isaacsim.sensors.rtx-15.8.4] startup
[9.973s] [ext: omni.kit.stagerecorder.bundle-105.0.2] startup
[9.977s] [ext: omni.kit.viewport.menubar.camera-107.0.6] startup
[10.067s] [ext: omni.kit.property.physx-107.3.26] startup
[10.095s] [ext: omni.physx.demos-107.3.26] startup
[10.134s] [ext: omni.kit.viewport.menubar.lighting-107.3.1] startup
[10.139s] [ext: omni.kit.viewport.menubar.render-107.0.10] startup
[10.144s] [ext: isaacsim.sensors.physics-0.4.3] startup
[10.152s] [ext: omni.asset_validator.core-1.1.6] startup
[10.236s] [ext: omni.physx.vehicle-107.3.26] startup
[10.261s] [ext: omni.kit.window.toolbar-2.0.0] startup
[10.264s] [ext: omni.kit.viewport.menubar.settings-107.0.3] startup
[10.267s] [ext: isaacsim.robot.policy.examples-4.1.11] startup
[10.268s] [ext: omni.physx.asset_validator-107.3.26] startup
[10.274s] [ext: omni.physx.graph-107.3.26] startup
[10.282s] [ext: omni.physx.camera-107.3.26] startup
[10.293s] [ext: omni.physx.supportui-107.3.26] startup
[10.303s] [ext: omni.physx.telemetry-107.3.26] startup
[10.305s] [ext: omni.physx.cct-107.3.26] startup
[10.317s] [ext: omni.ocio-0.1.1] startup
[10.319s] [ext: omni.rtx.window.settings-0.6.19] startup
[10.324s] [ext: omni.kit.window.console-1.1.4] startup
[10.330s] [ext: omni.kit.window.script_editor-2.0.1] startup
[10.333s] [ext: omni.physx.bundle-107.3.26] startup
[10.333s] [ext: omni.replicator.replicator_yaml-2.0.11] startup
[10.345s] [ext: omni.rtx.settings.core-0.6.5] startup
[10.349s] [ext: omni.usd.metrics.assembler.ui-107.3.1] startup
[10.360s] [ext: isaacsim.examples.browser-0.2.1] startup
[10.363s] [ext: isaacsim.asset.importer.urdf-2.4.31] startup
[10.377s] [ext: isaacsim.core.throttling-2.2.2] startup
[10.379s] [ext: omni.kit.ui.actions-1.0.5] startup
[10.381s] [ext: omni.kit.window.status_bar-0.1.9] startup
[10.388s] [ext: isaacsim.robot.wheeled_robots-4.0.24] startup
[10.392s] [ext: isaaclab-0.49.0] startup
[10.549s] [ext: isaaclab_assets-0.2.3] startup
[10.756s] [ext: isaaclab_tasks-0.11.9] startup
[11.106s] [ext: omni.kit.menu.common-2.0.1] startup
[11.107s] [ext: isaaclab_rl-0.4.4] startup
[11.107s] [ext: isaaclab_mimic-1.0.16] startup
[11.108s] [ext: isaaclab.python-2.3.0] startup
[11.109s] Simulation App Starting
2025-12-05T08:21:11Z [11,116ms] [Warning] [gpu.foundation.plugin] Invalid sync scope for buffer resource 'shared swapchain buffer'. Create resource with valid sync scope for lifetime tracking or use kResourceUsageFlagNoSyncScope.
2025-12-05T08:21:11Z [11,116ms] [Warning] [gpu.foundation.plugin] Invalid sync scope for buffer resource 'shared swapchain buffer'. Create resource with valid sync scope for lifetime tracking or use kResourceUsageFlagNoSyncScope.
2025-12-05T08:21:11Z [11,116ms] [Warning] [gpu.foundation.plugin] Invalid sync scope for buffer resource 'shared swapchain buffer'. Create resource with valid sync scope for lifetime tracking or use kResourceUsageFlagNoSyncScope.
2025-12-05T08:21:11Z [11,184ms] [Warning] [omni.fabric.plugin] Warning: attribute viewportHandle not found for bucket id 9

2025-12-05T08:21:11Z [11,214ms] [Warning] [omni.kit.window.filepicker.collections.collection_item] Failed to add '/root/Documents': '/root/Documents/' as dupliated path in 'Documents'
[11.305s] app ready
[INFO][AppLauncher]: Input keyword argument `livestream=2` has overridden the environment variable `LIVESTREAM=0`.
[INFO][AppLauncher]: Input keyword argument `livestream=2` has implicitly overridden the environment variable `HEADLESS=0` to True.
[INFO][AppLauncher]: Using device: cuda:0
[INFO][AppLauncher]: Loading experience file: /workspace/isaaclab/apps/isaaclab.python.kit
[INFO]: Parsing configuration from: isaaclab_tasks.direct.anymal_c.anymal_c_env_cfg:AnymalCFlatEnvCfg
[INFO]: Parsing configuration from: isaaclab_tasks.direct.anymal_c.agents.rsl_rl_ppo_cfg:AnymalCFlatPPORunnerCfg
[INFO] Logging experiment in directory: /workspace/isaaclab/logs/rsl_rl/anymal_c_flat_direct
Exact experiment name requested from command line: 2025-12-05_08-21-14
[2025-12-05 08:21:14,955][__main__][WARNING] - IO descriptors are only supported for manager based RL environments. No IO descriptors will be exported.
[INFO] IsaacLab logging to file: /tmp/isaaclab_2025-12-05_08-21-14.log
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.005
	Rendering step-size   : 0.02
	Environment step-size : 0.02
[INFO]: Time taken for scene creation : 3.852489 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 512
	Environment spacing   : 4.0
	Source prim name      : /World/envs/env_0
	Global prim paths     : []
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[INFO]: Time taken for simulation start : 2.020282 seconds
Creating window for environment.
ManagerLiveVisualizer cannot be created for manager: action_manager, Manager does not exist
ManagerLiveVisualizer cannot be created for manager: observation_manager, Manager does not exist
[INFO] Event Manager:  <EventManager> contains 1 active terms.
+--------------------------------------+
| Active Event Terms in Mode: 'startup' |
+----------+---------------------------+
|  Index   | Name                      |
+----------+---------------------------+
|    0     | physics_material          |
|    1     | add_base_mass             |
+----------+---------------------------+

[INFO]: Completed setting up the environment...
--------------------------------------------------------------------------------
Resolved observation sets: 
	 policy :  ['policy']
	 critic :  ['policy']
--------------------------------------------------------------------------------
Actor MLP: MLP(
  (0): Linear(in_features=48, out_features=128, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=128, out_features=128, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=128, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=12, bias=True)
)
Critic MLP: MLP(
  (0): Linear(in_features=48, out_features=128, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=128, out_features=128, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=128, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/1000 [0m                       

                       Computation: 9629 steps/s (collection: 1.140s, learning 0.136s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.1009
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 17.0058
                       Mean reward: -0.62
               Mean episode length: 12.78
Episode_Reward/track_lin_vel_xy_exp: 0.0017
Episode_Reward/track_ang_vel_z_exp: 0.0024
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0031
     Episode_Reward/dof_torques_l2: -0.0032
         Episode_Reward/dof_acc_l2: -0.0105
     Episode_Reward/action_rate_l2: -0.0027
      Episode_Reward/feet_air_time: -0.0009
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0014
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12288
                    Iteration time: 1.28s
                      Time elapsed: 00:00:01
                               ETA: 00:21:16

Could not find git repository in /workspace/isaaclab/_isaac_sim/kit/python/lib/python3.11/site-packages/rsl_rl/__init__.py. Skipping.
Could not find git repository in /workspace/isaaclab/scripts/reinforcement_learning/rsl_rl/train.py. Skipping.
################################################################################
                      [1m Learning iteration 1/1000 [0m                       

                       Computation: 11362 steps/s (collection: 1.030s, learning 0.051s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0490
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 16.9593
                       Mean reward: -1.05
               Mean episode length: 22.48
Episode_Reward/track_lin_vel_xy_exp: 0.0060
Episode_Reward/track_ang_vel_z_exp: 0.0060
       Episode_Reward/lin_vel_z_l2: -0.0217
      Episode_Reward/ang_vel_xy_l2: -0.0093
     Episode_Reward/dof_torques_l2: -0.0115
         Episode_Reward/dof_acc_l2: -0.0220
     Episode_Reward/action_rate_l2: -0.0080
      Episode_Reward/feet_air_time: -0.0039
 Episode_Reward/undesired_contacts: -0.0079
Episode_Reward/flat_orientation_l2: -0.0070
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 1.08s
                      Time elapsed: 00:00:02
                               ETA: 00:19:37

################################################################################
                      [1m Learning iteration 2/1000 [0m                       

                       Computation: 11211 steps/s (collection: 1.051s, learning 0.045s)
             Mean action noise std: 0.98
          Mean value_function loss: 0.0331
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 16.8349
                       Mean reward: -1.19
               Mean episode length: 28.15
Episode_Reward/track_lin_vel_xy_exp: 0.0083
Episode_Reward/track_ang_vel_z_exp: 0.0075
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0145
     Episode_Reward/dof_torques_l2: -0.0194
         Episode_Reward/dof_acc_l2: -0.0383
     Episode_Reward/action_rate_l2: -0.0134
      Episode_Reward/feet_air_time: -0.0064
 Episode_Reward/undesired_contacts: -0.0105
Episode_Reward/flat_orientation_l2: -0.0113
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36864
                    Iteration time: 1.10s
                      Time elapsed: 00:00:03
                               ETA: 00:19:08

################################################################################
                      [1m Learning iteration 3/1000 [0m                       

                       Computation: 10999 steps/s (collection: 1.069s, learning 0.048s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 16.7395
                       Mean reward: -1.66
               Mean episode length: 43.16
Episode_Reward/track_lin_vel_xy_exp: 0.0147
Episode_Reward/track_ang_vel_z_exp: 0.0135
       Episode_Reward/lin_vel_z_l2: -0.0199
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.0286
         Episode_Reward/dof_acc_l2: -0.0353
     Episode_Reward/action_rate_l2: -0.0203
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0323
Episode_Reward/flat_orientation_l2: -0.0085
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 1.12s
                      Time elapsed: 00:00:04
                               ETA: 00:18:59

################################################################################
                      [1m Learning iteration 4/1000 [0m                       

                       Computation: 11257 steps/s (collection: 1.046s, learning 0.046s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0204
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 16.6218
                       Mean reward: -2.05
               Mean episode length: 56.70
Episode_Reward/track_lin_vel_xy_exp: 0.0208
Episode_Reward/track_ang_vel_z_exp: 0.0195
       Episode_Reward/lin_vel_z_l2: -0.0249
      Episode_Reward/ang_vel_xy_l2: -0.0269
     Episode_Reward/dof_torques_l2: -0.0378
         Episode_Reward/dof_acc_l2: -0.0398
     Episode_Reward/action_rate_l2: -0.0252
      Episode_Reward/feet_air_time: -0.0111
 Episode_Reward/undesired_contacts: -0.0441
Episode_Reward/flat_orientation_l2: -0.0086
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61440
                    Iteration time: 1.09s
                      Time elapsed: 00:00:05
                               ETA: 00:18:47

################################################################################
                      [1m Learning iteration 5/1000 [0m                       

                       Computation: 11848 steps/s (collection: 0.986s, learning 0.051s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 16.5548
                       Mean reward: -2.39
               Mean episode length: 67.34
Episode_Reward/track_lin_vel_xy_exp: 0.0205
Episode_Reward/track_ang_vel_z_exp: 0.0266
       Episode_Reward/lin_vel_z_l2: -0.0225
      Episode_Reward/ang_vel_xy_l2: -0.0292
     Episode_Reward/dof_torques_l2: -0.0475
         Episode_Reward/dof_acc_l2: -0.0435
     Episode_Reward/action_rate_l2: -0.0312
      Episode_Reward/feet_air_time: -0.0138
 Episode_Reward/undesired_contacts: -0.0492
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 1.04s
                      Time elapsed: 00:00:06
                               ETA: 00:18:30

################################################################################
                      [1m Learning iteration 6/1000 [0m                       

                       Computation: 12740 steps/s (collection: 0.919s, learning 0.046s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 16.4790
                       Mean reward: -2.69
               Mean episode length: 79.74
Episode_Reward/track_lin_vel_xy_exp: 0.0452
Episode_Reward/track_ang_vel_z_exp: 0.0284
       Episode_Reward/lin_vel_z_l2: -0.0276
      Episode_Reward/ang_vel_xy_l2: -0.0367
     Episode_Reward/dof_torques_l2: -0.0569
         Episode_Reward/dof_acc_l2: -0.0492
     Episode_Reward/action_rate_l2: -0.0357
      Episode_Reward/feet_air_time: -0.0154
 Episode_Reward/undesired_contacts: -0.0548
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86016
                    Iteration time: 0.96s
                      Time elapsed: 00:00:07
                               ETA: 00:18:08

################################################################################
                      [1m Learning iteration 7/1000 [0m                       

                       Computation: 11876 steps/s (collection: 0.987s, learning 0.048s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 16.3585
                       Mean reward: -3.18
               Mean episode length: 96.81
Episode_Reward/track_lin_vel_xy_exp: 0.0255
Episode_Reward/track_ang_vel_z_exp: 0.0292
       Episode_Reward/lin_vel_z_l2: -0.0275
      Episode_Reward/ang_vel_xy_l2: -0.0367
     Episode_Reward/dof_torques_l2: -0.0658
         Episode_Reward/dof_acc_l2: -0.0592
     Episode_Reward/action_rate_l2: -0.0427
      Episode_Reward/feet_air_time: -0.0167
 Episode_Reward/undesired_contacts: -0.0800
Episode_Reward/flat_orientation_l2: -0.0144
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 1.03s
                      Time elapsed: 00:00:08
                               ETA: 00:17:59

################################################################################
                      [1m Learning iteration 8/1000 [0m                       

                       Computation: 12322 steps/s (collection: 0.952s, learning 0.045s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 16.2620
                       Mean reward: -4.07
               Mean episode length: 130.29
Episode_Reward/track_lin_vel_xy_exp: 0.0305
Episode_Reward/track_ang_vel_z_exp: 0.0397
       Episode_Reward/lin_vel_z_l2: -0.0303
      Episode_Reward/ang_vel_xy_l2: -0.0435
     Episode_Reward/dof_torques_l2: -0.0736
         Episode_Reward/dof_acc_l2: -0.0658
     Episode_Reward/action_rate_l2: -0.0481
      Episode_Reward/feet_air_time: -0.0193
 Episode_Reward/undesired_contacts: -0.0681
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110592
                    Iteration time: 1.00s
                      Time elapsed: 00:00:09
                               ETA: 00:17:48

################################################################################
                      [1m Learning iteration 9/1000 [0m                       

                       Computation: 11623 steps/s (collection: 1.009s, learning 0.048s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0117
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 16.2062
                       Mean reward: -4.68
               Mean episode length: 152.44
Episode_Reward/track_lin_vel_xy_exp: 0.0206
Episode_Reward/track_ang_vel_z_exp: 0.0375
       Episode_Reward/lin_vel_z_l2: -0.0290
      Episode_Reward/ang_vel_xy_l2: -0.0520
     Episode_Reward/dof_torques_l2: -0.0838
         Episode_Reward/dof_acc_l2: -0.0635
     Episode_Reward/action_rate_l2: -0.0529
      Episode_Reward/feet_air_time: -0.0207
 Episode_Reward/undesired_contacts: -0.0797
Episode_Reward/flat_orientation_l2: -0.0185
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 1.06s
                      Time elapsed: 00:00:10
                               ETA: 00:17:45

################################################################################
                      [1m Learning iteration 10/1000 [0m                      

                       Computation: 11703 steps/s (collection: 1.001s, learning 0.049s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0105
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 16.1672
                       Mean reward: -5.22
               Mean episode length: 173.86
Episode_Reward/track_lin_vel_xy_exp: 0.0312
Episode_Reward/track_ang_vel_z_exp: 0.0540
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0496
     Episode_Reward/dof_torques_l2: -0.0949
         Episode_Reward/dof_acc_l2: -0.0654
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.1004
Episode_Reward/flat_orientation_l2: -0.0199
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135168
                    Iteration time: 1.05s
                      Time elapsed: 00:00:11
                               ETA: 00:17:42

################################################################################
                      [1m Learning iteration 11/1000 [0m                      

                       Computation: 11270 steps/s (collection: 1.041s, learning 0.049s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0111
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 16.1161
                       Mean reward: -6.03
               Mean episode length: 210.63
Episode_Reward/track_lin_vel_xy_exp: 0.0571
Episode_Reward/track_ang_vel_z_exp: 0.0538
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.0572
     Episode_Reward/dof_torques_l2: -0.1015
         Episode_Reward/dof_acc_l2: -0.0762
     Episode_Reward/action_rate_l2: -0.0630
      Episode_Reward/feet_air_time: -0.0239
 Episode_Reward/undesired_contacts: -0.0893
Episode_Reward/flat_orientation_l2: -0.0234
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 1.09s
                      Time elapsed: 00:00:12
                               ETA: 00:17:42

################################################################################
                      [1m Learning iteration 12/1000 [0m                      

                       Computation: 11752 steps/s (collection: 0.999s, learning 0.046s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 16.0575
                       Mean reward: -6.48
               Mean episode length: 232.70
Episode_Reward/track_lin_vel_xy_exp: 0.0889
Episode_Reward/track_ang_vel_z_exp: 0.0502
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.0615
     Episode_Reward/dof_torques_l2: -0.1103
         Episode_Reward/dof_acc_l2: -0.0843
     Episode_Reward/action_rate_l2: -0.0679
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0867
Episode_Reward/flat_orientation_l2: -0.0278
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159744
                    Iteration time: 1.05s
                      Time elapsed: 00:00:13
                               ETA: 00:17:39

################################################################################
                      [1m Learning iteration 13/1000 [0m                      

                       Computation: 11706 steps/s (collection: 0.999s, learning 0.050s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 15.9526
                       Mean reward: -7.08
               Mean episode length: 259.83
Episode_Reward/track_lin_vel_xy_exp: 0.0598
Episode_Reward/track_ang_vel_z_exp: 0.0471
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0584
     Episode_Reward/dof_torques_l2: -0.1012
         Episode_Reward/dof_acc_l2: -0.0743
     Episode_Reward/action_rate_l2: -0.0627
      Episode_Reward/feet_air_time: -0.0238
 Episode_Reward/undesired_contacts: -0.0792
Episode_Reward/flat_orientation_l2: -0.0454
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 1.05s
                      Time elapsed: 00:00:14
                               ETA: 00:17:36

################################################################################
                      [1m Learning iteration 14/1000 [0m                      

                       Computation: 12338 steps/s (collection: 0.951s, learning 0.045s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 15.8939
                       Mean reward: -7.79
               Mean episode length: 282.39
Episode_Reward/track_lin_vel_xy_exp: 0.0309
Episode_Reward/track_ang_vel_z_exp: 0.0524
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.0596
     Episode_Reward/dof_torques_l2: -0.0952
         Episode_Reward/dof_acc_l2: -0.0784
     Episode_Reward/action_rate_l2: -0.0603
      Episode_Reward/feet_air_time: -0.0229
 Episode_Reward/undesired_contacts: -0.0726
Episode_Reward/flat_orientation_l2: -0.0806
  Episode_Termination/base_contact: 1.1250
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184320
                    Iteration time: 1.00s
                      Time elapsed: 00:00:15
                               ETA: 00:17:30

################################################################################
                      [1m Learning iteration 15/1000 [0m                      

                       Computation: 11753 steps/s (collection: 1.000s, learning 0.046s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0204
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 15.8354
                       Mean reward: -8.45
               Mean episode length: 300.27
Episode_Reward/track_lin_vel_xy_exp: 0.0449
Episode_Reward/track_ang_vel_z_exp: 0.0535
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.0660
     Episode_Reward/dof_torques_l2: -0.0986
         Episode_Reward/dof_acc_l2: -0.0857
     Episode_Reward/action_rate_l2: -0.0637
      Episode_Reward/feet_air_time: -0.0230
 Episode_Reward/undesired_contacts: -0.0637
Episode_Reward/flat_orientation_l2: -0.0894
  Episode_Termination/base_contact: 1.6667
      Episode_Termination/time_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 1.05s
                      Time elapsed: 00:00:17
                               ETA: 00:17:28

################################################################################
                      [1m Learning iteration 16/1000 [0m                      

                       Computation: 12060 steps/s (collection: 0.974s, learning 0.045s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 15.7408
                       Mean reward: -9.24
               Mean episode length: 322.05
Episode_Reward/track_lin_vel_xy_exp: 0.0745
Episode_Reward/track_ang_vel_z_exp: 0.0598
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.0769
     Episode_Reward/dof_torques_l2: -0.1192
         Episode_Reward/dof_acc_l2: -0.0949
     Episode_Reward/action_rate_l2: -0.0753
      Episode_Reward/feet_air_time: -0.0279
 Episode_Reward/undesired_contacts: -0.0713
Episode_Reward/flat_orientation_l2: -0.0861
  Episode_Termination/base_contact: 1.6667
      Episode_Termination/time_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 208896
                    Iteration time: 1.02s
                      Time elapsed: 00:00:18
                               ETA: 00:17:24

################################################################################
                      [1m Learning iteration 17/1000 [0m                      

                       Computation: 12171 steps/s (collection: 0.962s, learning 0.047s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0201
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 15.6698
                       Mean reward: -9.69
               Mean episode length: 344.61
Episode_Reward/track_lin_vel_xy_exp: 0.0576
Episode_Reward/track_ang_vel_z_exp: 0.0720
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.0791
     Episode_Reward/dof_torques_l2: -0.1347
         Episode_Reward/dof_acc_l2: -0.0996
     Episode_Reward/action_rate_l2: -0.0827
      Episode_Reward/feet_air_time: -0.0300
 Episode_Reward/undesired_contacts: -0.0954
Episode_Reward/flat_orientation_l2: -0.0919
  Episode_Termination/base_contact: 2.2500
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 1.01s
                      Time elapsed: 00:00:19
                               ETA: 00:17:20

################################################################################
                      [1m Learning iteration 18/1000 [0m                      

                       Computation: 12304 steps/s (collection: 0.953s, learning 0.045s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 15.5926
                       Mean reward: -8.93
               Mean episode length: 330.29
Episode_Reward/track_lin_vel_xy_exp: 0.0588
Episode_Reward/track_ang_vel_z_exp: 0.0681
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.0710
     Episode_Reward/dof_torques_l2: -0.1188
         Episode_Reward/dof_acc_l2: -0.0934
     Episode_Reward/action_rate_l2: -0.0745
      Episode_Reward/feet_air_time: -0.0280
 Episode_Reward/undesired_contacts: -0.0755
Episode_Reward/flat_orientation_l2: -0.0845
  Episode_Termination/base_contact: 1.9167
      Episode_Termination/time_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 233472
                    Iteration time: 1.00s
                      Time elapsed: 00:00:20
                               ETA: 00:17:16

################################################################################
                      [1m Learning iteration 19/1000 [0m                      

                       Computation: 11871 steps/s (collection: 0.988s, learning 0.047s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 15.4952
                       Mean reward: -9.28
               Mean episode length: 354.50
Episode_Reward/track_lin_vel_xy_exp: 0.0548
Episode_Reward/track_ang_vel_z_exp: 0.0647
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.0683
     Episode_Reward/dof_torques_l2: -0.1182
         Episode_Reward/dof_acc_l2: -0.0948
     Episode_Reward/action_rate_l2: -0.0740
      Episode_Reward/feet_air_time: -0.0283
 Episode_Reward/undesired_contacts: -0.0760
Episode_Reward/flat_orientation_l2: -0.0775
  Episode_Termination/base_contact: 2.3333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 1.04s
                      Time elapsed: 00:00:21
                               ETA: 00:17:14

################################################################################
                      [1m Learning iteration 20/1000 [0m                      

                       Computation: 11965 steps/s (collection: 0.982s, learning 0.045s)
             Mean action noise std: 0.87
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 15.4304
                       Mean reward: -8.60
               Mean episode length: 332.48
Episode_Reward/track_lin_vel_xy_exp: 0.0818
Episode_Reward/track_ang_vel_z_exp: 0.0605
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0676
     Episode_Reward/dof_torques_l2: -0.1158
         Episode_Reward/dof_acc_l2: -0.0883
     Episode_Reward/action_rate_l2: -0.0701
      Episode_Reward/feet_air_time: -0.0259
 Episode_Reward/undesired_contacts: -0.0651
Episode_Reward/flat_orientation_l2: -0.0893
  Episode_Termination/base_contact: 2.3333
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 258048
                    Iteration time: 1.03s
                      Time elapsed: 00:00:22
                               ETA: 00:17:12

################################################################################
                      [1m Learning iteration 21/1000 [0m                      

                       Computation: 12247 steps/s (collection: 0.954s, learning 0.050s)
             Mean action noise std: 0.87
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 15.3414
                       Mean reward: -8.21
               Mean episode length: 314.61
Episode_Reward/track_lin_vel_xy_exp: 0.0458
Episode_Reward/track_ang_vel_z_exp: 0.0501
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.0568
     Episode_Reward/dof_torques_l2: -0.0913
         Episode_Reward/dof_acc_l2: -0.0826
     Episode_Reward/action_rate_l2: -0.0597
      Episode_Reward/feet_air_time: -0.0246
 Episode_Reward/undesired_contacts: -0.0383
Episode_Reward/flat_orientation_l2: -0.0697
  Episode_Termination/base_contact: 2.1250
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 1.00s
                      Time elapsed: 00:00:23
                               ETA: 00:17:08

################################################################################
                      [1m Learning iteration 22/1000 [0m                      

                       Computation: 12395 steps/s (collection: 0.946s, learning 0.046s)
             Mean action noise std: 0.86
          Mean value_function loss: 0.0106
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 15.2446
                       Mean reward: -8.31
               Mean episode length: 325.65
Episode_Reward/track_lin_vel_xy_exp: 0.0675
Episode_Reward/track_ang_vel_z_exp: 0.0851
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.0817
     Episode_Reward/dof_torques_l2: -0.1331
         Episode_Reward/dof_acc_l2: -0.1124
     Episode_Reward/action_rate_l2: -0.0874
      Episode_Reward/feet_air_time: -0.0350
 Episode_Reward/undesired_contacts: -0.0770
Episode_Reward/flat_orientation_l2: -0.0817
  Episode_Termination/base_contact: 2.2917
      Episode_Termination/time_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 282624
                    Iteration time: 0.99s
                      Time elapsed: 00:00:24
                               ETA: 00:17:05

################################################################################
                      [1m Learning iteration 23/1000 [0m                      

                       Computation: 12104 steps/s (collection: 0.971s, learning 0.045s)
             Mean action noise std: 0.85
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 15.1301
                       Mean reward: -7.87
               Mean episode length: 313.38
Episode_Reward/track_lin_vel_xy_exp: 0.0376
Episode_Reward/track_ang_vel_z_exp: 0.0450
       Episode_Reward/lin_vel_z_l2: -0.0287
      Episode_Reward/ang_vel_xy_l2: -0.0492
     Episode_Reward/dof_torques_l2: -0.0734
         Episode_Reward/dof_acc_l2: -0.0751
     Episode_Reward/action_rate_l2: -0.0501
      Episode_Reward/feet_air_time: -0.0227
 Episode_Reward/undesired_contacts: -0.0283
Episode_Reward/flat_orientation_l2: -0.0645
  Episode_Termination/base_contact: 1.9583
      Episode_Termination/time_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 1.02s
                      Time elapsed: 00:00:25
                               ETA: 00:17:02

################################################################################
                      [1m Learning iteration 24/1000 [0m                      

                       Computation: 12404 steps/s (collection: 0.945s, learning 0.045s)
             Mean action noise std: 0.85
          Mean value_function loss: 0.0107
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 15.0668
                       Mean reward: -7.43
               Mean episode length: 301.46
Episode_Reward/track_lin_vel_xy_exp: 0.0424
Episode_Reward/track_ang_vel_z_exp: 0.0598
       Episode_Reward/lin_vel_z_l2: -0.0339
      Episode_Reward/ang_vel_xy_l2: -0.0598
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.0869
     Episode_Reward/action_rate_l2: -0.0635
      Episode_Reward/feet_air_time: -0.0266
 Episode_Reward/undesired_contacts: -0.0348
Episode_Reward/flat_orientation_l2: -0.0617
  Episode_Termination/base_contact: 1.5000
      Episode_Termination/time_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 307200
                    Iteration time: 0.99s
                      Time elapsed: 00:00:26
                               ETA: 00:16:59

################################################################################
                      [1m Learning iteration 25/1000 [0m                      

                       Computation: 12309 steps/s (collection: 0.950s, learning 0.049s)
             Mean action noise std: 0.85
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 15.0120
                       Mean reward: -7.04
               Mean episode length: 291.86
Episode_Reward/track_lin_vel_xy_exp: 0.0658
Episode_Reward/track_ang_vel_z_exp: 0.0861
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.0741
     Episode_Reward/dof_torques_l2: -0.1304
         Episode_Reward/dof_acc_l2: -0.1152
     Episode_Reward/action_rate_l2: -0.0838
      Episode_Reward/feet_air_time: -0.0351
 Episode_Reward/undesired_contacts: -0.0674
Episode_Reward/flat_orientation_l2: -0.0696
  Episode_Termination/base_contact: 1.8750
      Episode_Termination/time_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 1.00s
                      Time elapsed: 00:00:27
                               ETA: 00:16:56

################################################################################
                      [1m Learning iteration 26/1000 [0m                      

                       Computation: 12116 steps/s (collection: 0.969s, learning 0.045s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0124
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 14.9694
                       Mean reward: -7.14
               Mean episode length: 313.45
Episode_Reward/track_lin_vel_xy_exp: 0.0522
Episode_Reward/track_ang_vel_z_exp: 0.0480
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0510
     Episode_Reward/dof_torques_l2: -0.0689
         Episode_Reward/dof_acc_l2: -0.0805
     Episode_Reward/action_rate_l2: -0.0507
      Episode_Reward/feet_air_time: -0.0237
 Episode_Reward/undesired_contacts: -0.0156
Episode_Reward/flat_orientation_l2: -0.0650
  Episode_Termination/base_contact: 1.9167
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 331776
                    Iteration time: 1.01s
                      Time elapsed: 00:00:28
                               ETA: 00:16:54

################################################################################
                      [1m Learning iteration 27/1000 [0m                      

                       Computation: 12234 steps/s (collection: 0.959s, learning 0.045s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 14.8870
                       Mean reward: -7.13
               Mean episode length: 319.41
Episode_Reward/track_lin_vel_xy_exp: 0.0597
Episode_Reward/track_ang_vel_z_exp: 0.0661
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0622
     Episode_Reward/dof_torques_l2: -0.1037
         Episode_Reward/dof_acc_l2: -0.1035
     Episode_Reward/action_rate_l2: -0.0696
      Episode_Reward/feet_air_time: -0.0307
 Episode_Reward/undesired_contacts: -0.0411
Episode_Reward/flat_orientation_l2: -0.0636
  Episode_Termination/base_contact: 1.5833
      Episode_Termination/time_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 1.00s
                      Time elapsed: 00:00:29
                               ETA: 00:16:52

################################################################################
                      [1m Learning iteration 28/1000 [0m                      

                       Computation: 12094 steps/s (collection: 0.970s, learning 0.046s)
             Mean action noise std: 0.83
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 14.8315
                       Mean reward: -6.80
               Mean episode length: 313.36
Episode_Reward/track_lin_vel_xy_exp: 0.0756
Episode_Reward/track_ang_vel_z_exp: 0.0625
       Episode_Reward/lin_vel_z_l2: -0.0323
      Episode_Reward/ang_vel_xy_l2: -0.0577
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1038
     Episode_Reward/action_rate_l2: -0.0631
      Episode_Reward/feet_air_time: -0.0309
 Episode_Reward/undesired_contacts: -0.0201
Episode_Reward/flat_orientation_l2: -0.0624
  Episode_Termination/base_contact: 1.9583
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 356352
                    Iteration time: 1.02s
                      Time elapsed: 00:00:30
                               ETA: 00:16:50

################################################################################
                      [1m Learning iteration 29/1000 [0m                      

                       Computation: 11974 steps/s (collection: 0.974s, learning 0.052s)
             Mean action noise std: 0.83
          Mean value_function loss: 0.0111
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 14.7648
                       Mean reward: -6.25
               Mean episode length: 295.30
Episode_Reward/track_lin_vel_xy_exp: 0.0663
Episode_Reward/track_ang_vel_z_exp: 0.0587
       Episode_Reward/lin_vel_z_l2: -0.0329
      Episode_Reward/ang_vel_xy_l2: -0.0544
     Episode_Reward/dof_torques_l2: -0.0774
         Episode_Reward/dof_acc_l2: -0.0903
     Episode_Reward/action_rate_l2: -0.0568
      Episode_Reward/feet_air_time: -0.0285
 Episode_Reward/undesired_contacts: -0.0186
Episode_Reward/flat_orientation_l2: -0.0556
  Episode_Termination/base_contact: 2.1250
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 1.03s
                      Time elapsed: 00:00:31
                               ETA: 00:16:49

################################################################################
                      [1m Learning iteration 30/1000 [0m                      

                       Computation: 12423 steps/s (collection: 0.944s, learning 0.045s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 14.6936
                       Mean reward: -6.03
               Mean episode length: 290.97
Episode_Reward/track_lin_vel_xy_exp: 0.0469
Episode_Reward/track_ang_vel_z_exp: 0.0556
       Episode_Reward/lin_vel_z_l2: -0.0299
      Episode_Reward/ang_vel_xy_l2: -0.0490
     Episode_Reward/dof_torques_l2: -0.0738
         Episode_Reward/dof_acc_l2: -0.0883
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0276
 Episode_Reward/undesired_contacts: -0.0163
Episode_Reward/flat_orientation_l2: -0.0518
  Episode_Termination/base_contact: 1.9167
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 380928
                    Iteration time: 0.99s
                      Time elapsed: 00:00:32
                               ETA: 00:16:46

################################################################################
                      [1m Learning iteration 31/1000 [0m                      

                       Computation: 12445 steps/s (collection: 0.942s, learning 0.045s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0122
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 14.6270
                       Mean reward: -5.45
               Mean episode length: 276.58
Episode_Reward/track_lin_vel_xy_exp: 0.0637
Episode_Reward/track_ang_vel_z_exp: 0.0622
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0489
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.0900
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0286
 Episode_Reward/undesired_contacts: -0.0248
Episode_Reward/flat_orientation_l2: -0.0494
  Episode_Termination/base_contact: 2.2500
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.99s
                      Time elapsed: 00:00:33
                               ETA: 00:16:43

################################################################################
                      [1m Learning iteration 32/1000 [0m                      

                       Computation: 12545 steps/s (collection: 0.927s, learning 0.053s)
             Mean action noise std: 0.81
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 14.5474
                       Mean reward: -4.71
               Mean episode length: 253.76
Episode_Reward/track_lin_vel_xy_exp: 0.0642
Episode_Reward/track_ang_vel_z_exp: 0.0561
       Episode_Reward/lin_vel_z_l2: -0.0238
      Episode_Reward/ang_vel_xy_l2: -0.0396
     Episode_Reward/dof_torques_l2: -0.0573
         Episode_Reward/dof_acc_l2: -0.0817
     Episode_Reward/action_rate_l2: -0.0448
      Episode_Reward/feet_air_time: -0.0258
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: -0.0462
  Episode_Termination/base_contact: 2.8750
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 405504
                    Iteration time: 0.98s
                      Time elapsed: 00:00:34
                               ETA: 00:16:41

################################################################################
                      [1m Learning iteration 33/1000 [0m                      

                       Computation: 12705 steps/s (collection: 0.923s, learning 0.045s)
             Mean action noise std: 0.81
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 14.4625
                       Mean reward: -4.51
               Mean episode length: 247.81
Episode_Reward/track_lin_vel_xy_exp: 0.0492
Episode_Reward/track_ang_vel_z_exp: 0.0715
       Episode_Reward/lin_vel_z_l2: -0.0328
      Episode_Reward/ang_vel_xy_l2: -0.0542
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1077
     Episode_Reward/action_rate_l2: -0.0626
      Episode_Reward/feet_air_time: -0.0348
 Episode_Reward/undesired_contacts: -0.0120
Episode_Reward/flat_orientation_l2: -0.0513
  Episode_Termination/base_contact: 2.1250
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 0.97s
                      Time elapsed: 00:00:35
                               ETA: 00:16:38

################################################################################
                      [1m Learning iteration 34/1000 [0m                      

                       Computation: 11793 steps/s (collection: 0.996s, learning 0.046s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0113
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 14.3868
                       Mean reward: -4.36
               Mean episode length: 237.78
Episode_Reward/track_lin_vel_xy_exp: 0.0509
Episode_Reward/track_ang_vel_z_exp: 0.0516
       Episode_Reward/lin_vel_z_l2: -0.0251
      Episode_Reward/ang_vel_xy_l2: -0.0386
     Episode_Reward/dof_torques_l2: -0.0548
         Episode_Reward/dof_acc_l2: -0.0777
     Episode_Reward/action_rate_l2: -0.0428
      Episode_Reward/feet_air_time: -0.0241
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0464
  Episode_Termination/base_contact: 3.0417
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 430080
                    Iteration time: 1.04s
                      Time elapsed: 00:00:36
                               ETA: 00:16:37

################################################################################
                      [1m Learning iteration 35/1000 [0m                      

                       Computation: 12304 steps/s (collection: 0.949s, learning 0.049s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 14.3212
                       Mean reward: -4.81
               Mean episode length: 273.76
Episode_Reward/track_lin_vel_xy_exp: 0.0588
Episode_Reward/track_ang_vel_z_exp: 0.0630
       Episode_Reward/lin_vel_z_l2: -0.0293
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0680
         Episode_Reward/dof_acc_l2: -0.0964
     Episode_Reward/action_rate_l2: -0.0531
      Episode_Reward/feet_air_time: -0.0315
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: -0.0456
  Episode_Termination/base_contact: 2.8750
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 1.00s
                      Time elapsed: 00:00:37
                               ETA: 00:16:35

################################################################################
                      [1m Learning iteration 36/1000 [0m                      

                       Computation: 11938 steps/s (collection: 0.965s, learning 0.064s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0100
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 14.2702
                       Mean reward: -4.70
               Mean episode length: 275.83
Episode_Reward/track_lin_vel_xy_exp: 0.0656
Episode_Reward/track_ang_vel_z_exp: 0.0716
       Episode_Reward/lin_vel_z_l2: -0.0275
      Episode_Reward/ang_vel_xy_l2: -0.0471
     Episode_Reward/dof_torques_l2: -0.0679
         Episode_Reward/dof_acc_l2: -0.0896
     Episode_Reward/action_rate_l2: -0.0529
      Episode_Reward/feet_air_time: -0.0299
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: -0.0447
  Episode_Termination/base_contact: 3.4167
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 454656
                    Iteration time: 1.03s
                      Time elapsed: 00:00:38
                               ETA: 00:16:34

################################################################################
                      [1m Learning iteration 37/1000 [0m                      

                       Computation: 11974 steps/s (collection: 0.972s, learning 0.054s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 14.2465
                       Mean reward: -4.20
               Mean episode length: 254.84
Episode_Reward/track_lin_vel_xy_exp: 0.0638
Episode_Reward/track_ang_vel_z_exp: 0.0613
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0414
     Episode_Reward/dof_torques_l2: -0.0575
         Episode_Reward/dof_acc_l2: -0.0816
     Episode_Reward/action_rate_l2: -0.0458
      Episode_Reward/feet_air_time: -0.0271
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0474
  Episode_Termination/base_contact: 3.0833
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 1.03s
                      Time elapsed: 00:00:39
                               ETA: 00:16:33

################################################################################
                      [1m Learning iteration 38/1000 [0m                      

                       Computation: 11922 steps/s (collection: 0.985s, learning 0.045s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 14.1887
                       Mean reward: -3.55
               Mean episode length: 219.62
Episode_Reward/track_lin_vel_xy_exp: 0.0568
Episode_Reward/track_ang_vel_z_exp: 0.0538
       Episode_Reward/lin_vel_z_l2: -0.0237
      Episode_Reward/ang_vel_xy_l2: -0.0370
     Episode_Reward/dof_torques_l2: -0.0518
         Episode_Reward/dof_acc_l2: -0.0745
     Episode_Reward/action_rate_l2: -0.0404
      Episode_Reward/feet_air_time: -0.0242
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: -0.0392
  Episode_Termination/base_contact: 3.7917
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 479232
                    Iteration time: 1.03s
                      Time elapsed: 00:00:40
                               ETA: 00:16:32

################################################################################
                      [1m Learning iteration 39/1000 [0m                      

                       Computation: 11988 steps/s (collection: 0.979s, learning 0.046s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 14.1350
                       Mean reward: -3.83
               Mean episode length: 225.04
Episode_Reward/track_lin_vel_xy_exp: 0.0483
Episode_Reward/track_ang_vel_z_exp: 0.0578
       Episode_Reward/lin_vel_z_l2: -0.0226
      Episode_Reward/ang_vel_xy_l2: -0.0362
     Episode_Reward/dof_torques_l2: -0.0526
         Episode_Reward/dof_acc_l2: -0.0738
     Episode_Reward/action_rate_l2: -0.0410
      Episode_Reward/feet_air_time: -0.0256
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0411
  Episode_Termination/base_contact: 4.0000
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 1.03s
                      Time elapsed: 00:00:41
                               ETA: 00:16:31

################################################################################
                      [1m Learning iteration 40/1000 [0m                      

                       Computation: 11426 steps/s (collection: 1.016s, learning 0.060s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0150
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 14.0898
                       Mean reward: -3.01
               Mean episode length: 168.29
Episode_Reward/track_lin_vel_xy_exp: 0.0340
Episode_Reward/track_ang_vel_z_exp: 0.0457
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0290
     Episode_Reward/dof_torques_l2: -0.0381
         Episode_Reward/dof_acc_l2: -0.0606
     Episode_Reward/action_rate_l2: -0.0304
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0388
  Episode_Termination/base_contact: 4.1667
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 503808
                    Iteration time: 1.08s
                      Time elapsed: 00:00:42
                               ETA: 00:16:31

################################################################################
                      [1m Learning iteration 41/1000 [0m                      

                       Computation: 11883 steps/s (collection: 0.989s, learning 0.045s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 14.0564
                       Mean reward: -2.75
               Mean episode length: 154.12
Episode_Reward/track_lin_vel_xy_exp: 0.0341
Episode_Reward/track_ang_vel_z_exp: 0.0388
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0238
     Episode_Reward/dof_torques_l2: -0.0314
         Episode_Reward/dof_acc_l2: -0.0502
     Episode_Reward/action_rate_l2: -0.0250
      Episode_Reward/feet_air_time: -0.0174
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: -0.0368
  Episode_Termination/base_contact: 5.0417
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 1.03s
                      Time elapsed: 00:00:43
                               ETA: 00:16:30

################################################################################
                      [1m Learning iteration 42/1000 [0m                      

                       Computation: 11979 steps/s (collection: 0.964s, learning 0.062s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 14.0052
                       Mean reward: -2.97
               Mean episode length: 152.65
Episode_Reward/track_lin_vel_xy_exp: 0.0340
Episode_Reward/track_ang_vel_z_exp: 0.0417
       Episode_Reward/lin_vel_z_l2: -0.0186
      Episode_Reward/ang_vel_xy_l2: -0.0266
     Episode_Reward/dof_torques_l2: -0.0359
         Episode_Reward/dof_acc_l2: -0.0568
     Episode_Reward/action_rate_l2: -0.0286
      Episode_Reward/feet_air_time: -0.0199
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: -0.0432
  Episode_Termination/base_contact: 5.2083
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 528384
                    Iteration time: 1.03s
                      Time elapsed: 00:00:44
                               ETA: 00:16:28

################################################################################
                      [1m Learning iteration 43/1000 [0m                      

                       Computation: 11974 steps/s (collection: 0.971s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.9502
                       Mean reward: -2.46
               Mean episode length: 139.98
Episode_Reward/track_lin_vel_xy_exp: 0.0316
Episode_Reward/track_ang_vel_z_exp: 0.0362
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.0289
         Episode_Reward/dof_acc_l2: -0.0450
     Episode_Reward/action_rate_l2: -0.0230
      Episode_Reward/feet_air_time: -0.0164
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0337
  Episode_Termination/base_contact: 4.7917
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 1.03s
                      Time elapsed: 00:00:45
                               ETA: 00:16:27

################################################################################
                      [1m Learning iteration 44/1000 [0m                      

                       Computation: 12191 steps/s (collection: 0.953s, learning 0.055s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.9027
                       Mean reward: -1.94
               Mean episode length: 104.90
Episode_Reward/track_lin_vel_xy_exp: 0.0275
Episode_Reward/track_ang_vel_z_exp: 0.0295
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0229
         Episode_Reward/dof_acc_l2: -0.0377
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0137
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0321
  Episode_Termination/base_contact: 5.6250
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 552960
                    Iteration time: 1.01s
                      Time elapsed: 00:00:46
                               ETA: 00:16:26

################################################################################
                      [1m Learning iteration 45/1000 [0m                      

                       Computation: 11705 steps/s (collection: 1.005s, learning 0.045s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.8748
                       Mean reward: -2.03
               Mean episode length: 111.06
Episode_Reward/track_lin_vel_xy_exp: 0.0297
Episode_Reward/track_ang_vel_z_exp: 0.0289
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.0231
         Episode_Reward/dof_acc_l2: -0.0388
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0132
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0321
  Episode_Termination/base_contact: 5.3333
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 1.05s
                      Time elapsed: 00:00:47
                               ETA: 00:16:25

################################################################################
                      [1m Learning iteration 46/1000 [0m                      

                       Computation: 12209 steps/s (collection: 0.960s, learning 0.046s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.8288
                       Mean reward: -1.88
               Mean episode length: 103.01
Episode_Reward/track_lin_vel_xy_exp: 0.0438
Episode_Reward/track_ang_vel_z_exp: 0.0349
       Episode_Reward/lin_vel_z_l2: -0.0138
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.0270
         Episode_Reward/dof_acc_l2: -0.0446
     Episode_Reward/action_rate_l2: -0.0213
      Episode_Reward/feet_air_time: -0.0156
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0343
  Episode_Termination/base_contact: 5.1667
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 577536
                    Iteration time: 1.01s
                      Time elapsed: 00:00:48
                               ETA: 00:16:24

################################################################################
                      [1m Learning iteration 47/1000 [0m                      

                       Computation: 12524 steps/s (collection: 0.936s, learning 0.045s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.7631
                       Mean reward: -1.91
               Mean episode length: 108.89
Episode_Reward/track_lin_vel_xy_exp: 0.0326
Episode_Reward/track_ang_vel_z_exp: 0.0296
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.0230
         Episode_Reward/dof_acc_l2: -0.0389
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0140
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0324
  Episode_Termination/base_contact: 6.6250
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.98s
                      Time elapsed: 00:00:49
                               ETA: 00:16:21

################################################################################
                      [1m Learning iteration 48/1000 [0m                      

                       Computation: 12203 steps/s (collection: 0.962s, learning 0.045s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.7224
                       Mean reward: -1.63
               Mean episode length: 85.75
Episode_Reward/track_lin_vel_xy_exp: 0.0247
Episode_Reward/track_ang_vel_z_exp: 0.0237
       Episode_Reward/lin_vel_z_l2: -0.0101
      Episode_Reward/ang_vel_xy_l2: -0.0147
     Episode_Reward/dof_torques_l2: -0.0178
         Episode_Reward/dof_acc_l2: -0.0294
     Episode_Reward/action_rate_l2: -0.0140
      Episode_Reward/feet_air_time: -0.0109
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0309
  Episode_Termination/base_contact: 5.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 602112
                    Iteration time: 1.01s
                      Time elapsed: 00:00:50
                               ETA: 00:16:20

################################################################################
                      [1m Learning iteration 49/1000 [0m                      

                       Computation: 12466 steps/s (collection: 0.940s, learning 0.045s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.6698
                       Mean reward: -1.63
               Mean episode length: 95.54
Episode_Reward/track_lin_vel_xy_exp: 0.0343
Episode_Reward/track_ang_vel_z_exp: 0.0273
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0167
     Episode_Reward/dof_torques_l2: -0.0207
         Episode_Reward/dof_acc_l2: -0.0333
     Episode_Reward/action_rate_l2: -0.0165
      Episode_Reward/feet_air_time: -0.0125
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0282
  Episode_Termination/base_contact: 5.3750
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 0.99s
                      Time elapsed: 00:00:51
                               ETA: 00:16:18

################################################################################
                      [1m Learning iteration 50/1000 [0m                      

                       Computation: 12198 steps/s (collection: 0.963s, learning 0.044s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 13.6460
                       Mean reward: -1.72
               Mean episode length: 97.13
Episode_Reward/track_lin_vel_xy_exp: 0.0282
Episode_Reward/track_ang_vel_z_exp: 0.0265
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.0200
         Episode_Reward/dof_acc_l2: -0.0328
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0121
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0287
  Episode_Termination/base_contact: 6.5833
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 626688
                    Iteration time: 1.01s
                      Time elapsed: 00:00:52
                               ETA: 00:16:17

################################################################################
                      [1m Learning iteration 51/1000 [0m                      

                       Computation: 12187 steps/s (collection: 0.961s, learning 0.047s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0122
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 13.6291
                       Mean reward: -1.46
               Mean episode length: 87.23
Episode_Reward/track_lin_vel_xy_exp: 0.0295
Episode_Reward/track_ang_vel_z_exp: 0.0237
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0143
     Episode_Reward/dof_torques_l2: -0.0185
         Episode_Reward/dof_acc_l2: -0.0296
     Episode_Reward/action_rate_l2: -0.0143
      Episode_Reward/feet_air_time: -0.0112
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0277
  Episode_Termination/base_contact: 5.7917
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 1.01s
                      Time elapsed: 00:00:53
                               ETA: 00:16:15

################################################################################
                      [1m Learning iteration 52/1000 [0m                      

                       Computation: 12029 steps/s (collection: 0.977s, learning 0.045s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.5728
                       Mean reward: -1.46
               Mean episode length: 84.83
Episode_Reward/track_lin_vel_xy_exp: 0.0265
Episode_Reward/track_ang_vel_z_exp: 0.0241
       Episode_Reward/lin_vel_z_l2: -0.0107
      Episode_Reward/ang_vel_xy_l2: -0.0141
     Episode_Reward/dof_torques_l2: -0.0183
         Episode_Reward/dof_acc_l2: -0.0293
     Episode_Reward/action_rate_l2: -0.0141
      Episode_Reward/feet_air_time: -0.0112
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0258
  Episode_Termination/base_contact: 6.6250
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 651264
                    Iteration time: 1.02s
                      Time elapsed: 00:00:54
                               ETA: 00:16:14

################################################################################
                      [1m Learning iteration 53/1000 [0m                      

                       Computation: 12567 steps/s (collection: 0.932s, learning 0.046s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.5258
                       Mean reward: -1.47
               Mean episode length: 91.22
Episode_Reward/track_lin_vel_xy_exp: 0.0299
Episode_Reward/track_ang_vel_z_exp: 0.0246
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0146
     Episode_Reward/dof_torques_l2: -0.0188
         Episode_Reward/dof_acc_l2: -0.0297
     Episode_Reward/action_rate_l2: -0.0144
      Episode_Reward/feet_air_time: -0.0112
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0255
  Episode_Termination/base_contact: 6.7083
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 0.98s
                      Time elapsed: 00:00:55
                               ETA: 00:16:12

################################################################################
                      [1m Learning iteration 54/1000 [0m                      

                       Computation: 12125 steps/s (collection: 0.968s, learning 0.046s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0109
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.4900
                       Mean reward: -1.28
               Mean episode length: 84.98
Episode_Reward/track_lin_vel_xy_exp: 0.0277
Episode_Reward/track_ang_vel_z_exp: 0.0222
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0133
     Episode_Reward/dof_torques_l2: -0.0168
         Episode_Reward/dof_acc_l2: -0.0267
     Episode_Reward/action_rate_l2: -0.0130
      Episode_Reward/feet_air_time: -0.0101
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0240
  Episode_Termination/base_contact: 7.2083
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 675840
                    Iteration time: 1.01s
                      Time elapsed: 00:00:56
                               ETA: 00:16:11

################################################################################
                      [1m Learning iteration 55/1000 [0m                      

                       Computation: 12081 steps/s (collection: 0.971s, learning 0.046s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0115
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 13.4719
                       Mean reward: -1.23
               Mean episode length: 71.65
Episode_Reward/track_lin_vel_xy_exp: 0.0224
Episode_Reward/track_ang_vel_z_exp: 0.0200
       Episode_Reward/lin_vel_z_l2: -0.0105
      Episode_Reward/ang_vel_xy_l2: -0.0116
     Episode_Reward/dof_torques_l2: -0.0150
         Episode_Reward/dof_acc_l2: -0.0237
     Episode_Reward/action_rate_l2: -0.0113
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0213
  Episode_Termination/base_contact: 6.6250
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 1.02s
                      Time elapsed: 00:00:57
                               ETA: 00:16:10

################################################################################
                      [1m Learning iteration 56/1000 [0m                      

                       Computation: 12015 steps/s (collection: 0.976s, learning 0.046s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.4289
                       Mean reward: -1.27
               Mean episode length: 69.86
Episode_Reward/track_lin_vel_xy_exp: 0.0235
Episode_Reward/track_ang_vel_z_exp: 0.0195
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0124
     Episode_Reward/dof_torques_l2: -0.0150
         Episode_Reward/dof_acc_l2: -0.0234
     Episode_Reward/action_rate_l2: -0.0113
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0236
  Episode_Termination/base_contact: 6.5417
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 700416
                    Iteration time: 1.02s
                      Time elapsed: 00:00:58
                               ETA: 00:16:09

################################################################################
                      [1m Learning iteration 57/1000 [0m                      

                       Computation: 12209 steps/s (collection: 0.962s, learning 0.044s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.3878
                       Mean reward: -1.24
               Mean episode length: 82.96
Episode_Reward/track_lin_vel_xy_exp: 0.0319
Episode_Reward/track_ang_vel_z_exp: 0.0222
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0130
     Episode_Reward/dof_torques_l2: -0.0168
         Episode_Reward/dof_acc_l2: -0.0267
     Episode_Reward/action_rate_l2: -0.0131
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0216
  Episode_Termination/base_contact: 6.9583
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 1.01s
                      Time elapsed: 00:00:59
                               ETA: 00:16:07

################################################################################
                      [1m Learning iteration 58/1000 [0m                      

                       Computation: 11902 steps/s (collection: 0.983s, learning 0.049s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.3379
                       Mean reward: -1.23
               Mean episode length: 71.59
Episode_Reward/track_lin_vel_xy_exp: 0.0253
Episode_Reward/track_ang_vel_z_exp: 0.0207
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0123
     Episode_Reward/dof_torques_l2: -0.0158
         Episode_Reward/dof_acc_l2: -0.0254
     Episode_Reward/action_rate_l2: -0.0120
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0208
  Episode_Termination/base_contact: 8.3750
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 724992
                    Iteration time: 1.03s
                      Time elapsed: 00:01:00
                               ETA: 00:16:06

################################################################################
                      [1m Learning iteration 59/1000 [0m                      

                       Computation: 12221 steps/s (collection: 0.960s, learning 0.045s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0092
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.2755
                       Mean reward: -1.11
               Mean episode length: 65.48
Episode_Reward/track_lin_vel_xy_exp: 0.0246
Episode_Reward/track_ang_vel_z_exp: 0.0198
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0120
     Episode_Reward/dof_torques_l2: -0.0152
         Episode_Reward/dof_acc_l2: -0.0237
     Episode_Reward/action_rate_l2: -0.0115
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0196
  Episode_Termination/base_contact: 6.7917
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 1.01s
                      Time elapsed: 00:01:01
                               ETA: 00:16:05

################################################################################
                      [1m Learning iteration 60/1000 [0m                      

                       Computation: 11593 steps/s (collection: 1.008s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0104
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.2249
                       Mean reward: -1.11
               Mean episode length: 72.35
Episode_Reward/track_lin_vel_xy_exp: 0.0234
Episode_Reward/track_ang_vel_z_exp: 0.0200
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0113
     Episode_Reward/dof_torques_l2: -0.0146
         Episode_Reward/dof_acc_l2: -0.0220
     Episode_Reward/action_rate_l2: -0.0110
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0199
  Episode_Termination/base_contact: 8.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 749568
                    Iteration time: 1.06s
                      Time elapsed: 00:01:02
                               ETA: 00:16:05

################################################################################
                      [1m Learning iteration 61/1000 [0m                      

                       Computation: 11710 steps/s (collection: 1.002s, learning 0.047s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0079
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.1376
                       Mean reward: -1.05
               Mean episode length: 68.70
Episode_Reward/track_lin_vel_xy_exp: 0.0232
Episode_Reward/track_ang_vel_z_exp: 0.0186
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0110
     Episode_Reward/dof_torques_l2: -0.0139
         Episode_Reward/dof_acc_l2: -0.0216
     Episode_Reward/action_rate_l2: -0.0104
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0179
  Episode_Termination/base_contact: 8.0417
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 1.05s
                      Time elapsed: 00:01:03
                               ETA: 00:16:04

################################################################################
                      [1m Learning iteration 62/1000 [0m                      

                       Computation: 12299 steps/s (collection: 0.949s, learning 0.050s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0092
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.0412
                       Mean reward: -1.03
               Mean episode length: 62.79
Episode_Reward/track_lin_vel_xy_exp: 0.0226
Episode_Reward/track_ang_vel_z_exp: 0.0175
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0103
     Episode_Reward/dof_torques_l2: -0.0137
         Episode_Reward/dof_acc_l2: -0.0208
     Episode_Reward/action_rate_l2: -0.0101
      Episode_Reward/feet_air_time: -0.0083
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 7.1250
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 774144
                    Iteration time: 1.00s
                      Time elapsed: 00:01:04
                               ETA: 00:16:02

################################################################################
                      [1m Learning iteration 63/1000 [0m                      

                       Computation: 12332 steps/s (collection: 0.950s, learning 0.046s)
             Mean action noise std: 0.71
          Mean value_function loss: 0.0085
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 12.9796
                       Mean reward: -0.99
               Mean episode length: 64.75
Episode_Reward/track_lin_vel_xy_exp: 0.0216
Episode_Reward/track_ang_vel_z_exp: 0.0172
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0101
     Episode_Reward/dof_torques_l2: -0.0131
         Episode_Reward/dof_acc_l2: -0.0195
     Episode_Reward/action_rate_l2: -0.0095
      Episode_Reward/feet_air_time: -0.0078
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0179
  Episode_Termination/base_contact: 7.6250
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 1.00s
                      Time elapsed: 00:01:05
                               ETA: 00:16:01

################################################################################
                      [1m Learning iteration 64/1000 [0m                      

                       Computation: 12166 steps/s (collection: 0.964s, learning 0.046s)
             Mean action noise std: 0.71
          Mean value_function loss: 0.0108
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 12.9088
                       Mean reward: -1.08
               Mean episode length: 67.68
Episode_Reward/track_lin_vel_xy_exp: 0.0235
Episode_Reward/track_ang_vel_z_exp: 0.0190
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0108
     Episode_Reward/dof_torques_l2: -0.0144
         Episode_Reward/dof_acc_l2: -0.0206
     Episode_Reward/action_rate_l2: -0.0104
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 7.9583
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 798720
                    Iteration time: 1.01s
                      Time elapsed: 00:01:06
                               ETA: 00:16:00

################################################################################
                      [1m Learning iteration 65/1000 [0m                      

                       Computation: 11658 steps/s (collection: 1.002s, learning 0.052s)
             Mean action noise std: 0.71
          Mean value_function loss: 0.0092
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 12.8468
                       Mean reward: -0.99
               Mean episode length: 71.86
Episode_Reward/track_lin_vel_xy_exp: 0.0301
Episode_Reward/track_ang_vel_z_exp: 0.0213
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0114
     Episode_Reward/dof_torques_l2: -0.0151
         Episode_Reward/dof_acc_l2: -0.0218
     Episode_Reward/action_rate_l2: -0.0111
      Episode_Reward/feet_air_time: -0.0090
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 6.9167
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 1.05s
                      Time elapsed: 00:01:07
                               ETA: 00:15:59

################################################################################
                      [1m Learning iteration 66/1000 [0m                      

                       Computation: 12183 steps/s (collection: 0.963s, learning 0.045s)
             Mean action noise std: 0.70
          Mean value_function loss: 0.0093
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 12.7942
                       Mean reward: -0.94
               Mean episode length: 64.56
Episode_Reward/track_lin_vel_xy_exp: 0.0246
Episode_Reward/track_ang_vel_z_exp: 0.0180
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0102
     Episode_Reward/dof_torques_l2: -0.0135
         Episode_Reward/dof_acc_l2: -0.0196
     Episode_Reward/action_rate_l2: -0.0098
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0180
  Episode_Termination/base_contact: 8.1667
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 823296
                    Iteration time: 1.01s
                      Time elapsed: 00:01:08
                               ETA: 00:15:58

################################################################################
                      [1m Learning iteration 67/1000 [0m                      

                       Computation: 11938 steps/s (collection: 0.979s, learning 0.051s)
             Mean action noise std: 0.70
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 12.7538
                       Mean reward: -0.91
               Mean episode length: 71.41
Episode_Reward/track_lin_vel_xy_exp: 0.0274
Episode_Reward/track_ang_vel_z_exp: 0.0197
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0105
     Episode_Reward/dof_torques_l2: -0.0143
         Episode_Reward/dof_acc_l2: -0.0213
     Episode_Reward/action_rate_l2: -0.0106
      Episode_Reward/feet_air_time: -0.0089
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0170
  Episode_Termination/base_contact: 7.2083
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 1.03s
                      Time elapsed: 00:01:09
                               ETA: 00:15:57

################################################################################
                      [1m Learning iteration 68/1000 [0m                      

                       Computation: 11729 steps/s (collection: 0.973s, learning 0.075s)
             Mean action noise std: 0.70
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 12.6955
                       Mean reward: -0.91
               Mean episode length: 67.37
Episode_Reward/track_lin_vel_xy_exp: 0.0272
Episode_Reward/track_ang_vel_z_exp: 0.0192
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0103
     Episode_Reward/dof_torques_l2: -0.0137
         Episode_Reward/dof_acc_l2: -0.0206
     Episode_Reward/action_rate_l2: -0.0101
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0164
  Episode_Termination/base_contact: 7.9583
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 847872
                    Iteration time: 1.05s
                      Time elapsed: 00:01:10
                               ETA: 00:15:56

################################################################################
                      [1m Learning iteration 69/1000 [0m                      

                       Computation: 12382 steps/s (collection: 0.948s, learning 0.044s)
             Mean action noise std: 0.70
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 12.6550
                       Mean reward: -0.89
               Mean episode length: 65.74
Episode_Reward/track_lin_vel_xy_exp: 0.0254
Episode_Reward/track_ang_vel_z_exp: 0.0181
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0098
     Episode_Reward/dof_torques_l2: -0.0131
         Episode_Reward/dof_acc_l2: -0.0191
     Episode_Reward/action_rate_l2: -0.0096
      Episode_Reward/feet_air_time: -0.0081
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0170
  Episode_Termination/base_contact: 6.7917
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 0.99s
                      Time elapsed: 00:01:11
                               ETA: 00:15:55

################################################################################
                      [1m Learning iteration 70/1000 [0m                      

                       Computation: 12090 steps/s (collection: 0.968s, learning 0.048s)
             Mean action noise std: 0.69
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 12.6182
                       Mean reward: -0.86
               Mean episode length: 67.07
Episode_Reward/track_lin_vel_xy_exp: 0.0276
Episode_Reward/track_ang_vel_z_exp: 0.0192
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0100
     Episode_Reward/dof_torques_l2: -0.0138
         Episode_Reward/dof_acc_l2: -0.0202
     Episode_Reward/action_rate_l2: -0.0099
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0156
  Episode_Termination/base_contact: 7.5417
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 872448
                    Iteration time: 1.02s
                      Time elapsed: 00:01:12
                               ETA: 00:15:53

################################################################################
                      [1m Learning iteration 71/1000 [0m                      

                       Computation: 11255 steps/s (collection: 1.037s, learning 0.054s)
             Mean action noise std: 0.69
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 12.5561
                       Mean reward: -0.90
               Mean episode length: 67.03
Episode_Reward/track_lin_vel_xy_exp: 0.0270
Episode_Reward/track_ang_vel_z_exp: 0.0196
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0101
     Episode_Reward/dof_torques_l2: -0.0140
         Episode_Reward/dof_acc_l2: -0.0202
     Episode_Reward/action_rate_l2: -0.0100
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0166
  Episode_Termination/base_contact: 7.3333
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 1.09s
                      Time elapsed: 00:01:13
                               ETA: 00:15:53

################################################################################
                      [1m Learning iteration 72/1000 [0m                      

                       Computation: 11991 steps/s (collection: 0.975s, learning 0.050s)
             Mean action noise std: 0.69
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 12.4875
                       Mean reward: -0.87
               Mean episode length: 68.71
Episode_Reward/track_lin_vel_xy_exp: 0.0276
Episode_Reward/track_ang_vel_z_exp: 0.0200
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0101
     Episode_Reward/dof_torques_l2: -0.0141
         Episode_Reward/dof_acc_l2: -0.0199
     Episode_Reward/action_rate_l2: -0.0102
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 6.5000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 897024
                    Iteration time: 1.02s
                      Time elapsed: 00:01:14
                               ETA: 00:15:52

################################################################################
                      [1m Learning iteration 73/1000 [0m                      

                       Computation: 12042 steps/s (collection: 0.975s, learning 0.046s)
             Mean action noise std: 0.68
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 12.4344
                       Mean reward: -0.90
               Mean episode length: 74.91
Episode_Reward/track_lin_vel_xy_exp: 0.0285
Episode_Reward/track_ang_vel_z_exp: 0.0202
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0102
     Episode_Reward/dof_torques_l2: -0.0142
         Episode_Reward/dof_acc_l2: -0.0202
     Episode_Reward/action_rate_l2: -0.0102
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0164
  Episode_Termination/base_contact: 7.4167
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 1.02s
                      Time elapsed: 00:01:15
                               ETA: 00:15:51

################################################################################
                      [1m Learning iteration 74/1000 [0m                      

                       Computation: 11828 steps/s (collection: 0.994s, learning 0.045s)
             Mean action noise std: 0.68
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 12.4083
                       Mean reward: -0.81
               Mean episode length: 67.77
Episode_Reward/track_lin_vel_xy_exp: 0.0276
Episode_Reward/track_ang_vel_z_exp: 0.0190
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0099
     Episode_Reward/dof_torques_l2: -0.0133
         Episode_Reward/dof_acc_l2: -0.0185
     Episode_Reward/action_rate_l2: -0.0095
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0159
  Episode_Termination/base_contact: 6.6667
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 921600
                    Iteration time: 1.04s
                      Time elapsed: 00:01:17
                               ETA: 00:15:50

################################################################################
                      [1m Learning iteration 75/1000 [0m                      

                       Computation: 12099 steps/s (collection: 0.970s, learning 0.045s)
             Mean action noise std: 0.68
          Mean value_function loss: 0.0101
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 12.3906
                       Mean reward: -0.87
               Mean episode length: 68.86
Episode_Reward/track_lin_vel_xy_exp: 0.0252
Episode_Reward/track_ang_vel_z_exp: 0.0194
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0096
     Episode_Reward/dof_torques_l2: -0.0131
         Episode_Reward/dof_acc_l2: -0.0184
     Episode_Reward/action_rate_l2: -0.0094
      Episode_Reward/feet_air_time: -0.0082
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 6.8750
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 1.02s
                      Time elapsed: 00:01:18
                               ETA: 00:15:49

################################################################################
                      [1m Learning iteration 76/1000 [0m                      

                       Computation: 12021 steps/s (collection: 0.971s, learning 0.051s)
             Mean action noise std: 0.68
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 12.3562
                       Mean reward: -0.83
               Mean episode length: 73.60
Episode_Reward/track_lin_vel_xy_exp: 0.0315
Episode_Reward/track_ang_vel_z_exp: 0.0199
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0103
     Episode_Reward/dof_torques_l2: -0.0141
         Episode_Reward/dof_acc_l2: -0.0201
     Episode_Reward/action_rate_l2: -0.0102
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 6.2083
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 946176
                    Iteration time: 1.02s
                      Time elapsed: 00:01:19
                               ETA: 00:15:48

################################################################################
                      [1m Learning iteration 77/1000 [0m                      

                       Computation: 11538 steps/s (collection: 1.018s, learning 0.047s)
             Mean action noise std: 0.67
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 12.2539
                       Mean reward: -0.83
               Mean episode length: 65.92
Episode_Reward/track_lin_vel_xy_exp: 0.0276
Episode_Reward/track_ang_vel_z_exp: 0.0199
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0099
     Episode_Reward/dof_torques_l2: -0.0141
         Episode_Reward/dof_acc_l2: -0.0197
     Episode_Reward/action_rate_l2: -0.0099
      Episode_Reward/feet_air_time: -0.0085
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0149
  Episode_Termination/base_contact: 7.1667
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 1.06s
                      Time elapsed: 00:01:20
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 78/1000 [0m                      

                       Computation: 11298 steps/s (collection: 1.043s, learning 0.044s)
             Mean action noise std: 0.67
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 12.1707
                       Mean reward: -0.83
               Mean episode length: 78.40
Episode_Reward/track_lin_vel_xy_exp: 0.0307
Episode_Reward/track_ang_vel_z_exp: 0.0212
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0106
     Episode_Reward/dof_torques_l2: -0.0142
         Episode_Reward/dof_acc_l2: -0.0196
     Episode_Reward/action_rate_l2: -0.0103
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0164
  Episode_Termination/base_contact: 5.8750
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 970752
                    Iteration time: 1.09s
                      Time elapsed: 00:01:21
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 79/1000 [0m                      

                       Computation: 10865 steps/s (collection: 1.084s, learning 0.047s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 12.1106
                       Mean reward: -0.73
               Mean episode length: 79.40
Episode_Reward/track_lin_vel_xy_exp: 0.0333
Episode_Reward/track_ang_vel_z_exp: 0.0213
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0104
     Episode_Reward/dof_torques_l2: -0.0148
         Episode_Reward/dof_acc_l2: -0.0195
     Episode_Reward/action_rate_l2: -0.0105
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 6.4167
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.13s
                      Time elapsed: 00:01:22
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 80/1000 [0m                      

                       Computation: 11272 steps/s (collection: 1.029s, learning 0.061s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 12.0445
                       Mean reward: -0.76
               Mean episode length: 82.16
Episode_Reward/track_lin_vel_xy_exp: 0.0314
Episode_Reward/track_ang_vel_z_exp: 0.0210
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0106
     Episode_Reward/dof_torques_l2: -0.0141
         Episode_Reward/dof_acc_l2: -0.0191
     Episode_Reward/action_rate_l2: -0.0099
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0159
  Episode_Termination/base_contact: 6.6250
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 995328
                    Iteration time: 1.09s
                      Time elapsed: 00:01:23
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 81/1000 [0m                      

                       Computation: 10943 steps/s (collection: 1.071s, learning 0.052s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 11.9790
                       Mean reward: -0.82
               Mean episode length: 74.03
Episode_Reward/track_lin_vel_xy_exp: 0.0287
Episode_Reward/track_ang_vel_z_exp: 0.0203
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0100
     Episode_Reward/dof_torques_l2: -0.0145
         Episode_Reward/dof_acc_l2: -0.0182
     Episode_Reward/action_rate_l2: -0.0099
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 5.4583
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 1.12s
                      Time elapsed: 00:01:24
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 82/1000 [0m                      

                       Computation: 10901 steps/s (collection: 1.082s, learning 0.045s)
             Mean action noise std: 0.66
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 11.9376
                       Mean reward: -0.78
               Mean episode length: 74.72
Episode_Reward/track_lin_vel_xy_exp: 0.0309
Episode_Reward/track_ang_vel_z_exp: 0.0214
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0100
     Episode_Reward/dof_torques_l2: -0.0144
         Episode_Reward/dof_acc_l2: -0.0186
     Episode_Reward/action_rate_l2: -0.0100
      Episode_Reward/feet_air_time: -0.0086
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 5.8750
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1019904
                    Iteration time: 1.13s
                      Time elapsed: 00:01:25
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 83/1000 [0m                      

                       Computation: 10877 steps/s (collection: 1.085s, learning 0.045s)
             Mean action noise std: 0.65
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 11.9101
                       Mean reward: -0.79
               Mean episode length: 82.48
Episode_Reward/track_lin_vel_xy_exp: 0.0338
Episode_Reward/track_ang_vel_z_exp: 0.0236
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0107
     Episode_Reward/dof_torques_l2: -0.0156
         Episode_Reward/dof_acc_l2: -0.0202
     Episode_Reward/action_rate_l2: -0.0109
      Episode_Reward/feet_air_time: -0.0096
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 6.2917
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 1.13s
                      Time elapsed: 00:01:26
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 84/1000 [0m                      

                       Computation: 11091 steps/s (collection: 1.048s, learning 0.060s)
             Mean action noise std: 0.65
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 11.8697
                       Mean reward: -0.77
               Mean episode length: 82.08
Episode_Reward/track_lin_vel_xy_exp: 0.0336
Episode_Reward/track_ang_vel_z_exp: 0.0216
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0102
     Episode_Reward/dof_torques_l2: -0.0154
         Episode_Reward/dof_acc_l2: -0.0188
     Episode_Reward/action_rate_l2: -0.0106
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 5.5833
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1044480
                    Iteration time: 1.11s
                      Time elapsed: 00:01:27
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 85/1000 [0m                      

                       Computation: 10509 steps/s (collection: 1.119s, learning 0.051s)
             Mean action noise std: 0.65
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 11.8130
                       Mean reward: -0.74
               Mean episode length: 72.21
Episode_Reward/track_lin_vel_xy_exp: 0.0281
Episode_Reward/track_ang_vel_z_exp: 0.0211
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0098
     Episode_Reward/dof_torques_l2: -0.0145
         Episode_Reward/dof_acc_l2: -0.0186
     Episode_Reward/action_rate_l2: -0.0098
      Episode_Reward/feet_air_time: -0.0088
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 5.9167
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 1.17s
                      Time elapsed: 00:01:29
                               ETA: 00:15:47

################################################################################
                      [1m Learning iteration 86/1000 [0m                      

                       Computation: 11659 steps/s (collection: 1.009s, learning 0.045s)
             Mean action noise std: 0.65
          Mean value_function loss: 0.0073
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 11.7609
                       Mean reward: -0.73
               Mean episode length: 72.11
Episode_Reward/track_lin_vel_xy_exp: 0.0315
Episode_Reward/track_ang_vel_z_exp: 0.0222
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0100
     Episode_Reward/dof_torques_l2: -0.0147
         Episode_Reward/dof_acc_l2: -0.0181
     Episode_Reward/action_rate_l2: -0.0100
      Episode_Reward/feet_air_time: -0.0087
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 4.4167
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1069056
                    Iteration time: 1.05s
                      Time elapsed: 00:01:30
                               ETA: 00:15:46

################################################################################
                      [1m Learning iteration 87/1000 [0m                      

                       Computation: 10927 steps/s (collection: 1.080s, learning 0.044s)
             Mean action noise std: 0.64
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 11.7294
                       Mean reward: -0.66
               Mean episode length: 85.69
Episode_Reward/track_lin_vel_xy_exp: 0.0359
Episode_Reward/track_ang_vel_z_exp: 0.0229
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0104
     Episode_Reward/dof_torques_l2: -0.0161
         Episode_Reward/dof_acc_l2: -0.0182
     Episode_Reward/action_rate_l2: -0.0107
      Episode_Reward/feet_air_time: -0.0094
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0156
  Episode_Termination/base_contact: 4.5417
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.12s
                      Time elapsed: 00:01:31
                               ETA: 00:15:46

################################################################################
                      [1m Learning iteration 88/1000 [0m                      

                       Computation: 11281 steps/s (collection: 1.044s, learning 0.045s)
             Mean action noise std: 0.64
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 11.6824
                       Mean reward: -0.75
               Mean episode length: 83.17
Episode_Reward/track_lin_vel_xy_exp: 0.0302
Episode_Reward/track_ang_vel_z_exp: 0.0218
       Episode_Reward/lin_vel_z_l2: -0.0107
      Episode_Reward/ang_vel_xy_l2: -0.0103
     Episode_Reward/dof_torques_l2: -0.0150
         Episode_Reward/dof_acc_l2: -0.0189
     Episode_Reward/action_rate_l2: -0.0103
      Episode_Reward/feet_air_time: -0.0092
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 4.6250
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1093632
                    Iteration time: 1.09s
                      Time elapsed: 00:01:32
                               ETA: 00:15:46

################################################################################
                      [1m Learning iteration 89/1000 [0m                      

                       Computation: 11338 steps/s (collection: 1.037s, learning 0.047s)
             Mean action noise std: 0.64
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 11.6302
                       Mean reward: -0.81
               Mean episode length: 93.46
Episode_Reward/track_lin_vel_xy_exp: 0.0365
Episode_Reward/track_ang_vel_z_exp: 0.0267
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0115
     Episode_Reward/dof_torques_l2: -0.0180
         Episode_Reward/dof_acc_l2: -0.0207
     Episode_Reward/action_rate_l2: -0.0124
      Episode_Reward/feet_air_time: -0.0104
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0206
  Episode_Termination/base_contact: 4.4583
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 1.08s
                      Time elapsed: 00:01:33
                               ETA: 00:15:45

################################################################################
                      [1m Learning iteration 90/1000 [0m                      

                       Computation: 11253 steps/s (collection: 1.031s, learning 0.061s)
             Mean action noise std: 0.64
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 11.5771
                       Mean reward: -0.65
               Mean episode length: 95.03
Episode_Reward/track_lin_vel_xy_exp: 0.0387
Episode_Reward/track_ang_vel_z_exp: 0.0269
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0108
     Episode_Reward/dof_torques_l2: -0.0181
         Episode_Reward/dof_acc_l2: -0.0202
     Episode_Reward/action_rate_l2: -0.0122
      Episode_Reward/feet_air_time: -0.0106
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0152
  Episode_Termination/base_contact: 4.5833
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1118208
                    Iteration time: 1.09s
                      Time elapsed: 00:01:34
                               ETA: 00:15:45

################################################################################
                      [1m Learning iteration 91/1000 [0m                      

                       Computation: 10661 steps/s (collection: 1.107s, learning 0.045s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0071
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 11.5399
                       Mean reward: -0.65
               Mean episode length: 102.32
Episode_Reward/track_lin_vel_xy_exp: 0.0445
Episode_Reward/track_ang_vel_z_exp: 0.0285
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0112
     Episode_Reward/dof_torques_l2: -0.0191
         Episode_Reward/dof_acc_l2: -0.0218
     Episode_Reward/action_rate_l2: -0.0131
      Episode_Reward/feet_air_time: -0.0116
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 3.2500
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 1.15s
                      Time elapsed: 00:01:35
                               ETA: 00:15:45

################################################################################
                      [1m Learning iteration 92/1000 [0m                      

                       Computation: 10938 steps/s (collection: 1.077s, learning 0.046s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 11.5027
                       Mean reward: -0.66
               Mean episode length: 96.69
Episode_Reward/track_lin_vel_xy_exp: 0.0395
Episode_Reward/track_ang_vel_z_exp: 0.0284
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0107
     Episode_Reward/dof_torques_l2: -0.0187
         Episode_Reward/dof_acc_l2: -0.0203
     Episode_Reward/action_rate_l2: -0.0124
      Episode_Reward/feet_air_time: -0.0106
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0159
  Episode_Termination/base_contact: 4.0000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1142784
                    Iteration time: 1.12s
                      Time elapsed: 00:01:36
                               ETA: 00:15:45

################################################################################
                      [1m Learning iteration 93/1000 [0m                      

                       Computation: 10895 steps/s (collection: 1.058s, learning 0.070s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0066
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 11.4549
                       Mean reward: -0.72
               Mean episode length: 106.31
Episode_Reward/track_lin_vel_xy_exp: 0.0412
Episode_Reward/track_ang_vel_z_exp: 0.0297
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0120
     Episode_Reward/dof_torques_l2: -0.0197
         Episode_Reward/dof_acc_l2: -0.0217
     Episode_Reward/action_rate_l2: -0.0135
      Episode_Reward/feet_air_time: -0.0111
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0178
  Episode_Termination/base_contact: 3.8750
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 1.13s
                      Time elapsed: 00:01:37
                               ETA: 00:15:44

################################################################################
                      [1m Learning iteration 94/1000 [0m                      

                       Computation: 10933 steps/s (collection: 1.059s, learning 0.065s)
             Mean action noise std: 0.63
          Mean value_function loss: 0.0068
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 11.3771
                       Mean reward: -0.75
               Mean episode length: 109.91
Episode_Reward/track_lin_vel_xy_exp: 0.0415
Episode_Reward/track_ang_vel_z_exp: 0.0324
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0119
     Episode_Reward/dof_torques_l2: -0.0200
         Episode_Reward/dof_acc_l2: -0.0202
     Episode_Reward/action_rate_l2: -0.0136
      Episode_Reward/feet_air_time: -0.0100
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0165
  Episode_Termination/base_contact: 2.7917
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1167360
                    Iteration time: 1.12s
                      Time elapsed: 00:01:39
                               ETA: 00:15:44

################################################################################
                      [1m Learning iteration 95/1000 [0m                      

                       Computation: 10935 steps/s (collection: 1.078s, learning 0.046s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 11.3233
                       Mean reward: -0.76
               Mean episode length: 114.74
Episode_Reward/track_lin_vel_xy_exp: 0.0408
Episode_Reward/track_ang_vel_z_exp: 0.0320
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0211
         Episode_Reward/dof_acc_l2: -0.0236
     Episode_Reward/action_rate_l2: -0.0145
      Episode_Reward/feet_air_time: -0.0113
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0213
  Episode_Termination/base_contact: 3.2917
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.12s
                      Time elapsed: 00:01:40
                               ETA: 00:15:44

################################################################################
                      [1m Learning iteration 96/1000 [0m                      

                       Computation: 10766 steps/s (collection: 1.095s, learning 0.046s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 11.2431
                       Mean reward: -0.82
               Mean episode length: 107.77
Episode_Reward/track_lin_vel_xy_exp: 0.0334
Episode_Reward/track_ang_vel_z_exp: 0.0266
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0109
     Episode_Reward/dof_torques_l2: -0.0191
         Episode_Reward/dof_acc_l2: -0.0187
     Episode_Reward/action_rate_l2: -0.0124
      Episode_Reward/feet_air_time: -0.0108
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0169
  Episode_Termination/base_contact: 2.1667
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1191936
                    Iteration time: 1.14s
                      Time elapsed: 00:01:41
                               ETA: 00:15:44

################################################################################
                      [1m Learning iteration 97/1000 [0m                      

                       Computation: 10941 steps/s (collection: 1.077s, learning 0.046s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 11.1879
                       Mean reward: -0.59
               Mean episode length: 126.57
Episode_Reward/track_lin_vel_xy_exp: 0.0696
Episode_Reward/track_ang_vel_z_exp: 0.0469
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0152
     Episode_Reward/dof_torques_l2: -0.0278
         Episode_Reward/dof_acc_l2: -0.0263
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0157
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 2.8333
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 1.12s
                      Time elapsed: 00:01:42
                               ETA: 00:15:43

################################################################################
                      [1m Learning iteration 98/1000 [0m                      

                       Computation: 10861 steps/s (collection: 1.087s, learning 0.044s)
             Mean action noise std: 0.62
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 11.1621
                       Mean reward: -0.66
               Mean episode length: 132.76
Episode_Reward/track_lin_vel_xy_exp: 0.0293
Episode_Reward/track_ang_vel_z_exp: 0.0310
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0115
     Episode_Reward/dof_torques_l2: -0.0217
         Episode_Reward/dof_acc_l2: -0.0214
     Episode_Reward/action_rate_l2: -0.0139
      Episode_Reward/feet_air_time: -0.0119
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0221
  Episode_Termination/base_contact: 2.3333
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1216512
                    Iteration time: 1.13s
                      Time elapsed: 00:01:43
                               ETA: 00:15:43

################################################################################
                      [1m Learning iteration 99/1000 [0m                      

                       Computation: 10919 steps/s (collection: 1.069s, learning 0.056s)
             Mean action noise std: 0.61
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 11.1326
                       Mean reward: -0.76
               Mean episode length: 136.48
Episode_Reward/track_lin_vel_xy_exp: 0.0559
Episode_Reward/track_ang_vel_z_exp: 0.0442
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0154
     Episode_Reward/dof_torques_l2: -0.0266
         Episode_Reward/dof_acc_l2: -0.0256
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0136
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0208
  Episode_Termination/base_contact: 2.4583
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 1.13s
                      Time elapsed: 00:01:44
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 100/1000 [0m                      

                       Computation: 11007 steps/s (collection: 1.052s, learning 0.065s)
             Mean action noise std: 0.61
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 11.0532
                       Mean reward: -0.88
               Mean episode length: 142.60
Episode_Reward/track_lin_vel_xy_exp: 0.0561
Episode_Reward/track_ang_vel_z_exp: 0.0426
       Episode_Reward/lin_vel_z_l2: -0.0122
      Episode_Reward/ang_vel_xy_l2: -0.0159
     Episode_Reward/dof_torques_l2: -0.0332
         Episode_Reward/dof_acc_l2: -0.0310
     Episode_Reward/action_rate_l2: -0.0209
      Episode_Reward/feet_air_time: -0.0175
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0278
  Episode_Termination/base_contact: 1.8333
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1241088
                    Iteration time: 1.12s
                      Time elapsed: 00:01:45
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 101/1000 [0m                      

                       Computation: 10965 steps/s (collection: 1.071s, learning 0.049s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 10.9804
                       Mean reward: -0.79
               Mean episode length: 141.14
Episode_Reward/track_lin_vel_xy_exp: 0.0508
Episode_Reward/track_ang_vel_z_exp: 0.0396
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0139
     Episode_Reward/dof_torques_l2: -0.0252
         Episode_Reward/dof_acc_l2: -0.0241
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0124
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0174
  Episode_Termination/base_contact: 1.6667
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 1.12s
                      Time elapsed: 00:01:46
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 102/1000 [0m                      

                       Computation: 10834 steps/s (collection: 1.086s, learning 0.048s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 10.8925
                       Mean reward: -0.86
               Mean episode length: 139.08
Episode_Reward/track_lin_vel_xy_exp: 0.0429
Episode_Reward/track_ang_vel_z_exp: 0.0393
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0133
     Episode_Reward/dof_torques_l2: -0.0241
         Episode_Reward/dof_acc_l2: -0.0240
     Episode_Reward/action_rate_l2: -0.0159
      Episode_Reward/feet_air_time: -0.0134
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0190
  Episode_Termination/base_contact: 1.5833
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1265664
                    Iteration time: 1.13s
                      Time elapsed: 00:01:48
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 103/1000 [0m                      

                       Computation: 10465 steps/s (collection: 1.130s, learning 0.044s)
             Mean action noise std: 0.60
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 10.8068
                       Mean reward: -0.90
               Mean episode length: 157.88
Episode_Reward/track_lin_vel_xy_exp: 0.0562
Episode_Reward/track_ang_vel_z_exp: 0.0455
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0156
     Episode_Reward/dof_torques_l2: -0.0330
         Episode_Reward/dof_acc_l2: -0.0281
     Episode_Reward/action_rate_l2: -0.0206
      Episode_Reward/feet_air_time: -0.0161
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: -0.0195
  Episode_Termination/base_contact: 1.8333
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.17s
                      Time elapsed: 00:01:49
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 104/1000 [0m                      

                       Computation: 11132 steps/s (collection: 1.058s, learning 0.045s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 10.7593
                       Mean reward: -0.73
               Mean episode length: 167.00
Episode_Reward/track_lin_vel_xy_exp: 0.0956
Episode_Reward/track_ang_vel_z_exp: 0.0575
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0311
         Episode_Reward/dof_acc_l2: -0.0294
     Episode_Reward/action_rate_l2: -0.0214
      Episode_Reward/feet_air_time: -0.0165
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: -0.0212
  Episode_Termination/base_contact: 1.3333
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1290240
                    Iteration time: 1.10s
                      Time elapsed: 00:01:50
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 105/1000 [0m                      

                       Computation: 11060 steps/s (collection: 1.061s, learning 0.050s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 10.6829
                       Mean reward: -0.68
               Mean episode length: 178.45
Episode_Reward/track_lin_vel_xy_exp: 0.0461
Episode_Reward/track_ang_vel_z_exp: 0.0416
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0150
     Episode_Reward/dof_torques_l2: -0.0272
         Episode_Reward/dof_acc_l2: -0.0278
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0146
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 1.4167
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 1.11s
                      Time elapsed: 00:01:51
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 106/1000 [0m                      

                       Computation: 11018 steps/s (collection: 1.069s, learning 0.047s)
             Mean action noise std: 0.59
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 10.6054
                       Mean reward: -0.65
               Mean episode length: 199.51
Episode_Reward/track_lin_vel_xy_exp: 0.0729
Episode_Reward/track_ang_vel_z_exp: 0.0697
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.0372
         Episode_Reward/dof_acc_l2: -0.0329
     Episode_Reward/action_rate_l2: -0.0246
      Episode_Reward/feet_air_time: -0.0199
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0241
  Episode_Termination/base_contact: 1.6667
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1314816
                    Iteration time: 1.12s
                      Time elapsed: 00:01:52
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 107/1000 [0m                      

                       Computation: 10618 steps/s (collection: 1.063s, learning 0.095s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 10.5250
                       Mean reward: -0.62
               Mean episode length: 213.70
Episode_Reward/track_lin_vel_xy_exp: 0.0894
Episode_Reward/track_ang_vel_z_exp: 0.0774
       Episode_Reward/lin_vel_z_l2: -0.0154
      Episode_Reward/ang_vel_xy_l2: -0.0229
     Episode_Reward/dof_torques_l2: -0.0454
         Episode_Reward/dof_acc_l2: -0.0385
     Episode_Reward/action_rate_l2: -0.0309
      Episode_Reward/feet_air_time: -0.0240
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: -0.0219
  Episode_Termination/base_contact: 1.7500
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 1.16s
                      Time elapsed: 00:01:53
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 108/1000 [0m                      

                       Computation: 10294 steps/s (collection: 1.088s, learning 0.105s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 10.4677
                       Mean reward: -0.52
               Mean episode length: 219.16
Episode_Reward/track_lin_vel_xy_exp: 0.1000
Episode_Reward/track_ang_vel_z_exp: 0.0778
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.0459
         Episode_Reward/dof_acc_l2: -0.0341
     Episode_Reward/action_rate_l2: -0.0288
      Episode_Reward/feet_air_time: -0.0230
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 1.5000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1339392
                    Iteration time: 1.19s
                      Time elapsed: 00:01:54
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 109/1000 [0m                      

                       Computation: 11507 steps/s (collection: 1.023s, learning 0.045s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 10.4126
                       Mean reward: -0.77
               Mean episode length: 237.65
Episode_Reward/track_lin_vel_xy_exp: 0.0757
Episode_Reward/track_ang_vel_z_exp: 0.0660
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0218
     Episode_Reward/dof_torques_l2: -0.0430
         Episode_Reward/dof_acc_l2: -0.0407
     Episode_Reward/action_rate_l2: -0.0292
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0248
  Episode_Termination/base_contact: 1.5417
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 1.07s
                      Time elapsed: 00:01:55
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 110/1000 [0m                      

                       Computation: 10857 steps/s (collection: 1.062s, learning 0.070s)
             Mean action noise std: 0.58
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 10.3544
                       Mean reward: -1.13
               Mean episode length: 246.66
Episode_Reward/track_lin_vel_xy_exp: 0.0552
Episode_Reward/track_ang_vel_z_exp: 0.0439
       Episode_Reward/lin_vel_z_l2: -0.0138
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.0504
         Episode_Reward/dof_acc_l2: -0.0333
     Episode_Reward/action_rate_l2: -0.0280
      Episode_Reward/feet_air_time: -0.0215
 Episode_Reward/undesired_contacts: -0.0148
Episode_Reward/flat_orientation_l2: -0.0256
  Episode_Termination/base_contact: 1.5000
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1363968
                    Iteration time: 1.13s
                      Time elapsed: 00:01:57
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 111/1000 [0m                      

                       Computation: 11379 steps/s (collection: 1.007s, learning 0.073s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 10.3219
                       Mean reward: -0.97
               Mean episode length: 271.60
Episode_Reward/track_lin_vel_xy_exp: 0.1347
Episode_Reward/track_ang_vel_z_exp: 0.0959
       Episode_Reward/lin_vel_z_l2: -0.0175
      Episode_Reward/ang_vel_xy_l2: -0.0272
     Episode_Reward/dof_torques_l2: -0.0591
         Episode_Reward/dof_acc_l2: -0.0459
     Episode_Reward/action_rate_l2: -0.0378
      Episode_Reward/feet_air_time: -0.0301
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: -0.0262
  Episode_Termination/base_contact: 1.6667
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.08s
                      Time elapsed: 00:01:58
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 112/1000 [0m                      

                       Computation: 10875 steps/s (collection: 1.082s, learning 0.048s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 10.2700
                       Mean reward: -0.82
               Mean episode length: 281.22
Episode_Reward/track_lin_vel_xy_exp: 0.0936
Episode_Reward/track_ang_vel_z_exp: 0.0555
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.0369
         Episode_Reward/dof_acc_l2: -0.0301
     Episode_Reward/action_rate_l2: -0.0234
      Episode_Reward/feet_air_time: -0.0157
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: -0.0207
  Episode_Termination/base_contact: 1.4167
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1388544
                    Iteration time: 1.13s
                      Time elapsed: 00:01:59
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 113/1000 [0m                      

                       Computation: 10772 steps/s (collection: 1.089s, learning 0.052s)
             Mean action noise std: 0.57
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 10.1872
                       Mean reward: -0.78
               Mean episode length: 300.92
Episode_Reward/track_lin_vel_xy_exp: 0.1270
Episode_Reward/track_ang_vel_z_exp: 0.0821
       Episode_Reward/lin_vel_z_l2: -0.0151
      Episode_Reward/ang_vel_xy_l2: -0.0249
     Episode_Reward/dof_torques_l2: -0.0582
         Episode_Reward/dof_acc_l2: -0.0414
     Episode_Reward/action_rate_l2: -0.0352
      Episode_Reward/feet_air_time: -0.0239
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: -0.0278
  Episode_Termination/base_contact: 1.5417
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 1.14s
                      Time elapsed: 00:02:00
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 114/1000 [0m                      

                       Computation: 11565 steps/s (collection: 1.017s, learning 0.045s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 10.1154
                       Mean reward: -0.56
               Mean episode length: 337.19
Episode_Reward/track_lin_vel_xy_exp: 0.2309
Episode_Reward/track_ang_vel_z_exp: 0.1124
       Episode_Reward/lin_vel_z_l2: -0.0210
      Episode_Reward/ang_vel_xy_l2: -0.0386
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.0610
     Episode_Reward/action_rate_l2: -0.0544
      Episode_Reward/feet_air_time: -0.0312
 Episode_Reward/undesired_contacts: -0.0104
Episode_Reward/flat_orientation_l2: -0.0313
  Episode_Termination/base_contact: 1.3333
      Episode_Termination/time_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1413120
                    Iteration time: 1.06s
                      Time elapsed: 00:02:01
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 115/1000 [0m                      

                       Computation: 11234 steps/s (collection: 1.034s, learning 0.060s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 10.0448
                       Mean reward: -0.51
               Mean episode length: 354.23
Episode_Reward/track_lin_vel_xy_exp: 0.0771
Episode_Reward/track_ang_vel_z_exp: 0.0819
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0267
     Episode_Reward/dof_torques_l2: -0.0586
         Episode_Reward/dof_acc_l2: -0.0431
     Episode_Reward/action_rate_l2: -0.0351
      Episode_Reward/feet_air_time: -0.0267
 Episode_Reward/undesired_contacts: -0.0112
Episode_Reward/flat_orientation_l2: -0.0252
  Episode_Termination/base_contact: 1.6667
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 1.09s
                      Time elapsed: 00:02:02
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 116/1000 [0m                      

                       Computation: 10703 steps/s (collection: 1.101s, learning 0.047s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 10.0049
                       Mean reward: -0.66
               Mean episode length: 427.59
Episode_Reward/track_lin_vel_xy_exp: 0.1334
Episode_Reward/track_ang_vel_z_exp: 0.1293
       Episode_Reward/lin_vel_z_l2: -0.0212
      Episode_Reward/ang_vel_xy_l2: -0.0377
     Episode_Reward/dof_torques_l2: -0.0856
         Episode_Reward/dof_acc_l2: -0.0630
     Episode_Reward/action_rate_l2: -0.0541
      Episode_Reward/feet_air_time: -0.0395
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: -0.0310
  Episode_Termination/base_contact: 1.5000
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1437696
                    Iteration time: 1.15s
                      Time elapsed: 00:02:03
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 117/1000 [0m                      

                       Computation: 10990 steps/s (collection: 1.070s, learning 0.048s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 9.9867
                       Mean reward: -0.84
               Mean episode length: 427.50
Episode_Reward/track_lin_vel_xy_exp: 0.1201
Episode_Reward/track_ang_vel_z_exp: 0.1379
       Episode_Reward/lin_vel_z_l2: -0.0192
      Episode_Reward/ang_vel_xy_l2: -0.0328
     Episode_Reward/dof_torques_l2: -0.0796
         Episode_Reward/dof_acc_l2: -0.0501
     Episode_Reward/action_rate_l2: -0.0466
      Episode_Reward/feet_air_time: -0.0314
 Episode_Reward/undesired_contacts: -0.0107
Episode_Reward/flat_orientation_l2: -0.0265
  Episode_Termination/base_contact: 1.4583
      Episode_Termination/time_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 1.12s
                      Time elapsed: 00:02:04
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 118/1000 [0m                      

                       Computation: 11102 steps/s (collection: 1.056s, learning 0.051s)
             Mean action noise std: 0.56
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 9.9445
                       Mean reward: -0.77
               Mean episode length: 418.50
Episode_Reward/track_lin_vel_xy_exp: 0.1131
Episode_Reward/track_ang_vel_z_exp: 0.1093
       Episode_Reward/lin_vel_z_l2: -0.0186
      Episode_Reward/ang_vel_xy_l2: -0.0323
     Episode_Reward/dof_torques_l2: -0.0682
         Episode_Reward/dof_acc_l2: -0.0526
     Episode_Reward/action_rate_l2: -0.0433
      Episode_Reward/feet_air_time: -0.0328
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: -0.0285
  Episode_Termination/base_contact: 1.9583
      Episode_Termination/time_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1462272
                    Iteration time: 1.11s
                      Time elapsed: 00:02:05
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 119/1000 [0m                      

                       Computation: 10799 steps/s (collection: 1.080s, learning 0.057s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 9.9108
                       Mean reward: -0.50
               Mean episode length: 388.97
Episode_Reward/track_lin_vel_xy_exp: 0.1605
Episode_Reward/track_ang_vel_z_exp: 0.1123
       Episode_Reward/lin_vel_z_l2: -0.0183
      Episode_Reward/ang_vel_xy_l2: -0.0305
     Episode_Reward/dof_torques_l2: -0.0744
         Episode_Reward/dof_acc_l2: -0.0496
     Episode_Reward/action_rate_l2: -0.0440
      Episode_Reward/feet_air_time: -0.0310
 Episode_Reward/undesired_contacts: -0.0083
Episode_Reward/flat_orientation_l2: -0.0266
  Episode_Termination/base_contact: 1.3333
      Episode_Termination/time_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.14s
                      Time elapsed: 00:02:07
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 120/1000 [0m                      

                       Computation: 10889 steps/s (collection: 1.082s, learning 0.046s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 9.8699
                       Mean reward: -0.65
               Mean episode length: 410.32
Episode_Reward/track_lin_vel_xy_exp: 0.1013
Episode_Reward/track_ang_vel_z_exp: 0.1355
       Episode_Reward/lin_vel_z_l2: -0.0211
      Episode_Reward/ang_vel_xy_l2: -0.0361
     Episode_Reward/dof_torques_l2: -0.0748
         Episode_Reward/dof_acc_l2: -0.0614
     Episode_Reward/action_rate_l2: -0.0491
      Episode_Reward/feet_air_time: -0.0387
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: -0.0292
  Episode_Termination/base_contact: 1.9583
      Episode_Termination/time_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1486848
                    Iteration time: 1.13s
                      Time elapsed: 00:02:08
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 121/1000 [0m                      

                       Computation: 10501 steps/s (collection: 1.114s, learning 0.057s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 9.8507
                       Mean reward: -0.83
               Mean episode length: 407.64
Episode_Reward/track_lin_vel_xy_exp: 0.1470
Episode_Reward/track_ang_vel_z_exp: 0.1168
       Episode_Reward/lin_vel_z_l2: -0.0183
      Episode_Reward/ang_vel_xy_l2: -0.0314
     Episode_Reward/dof_torques_l2: -0.0745
         Episode_Reward/dof_acc_l2: -0.0539
     Episode_Reward/action_rate_l2: -0.0448
      Episode_Reward/feet_air_time: -0.0281
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: -0.0296
  Episode_Termination/base_contact: 2.0833
      Episode_Termination/time_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 1.17s
                      Time elapsed: 00:02:09
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 122/1000 [0m                      

                       Computation: 11105 steps/s (collection: 1.058s, learning 0.049s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 9.8116
                       Mean reward: -0.81
               Mean episode length: 373.16
Episode_Reward/track_lin_vel_xy_exp: 0.0822
Episode_Reward/track_ang_vel_z_exp: 0.0744
       Episode_Reward/lin_vel_z_l2: -0.0136
      Episode_Reward/ang_vel_xy_l2: -0.0215
     Episode_Reward/dof_torques_l2: -0.0543
         Episode_Reward/dof_acc_l2: -0.0341
     Episode_Reward/action_rate_l2: -0.0297
      Episode_Reward/feet_air_time: -0.0212
 Episode_Reward/undesired_contacts: -0.0131
Episode_Reward/flat_orientation_l2: -0.0240
  Episode_Termination/base_contact: 1.5417
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1511424
                    Iteration time: 1.11s
                      Time elapsed: 00:02:10
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 123/1000 [0m                      

                       Computation: 10673 steps/s (collection: 1.100s, learning 0.051s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 9.7695
                       Mean reward: -0.78
               Mean episode length: 396.32
Episode_Reward/track_lin_vel_xy_exp: 0.1753
Episode_Reward/track_ang_vel_z_exp: 0.1474
       Episode_Reward/lin_vel_z_l2: -0.0200
      Episode_Reward/ang_vel_xy_l2: -0.0346
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.0573
     Episode_Reward/action_rate_l2: -0.0520
      Episode_Reward/feet_air_time: -0.0368
 Episode_Reward/undesired_contacts: -0.0178
Episode_Reward/flat_orientation_l2: -0.0278
  Episode_Termination/base_contact: 1.6250
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 1.15s
                      Time elapsed: 00:02:11
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 124/1000 [0m                      

                       Computation: 11119 steps/s (collection: 1.045s, learning 0.060s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 9.7446
                       Mean reward: -0.57
               Mean episode length: 424.42
Episode_Reward/track_lin_vel_xy_exp: 0.2362
Episode_Reward/track_ang_vel_z_exp: 0.1241
       Episode_Reward/lin_vel_z_l2: -0.0219
      Episode_Reward/ang_vel_xy_l2: -0.0385
     Episode_Reward/dof_torques_l2: -0.0968
         Episode_Reward/dof_acc_l2: -0.0634
     Episode_Reward/action_rate_l2: -0.0561
      Episode_Reward/feet_air_time: -0.0404
 Episode_Reward/undesired_contacts: -0.0095
Episode_Reward/flat_orientation_l2: -0.0294
  Episode_Termination/base_contact: 1.5417
      Episode_Termination/time_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1536000
                    Iteration time: 1.11s
                      Time elapsed: 00:02:12
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 125/1000 [0m                      

                       Computation: 11085 steps/s (collection: 1.064s, learning 0.044s)
             Mean action noise std: 0.55
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 9.7207
                       Mean reward: -0.68
               Mean episode length: 461.11
Episode_Reward/track_lin_vel_xy_exp: 0.1337
Episode_Reward/track_ang_vel_z_exp: 0.1341
       Episode_Reward/lin_vel_z_l2: -0.0205
      Episode_Reward/ang_vel_xy_l2: -0.0341
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.0569
     Episode_Reward/action_rate_l2: -0.0496
      Episode_Reward/feet_air_time: -0.0362
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: -0.0261
  Episode_Termination/base_contact: 1.5833
      Episode_Termination/time_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 1.11s
                      Time elapsed: 00:02:13
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 126/1000 [0m                      

                       Computation: 11325 steps/s (collection: 1.040s, learning 0.045s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 9.6822
                       Mean reward: -0.77
               Mean episode length: 395.63
Episode_Reward/track_lin_vel_xy_exp: 0.0958
Episode_Reward/track_ang_vel_z_exp: 0.0693
       Episode_Reward/lin_vel_z_l2: -0.0137
      Episode_Reward/ang_vel_xy_l2: -0.0215
     Episode_Reward/dof_torques_l2: -0.0542
         Episode_Reward/dof_acc_l2: -0.0378
     Episode_Reward/action_rate_l2: -0.0297
      Episode_Reward/feet_air_time: -0.0239
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: -0.0230
  Episode_Termination/base_contact: 1.7917
      Episode_Termination/time_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1560576
                    Iteration time: 1.09s
                      Time elapsed: 00:02:14
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 127/1000 [0m                      

                       Computation: 11118 steps/s (collection: 1.042s, learning 0.063s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 9.6411
                       Mean reward: -0.79
               Mean episode length: 343.47
Episode_Reward/track_lin_vel_xy_exp: 0.0820
Episode_Reward/track_ang_vel_z_exp: 0.1119
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0242
     Episode_Reward/dof_torques_l2: -0.0580
         Episode_Reward/dof_acc_l2: -0.0451
     Episode_Reward/action_rate_l2: -0.0353
      Episode_Reward/feet_air_time: -0.0291
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: -0.0196
  Episode_Termination/base_contact: 1.6250
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.11s
                      Time elapsed: 00:02:16
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 128/1000 [0m                      

                       Computation: 10785 steps/s (collection: 1.094s, learning 0.045s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 9.5795
                       Mean reward: -0.77
               Mean episode length: 327.49
Episode_Reward/track_lin_vel_xy_exp: 0.1085
Episode_Reward/track_ang_vel_z_exp: 0.1003
       Episode_Reward/lin_vel_z_l2: -0.0159
      Episode_Reward/ang_vel_xy_l2: -0.0242
     Episode_Reward/dof_torques_l2: -0.0593
         Episode_Reward/dof_acc_l2: -0.0433
     Episode_Reward/action_rate_l2: -0.0349
      Episode_Reward/feet_air_time: -0.0273
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: -0.0234
  Episode_Termination/base_contact: 1.5417
      Episode_Termination/time_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1585152
                    Iteration time: 1.14s
                      Time elapsed: 00:02:17
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 129/1000 [0m                      

                       Computation: 11213 steps/s (collection: 1.046s, learning 0.050s)
             Mean action noise std: 0.54
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 9.4926
                       Mean reward: -0.52
               Mean episode length: 359.49
Episode_Reward/track_lin_vel_xy_exp: 0.1571
Episode_Reward/track_ang_vel_z_exp: 0.1247
       Episode_Reward/lin_vel_z_l2: -0.0205
      Episode_Reward/ang_vel_xy_l2: -0.0322
     Episode_Reward/dof_torques_l2: -0.0800
         Episode_Reward/dof_acc_l2: -0.0621
     Episode_Reward/action_rate_l2: -0.0486
      Episode_Reward/feet_air_time: -0.0390
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: -0.0312
  Episode_Termination/base_contact: 1.3333
      Episode_Termination/time_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 1.10s
                      Time elapsed: 00:02:18
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 130/1000 [0m                      

                       Computation: 11872 steps/s (collection: 0.978s, learning 0.057s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 9.4238
                       Mean reward: -0.10
               Mean episode length: 431.55
Episode_Reward/track_lin_vel_xy_exp: 0.3209
Episode_Reward/track_ang_vel_z_exp: 0.1882
       Episode_Reward/lin_vel_z_l2: -0.0263
      Episode_Reward/ang_vel_xy_l2: -0.0464
     Episode_Reward/dof_torques_l2: -0.1214
         Episode_Reward/dof_acc_l2: -0.0833
     Episode_Reward/action_rate_l2: -0.0740
      Episode_Reward/feet_air_time: -0.0558
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: -0.0334
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 1609728
                    Iteration time: 1.03s
                      Time elapsed: 00:02:19
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 131/1000 [0m                      

                       Computation: 11479 steps/s (collection: 1.024s, learning 0.046s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 9.3629
                       Mean reward: -0.08
               Mean episode length: 437.11
Episode_Reward/track_lin_vel_xy_exp: 0.1977
Episode_Reward/track_ang_vel_z_exp: 0.1965
       Episode_Reward/lin_vel_z_l2: -0.0298
      Episode_Reward/ang_vel_xy_l2: -0.0466
     Episode_Reward/dof_torques_l2: -0.1155
         Episode_Reward/dof_acc_l2: -0.0819
     Episode_Reward/action_rate_l2: -0.0736
      Episode_Reward/feet_air_time: -0.0585
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: -0.0355
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 1.07s
                      Time elapsed: 00:02:20
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 132/1000 [0m                      

                       Computation: 11548 steps/s (collection: 1.020s, learning 0.044s)
             Mean action noise std: 0.53
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 9.2990
                       Mean reward: -0.22
               Mean episode length: 475.52
Episode_Reward/track_lin_vel_xy_exp: 0.1557
Episode_Reward/track_ang_vel_z_exp: 0.1364
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0305
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.0513
     Episode_Reward/action_rate_l2: -0.0469
      Episode_Reward/feet_air_time: -0.0359
 Episode_Reward/undesired_contacts: -0.0096
Episode_Reward/flat_orientation_l2: -0.0254
  Episode_Termination/base_contact: 1.1667
      Episode_Termination/time_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 1634304
                    Iteration time: 1.06s
                      Time elapsed: 00:02:21
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 133/1000 [0m                      

                       Computation: 11789 steps/s (collection: 0.998s, learning 0.044s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 9.2273
                       Mean reward: -0.08
               Mean episode length: 490.40
Episode_Reward/track_lin_vel_xy_exp: 0.1540
Episode_Reward/track_ang_vel_z_exp: 0.1766
       Episode_Reward/lin_vel_z_l2: -0.0209
      Episode_Reward/ang_vel_xy_l2: -0.0349
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.0581
     Episode_Reward/action_rate_l2: -0.0526
      Episode_Reward/feet_air_time: -0.0407
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0229
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 1.04s
                      Time elapsed: 00:02:22
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 134/1000 [0m                      

                       Computation: 11488 steps/s (collection: 1.024s, learning 0.046s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 9.1275
                       Mean reward: 0.38
               Mean episode length: 554.14
Episode_Reward/track_lin_vel_xy_exp: 0.2643
Episode_Reward/track_ang_vel_z_exp: 0.2489
       Episode_Reward/lin_vel_z_l2: -0.0251
      Episode_Reward/ang_vel_xy_l2: -0.0450
     Episode_Reward/dof_torques_l2: -0.1339
         Episode_Reward/dof_acc_l2: -0.0725
     Episode_Reward/action_rate_l2: -0.0736
      Episode_Reward/feet_air_time: -0.0463
 Episode_Reward/undesired_contacts: -0.0123
Episode_Reward/flat_orientation_l2: -0.0256
  Episode_Termination/base_contact: 0.8750
      Episode_Termination/time_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 1658880
                    Iteration time: 1.07s
                      Time elapsed: 00:02:23
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 135/1000 [0m                      

                       Computation: 10848 steps/s (collection: 1.080s, learning 0.053s)
             Mean action noise std: 0.52
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 9.0502
                       Mean reward: 0.59
               Mean episode length: 573.56
Episode_Reward/track_lin_vel_xy_exp: 0.2402
Episode_Reward/track_ang_vel_z_exp: 0.1666
       Episode_Reward/lin_vel_z_l2: -0.0217
      Episode_Reward/ang_vel_xy_l2: -0.0349
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.0593
     Episode_Reward/action_rate_l2: -0.0533
      Episode_Reward/feet_air_time: -0.0334
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: -0.0213
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.13s
                      Time elapsed: 00:02:24
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 136/1000 [0m                      

                       Computation: 10615 steps/s (collection: 1.090s, learning 0.068s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 8.9901
                       Mean reward: 0.64
               Mean episode length: 539.76
Episode_Reward/track_lin_vel_xy_exp: 0.2355
Episode_Reward/track_ang_vel_z_exp: 0.1280
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0287
     Episode_Reward/dof_torques_l2: -0.0703
         Episode_Reward/dof_acc_l2: -0.0545
     Episode_Reward/action_rate_l2: -0.0428
      Episode_Reward/feet_air_time: -0.0314
 Episode_Reward/undesired_contacts: -0.0092
Episode_Reward/flat_orientation_l2: -0.0311
  Episode_Termination/base_contact: 1.0000
      Episode_Termination/time_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1683456
                    Iteration time: 1.16s
                      Time elapsed: 00:02:25
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 137/1000 [0m                      

                       Computation: 11001 steps/s (collection: 1.072s, learning 0.045s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 8.9168
                       Mean reward: 0.82
               Mean episode length: 598.19
Episode_Reward/track_lin_vel_xy_exp: 0.2649
Episode_Reward/track_ang_vel_z_exp: 0.3077
       Episode_Reward/lin_vel_z_l2: -0.0287
      Episode_Reward/ang_vel_xy_l2: -0.0517
     Episode_Reward/dof_torques_l2: -0.1473
         Episode_Reward/dof_acc_l2: -0.0811
     Episode_Reward/action_rate_l2: -0.0835
      Episode_Reward/feet_air_time: -0.0556
 Episode_Reward/undesired_contacts: -0.0170
Episode_Reward/flat_orientation_l2: -0.0271
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 1.12s
                      Time elapsed: 00:02:27
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 138/1000 [0m                      

                       Computation: 11557 steps/s (collection: 1.009s, learning 0.054s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 8.8553
                       Mean reward: 0.76
               Mean episode length: 644.81
Episode_Reward/track_lin_vel_xy_exp: 0.1758
Episode_Reward/track_ang_vel_z_exp: 0.2593
       Episode_Reward/lin_vel_z_l2: -0.0287
      Episode_Reward/ang_vel_xy_l2: -0.0505
     Episode_Reward/dof_torques_l2: -0.1311
         Episode_Reward/dof_acc_l2: -0.0845
     Episode_Reward/action_rate_l2: -0.0788
      Episode_Reward/feet_air_time: -0.0604
 Episode_Reward/undesired_contacts: -0.0056
Episode_Reward/flat_orientation_l2: -0.0290
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 1708032
                    Iteration time: 1.06s
                      Time elapsed: 00:02:28
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 139/1000 [0m                      

                       Computation: 10520 steps/s (collection: 1.087s, learning 0.081s)
             Mean action noise std: 0.51
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 8.7938
                       Mean reward: 0.77
               Mean episode length: 652.97
Episode_Reward/track_lin_vel_xy_exp: 0.1868
Episode_Reward/track_ang_vel_z_exp: 0.2318
       Episode_Reward/lin_vel_z_l2: -0.0252
      Episode_Reward/ang_vel_xy_l2: -0.0430
     Episode_Reward/dof_torques_l2: -0.1098
         Episode_Reward/dof_acc_l2: -0.0738
     Episode_Reward/action_rate_l2: -0.0665
      Episode_Reward/feet_air_time: -0.0538
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: -0.0286
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 1.17s
                      Time elapsed: 00:02:29
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 140/1000 [0m                      

                       Computation: 11530 steps/s (collection: 1.018s, learning 0.048s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 8.7454
                       Mean reward: 0.29
               Mean episode length: 684.62
Episode_Reward/track_lin_vel_xy_exp: 0.2168
Episode_Reward/track_ang_vel_z_exp: 0.2723
       Episode_Reward/lin_vel_z_l2: -0.0293
      Episode_Reward/ang_vel_xy_l2: -0.0537
     Episode_Reward/dof_torques_l2: -0.1382
         Episode_Reward/dof_acc_l2: -0.0878
     Episode_Reward/action_rate_l2: -0.0837
      Episode_Reward/feet_air_time: -0.0658
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: -0.0259
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1732608
                    Iteration time: 1.07s
                      Time elapsed: 00:02:30
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 141/1000 [0m                      

                       Computation: 11337 steps/s (collection: 1.032s, learning 0.052s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 8.6846
                       Mean reward: -0.06
               Mean episode length: 718.42
Episode_Reward/track_lin_vel_xy_exp: 0.1600
Episode_Reward/track_ang_vel_z_exp: 0.2053
       Episode_Reward/lin_vel_z_l2: -0.0249
      Episode_Reward/ang_vel_xy_l2: -0.0429
     Episode_Reward/dof_torques_l2: -0.1135
         Episode_Reward/dof_acc_l2: -0.0723
     Episode_Reward/action_rate_l2: -0.0675
      Episode_Reward/feet_air_time: -0.0533
 Episode_Reward/undesired_contacts: -0.0156
Episode_Reward/flat_orientation_l2: -0.0293
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 1.08s
                      Time elapsed: 00:02:31
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 142/1000 [0m                      

                       Computation: 11952 steps/s (collection: 0.983s, learning 0.045s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 8.6562
                       Mean reward: -0.05
               Mean episode length: 749.13
Episode_Reward/track_lin_vel_xy_exp: 0.2178
Episode_Reward/track_ang_vel_z_exp: 0.2045
       Episode_Reward/lin_vel_z_l2: -0.0223
      Episode_Reward/ang_vel_xy_l2: -0.0413
     Episode_Reward/dof_torques_l2: -0.1115
         Episode_Reward/dof_acc_l2: -0.0655
     Episode_Reward/action_rate_l2: -0.0632
      Episode_Reward/feet_air_time: -0.0485
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: -0.0222
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 1757184
                    Iteration time: 1.03s
                      Time elapsed: 00:02:32
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 143/1000 [0m                      

                       Computation: 11359 steps/s (collection: 1.035s, learning 0.047s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 8.6270
                       Mean reward: -0.33
               Mean episode length: 708.94
Episode_Reward/track_lin_vel_xy_exp: 0.1076
Episode_Reward/track_ang_vel_z_exp: 0.1968
       Episode_Reward/lin_vel_z_l2: -0.0231
      Episode_Reward/ang_vel_xy_l2: -0.0371
     Episode_Reward/dof_torques_l2: -0.1045
         Episode_Reward/dof_acc_l2: -0.0618
     Episode_Reward/action_rate_l2: -0.0578
      Episode_Reward/feet_air_time: -0.0414
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: -0.0286
  Episode_Termination/base_contact: 1.0000
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.08s
                      Time elapsed: 00:02:33
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 144/1000 [0m                      

                       Computation: 11072 steps/s (collection: 1.051s, learning 0.059s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 8.5996
                       Mean reward: -0.14
               Mean episode length: 649.35
Episode_Reward/track_lin_vel_xy_exp: 0.2397
Episode_Reward/track_ang_vel_z_exp: 0.2109
       Episode_Reward/lin_vel_z_l2: -0.0214
      Episode_Reward/ang_vel_xy_l2: -0.0387
     Episode_Reward/dof_torques_l2: -0.0909
         Episode_Reward/dof_acc_l2: -0.0645
     Episode_Reward/action_rate_l2: -0.0559
      Episode_Reward/feet_air_time: -0.0463
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0274
  Episode_Termination/base_contact: 1.0417
      Episode_Termination/time_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1781760
                    Iteration time: 1.11s
                      Time elapsed: 00:02:34
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 145/1000 [0m                      

                       Computation: 11322 steps/s (collection: 1.027s, learning 0.058s)
             Mean action noise std: 0.50
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 8.5443
                       Mean reward: -0.23
               Mean episode length: 625.10
Episode_Reward/track_lin_vel_xy_exp: 0.1251
Episode_Reward/track_ang_vel_z_exp: 0.1321
       Episode_Reward/lin_vel_z_l2: -0.0154
      Episode_Reward/ang_vel_xy_l2: -0.0263
     Episode_Reward/dof_torques_l2: -0.0698
         Episode_Reward/dof_acc_l2: -0.0451
     Episode_Reward/action_rate_l2: -0.0385
      Episode_Reward/feet_air_time: -0.0304
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: -0.0228
  Episode_Termination/base_contact: 1.0417
      Episode_Termination/time_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 1.09s
                      Time elapsed: 00:02:35
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 146/1000 [0m                      

                       Computation: 11117 steps/s (collection: 1.059s, learning 0.047s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 8.4909
                       Mean reward: -0.38
               Mean episode length: 575.37
Episode_Reward/track_lin_vel_xy_exp: 0.0651
Episode_Reward/track_ang_vel_z_exp: 0.1676
       Episode_Reward/lin_vel_z_l2: -0.0216
      Episode_Reward/ang_vel_xy_l2: -0.0373
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.0627
     Episode_Reward/action_rate_l2: -0.0536
      Episode_Reward/feet_air_time: -0.0461
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0307
  Episode_Termination/base_contact: 1.4167
      Episode_Termination/time_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1806336
                    Iteration time: 1.11s
                      Time elapsed: 00:02:36
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 147/1000 [0m                      

                       Computation: 11336 steps/s (collection: 1.038s, learning 0.046s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 8.4470
                       Mean reward: -0.14
               Mean episode length: 520.13
Episode_Reward/track_lin_vel_xy_exp: 0.1062
Episode_Reward/track_ang_vel_z_exp: 0.1497
       Episode_Reward/lin_vel_z_l2: -0.0183
      Episode_Reward/ang_vel_xy_l2: -0.0329
     Episode_Reward/dof_torques_l2: -0.0718
         Episode_Reward/dof_acc_l2: -0.0530
     Episode_Reward/action_rate_l2: -0.0416
      Episode_Reward/feet_air_time: -0.0367
 Episode_Reward/undesired_contacts: -0.0049
Episode_Reward/flat_orientation_l2: -0.0316
  Episode_Termination/base_contact: 1.3750
      Episode_Termination/time_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 1.08s
                      Time elapsed: 00:02:37
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 148/1000 [0m                      

                       Computation: 11561 steps/s (collection: 1.013s, learning 0.050s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 8.4168
                       Mean reward: -0.38
               Mean episode length: 510.63
Episode_Reward/track_lin_vel_xy_exp: 0.2504
Episode_Reward/track_ang_vel_z_exp: 0.1301
       Episode_Reward/lin_vel_z_l2: -0.0194
      Episode_Reward/ang_vel_xy_l2: -0.0345
     Episode_Reward/dof_torques_l2: -0.0862
         Episode_Reward/dof_acc_l2: -0.0563
     Episode_Reward/action_rate_l2: -0.0477
      Episode_Reward/feet_air_time: -0.0314
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: -0.0295
  Episode_Termination/base_contact: 1.0000
      Episode_Termination/time_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1830912
                    Iteration time: 1.06s
                      Time elapsed: 00:02:38
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 149/1000 [0m                      

                       Computation: 11062 steps/s (collection: 1.065s, learning 0.046s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 8.3905
                       Mean reward: -0.73
               Mean episode length: 472.47
Episode_Reward/track_lin_vel_xy_exp: 0.0908
Episode_Reward/track_ang_vel_z_exp: 0.1315
       Episode_Reward/lin_vel_z_l2: -0.0176
      Episode_Reward/ang_vel_xy_l2: -0.0298
     Episode_Reward/dof_torques_l2: -0.0666
         Episode_Reward/dof_acc_l2: -0.0440
     Episode_Reward/action_rate_l2: -0.0384
      Episode_Reward/feet_air_time: -0.0332
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0311
  Episode_Termination/base_contact: 1.6250
      Episode_Termination/time_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 1.11s
                      Time elapsed: 00:02:40
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 150/1000 [0m                      

                       Computation: 11262 steps/s (collection: 1.027s, learning 0.064s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 8.3672
                       Mean reward: -0.77
               Mean episode length: 502.02
Episode_Reward/track_lin_vel_xy_exp: 0.0819
Episode_Reward/track_ang_vel_z_exp: 0.1512
       Episode_Reward/lin_vel_z_l2: -0.0187
      Episode_Reward/ang_vel_xy_l2: -0.0325
     Episode_Reward/dof_torques_l2: -0.0806
         Episode_Reward/dof_acc_l2: -0.0467
     Episode_Reward/action_rate_l2: -0.0440
      Episode_Reward/feet_air_time: -0.0351
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: -0.0318
  Episode_Termination/base_contact: 1.6250
      Episode_Termination/time_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1855488
                    Iteration time: 1.09s
                      Time elapsed: 00:02:41
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 151/1000 [0m                      

                       Computation: 11126 steps/s (collection: 1.054s, learning 0.050s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 8.3403
                       Mean reward: -0.81
               Mean episode length: 492.62
Episode_Reward/track_lin_vel_xy_exp: 0.1165
Episode_Reward/track_ang_vel_z_exp: 0.1537
       Episode_Reward/lin_vel_z_l2: -0.0193
      Episode_Reward/ang_vel_xy_l2: -0.0336
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.0482
     Episode_Reward/action_rate_l2: -0.0461
      Episode_Reward/feet_air_time: -0.0355
 Episode_Reward/undesired_contacts: -0.0057
Episode_Reward/flat_orientation_l2: -0.0313
  Episode_Termination/base_contact: 1.4583
      Episode_Termination/time_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.10s
                      Time elapsed: 00:02:42
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 152/1000 [0m                      

                       Computation: 10935 steps/s (collection: 1.079s, learning 0.045s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0095
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 8.3142
                       Mean reward: -0.81
               Mean episode length: 521.38
Episode_Reward/track_lin_vel_xy_exp: 0.0985
Episode_Reward/track_ang_vel_z_exp: 0.1635
       Episode_Reward/lin_vel_z_l2: -0.0195
      Episode_Reward/ang_vel_xy_l2: -0.0342
     Episode_Reward/dof_torques_l2: -0.0856
         Episode_Reward/dof_acc_l2: -0.0482
     Episode_Reward/action_rate_l2: -0.0459
      Episode_Reward/feet_air_time: -0.0359
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: -0.0323
  Episode_Termination/base_contact: 1.5833
      Episode_Termination/time_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1880064
                    Iteration time: 1.12s
                      Time elapsed: 00:02:43
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 153/1000 [0m                      

                       Computation: 10896 steps/s (collection: 1.082s, learning 0.046s)
             Mean action noise std: 0.49
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 8.3015
                       Mean reward: -0.43
               Mean episode length: 492.87
Episode_Reward/track_lin_vel_xy_exp: 0.0922
Episode_Reward/track_ang_vel_z_exp: 0.1144
       Episode_Reward/lin_vel_z_l2: -0.0162
      Episode_Reward/ang_vel_xy_l2: -0.0266
     Episode_Reward/dof_torques_l2: -0.0613
         Episode_Reward/dof_acc_l2: -0.0370
     Episode_Reward/action_rate_l2: -0.0325
      Episode_Reward/feet_air_time: -0.0267
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0318
  Episode_Termination/base_contact: 1.7500
      Episode_Termination/time_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 1.13s
                      Time elapsed: 00:02:44
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 154/1000 [0m                      

                       Computation: 10905 steps/s (collection: 1.083s, learning 0.044s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 8.2580
                       Mean reward: -0.23
               Mean episode length: 489.25
Episode_Reward/track_lin_vel_xy_exp: 0.0928
Episode_Reward/track_ang_vel_z_exp: 0.1156
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0276
     Episode_Reward/dof_torques_l2: -0.0718
         Episode_Reward/dof_acc_l2: -0.0357
     Episode_Reward/action_rate_l2: -0.0361
      Episode_Reward/feet_air_time: -0.0263
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: -0.0311
  Episode_Termination/base_contact: 1.3750
      Episode_Termination/time_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1904640
                    Iteration time: 1.13s
                      Time elapsed: 00:02:45
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 155/1000 [0m                      

                       Computation: 11325 steps/s (collection: 1.039s, learning 0.046s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 8.1933
                       Mean reward: -0.32
               Mean episode length: 488.88
Episode_Reward/track_lin_vel_xy_exp: 0.1598
Episode_Reward/track_ang_vel_z_exp: 0.1257
       Episode_Reward/lin_vel_z_l2: -0.0199
      Episode_Reward/ang_vel_xy_l2: -0.0353
     Episode_Reward/dof_torques_l2: -0.1032
         Episode_Reward/dof_acc_l2: -0.0522
     Episode_Reward/action_rate_l2: -0.0520
      Episode_Reward/feet_air_time: -0.0381
 Episode_Reward/undesired_contacts: -0.0187
Episode_Reward/flat_orientation_l2: -0.0323
  Episode_Termination/base_contact: 0.9583
      Episode_Termination/time_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 1.08s
                      Time elapsed: 00:02:46
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 156/1000 [0m                      

                       Computation: 11378 steps/s (collection: 1.029s, learning 0.051s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 8.1266
                       Mean reward: 0.08
               Mean episode length: 493.00
Episode_Reward/track_lin_vel_xy_exp: 0.1512
Episode_Reward/track_ang_vel_z_exp: 0.1549
       Episode_Reward/lin_vel_z_l2: -0.0162
      Episode_Reward/ang_vel_xy_l2: -0.0319
     Episode_Reward/dof_torques_l2: -0.0712
         Episode_Reward/dof_acc_l2: -0.0447
     Episode_Reward/action_rate_l2: -0.0401
      Episode_Reward/feet_air_time: -0.0338
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: -0.0337
  Episode_Termination/base_contact: 1.2917
      Episode_Termination/time_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1929216
                    Iteration time: 1.08s
                      Time elapsed: 00:02:47
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 157/1000 [0m                      

                       Computation: 10504 steps/s (collection: 1.120s, learning 0.050s)
             Mean action noise std: 0.48
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 8.0673
                       Mean reward: 0.04
               Mean episode length: 521.67
Episode_Reward/track_lin_vel_xy_exp: 0.1829
Episode_Reward/track_ang_vel_z_exp: 0.1697
       Episode_Reward/lin_vel_z_l2: -0.0196
      Episode_Reward/ang_vel_xy_l2: -0.0347
     Episode_Reward/dof_torques_l2: -0.0917
         Episode_Reward/dof_acc_l2: -0.0531
     Episode_Reward/action_rate_l2: -0.0491
      Episode_Reward/feet_air_time: -0.0354
 Episode_Reward/undesired_contacts: -0.0103
Episode_Reward/flat_orientation_l2: -0.0309
  Episode_Termination/base_contact: 1.1667
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 1.17s
                      Time elapsed: 00:02:48
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 158/1000 [0m                      

                       Computation: 11121 steps/s (collection: 1.059s, learning 0.045s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 7.9883
                       Mean reward: 0.00
               Mean episode length: 543.38
Episode_Reward/track_lin_vel_xy_exp: 0.1499
Episode_Reward/track_ang_vel_z_exp: 0.1936
       Episode_Reward/lin_vel_z_l2: -0.0196
      Episode_Reward/ang_vel_xy_l2: -0.0358
     Episode_Reward/dof_torques_l2: -0.0976
         Episode_Reward/dof_acc_l2: -0.0590
     Episode_Reward/action_rate_l2: -0.0543
      Episode_Reward/feet_air_time: -0.0443
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: -0.0286
  Episode_Termination/base_contact: 0.9583
      Episode_Termination/time_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 1953792
                    Iteration time: 1.10s
                      Time elapsed: 00:02:50
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 159/1000 [0m                      

                       Computation: 10596 steps/s (collection: 1.116s, learning 0.044s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 7.9007
                       Mean reward: 0.29
               Mean episode length: 560.92
Episode_Reward/track_lin_vel_xy_exp: 0.1964
Episode_Reward/track_ang_vel_z_exp: 0.2477
       Episode_Reward/lin_vel_z_l2: -0.0231
      Episode_Reward/ang_vel_xy_l2: -0.0442
     Episode_Reward/dof_torques_l2: -0.1233
         Episode_Reward/dof_acc_l2: -0.0684
     Episode_Reward/action_rate_l2: -0.0679
      Episode_Reward/feet_air_time: -0.0556
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: -0.0319
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.16s
                      Time elapsed: 00:02:51
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 160/1000 [0m                      

                       Computation: 11103 steps/s (collection: 1.059s, learning 0.047s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 7.8388
                       Mean reward: 0.74
               Mean episode length: 607.00
Episode_Reward/track_lin_vel_xy_exp: 0.2244
Episode_Reward/track_ang_vel_z_exp: 0.2165
       Episode_Reward/lin_vel_z_l2: -0.0215
      Episode_Reward/ang_vel_xy_l2: -0.0393
     Episode_Reward/dof_torques_l2: -0.1128
         Episode_Reward/dof_acc_l2: -0.0620
     Episode_Reward/action_rate_l2: -0.0610
      Episode_Reward/feet_air_time: -0.0509
 Episode_Reward/undesired_contacts: -0.0123
Episode_Reward/flat_orientation_l2: -0.0267
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 1978368
                    Iteration time: 1.11s
                      Time elapsed: 00:02:52
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 161/1000 [0m                      

                       Computation: 11208 steps/s (collection: 1.049s, learning 0.047s)
             Mean action noise std: 0.47
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 7.7945
                       Mean reward: 0.62
               Mean episode length: 608.91
Episode_Reward/track_lin_vel_xy_exp: 0.1288
Episode_Reward/track_ang_vel_z_exp: 0.0947
       Episode_Reward/lin_vel_z_l2: -0.0162
      Episode_Reward/ang_vel_xy_l2: -0.0267
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.0494
     Episode_Reward/action_rate_l2: -0.0405
      Episode_Reward/feet_air_time: -0.0345
 Episode_Reward/undesired_contacts: -0.0125
Episode_Reward/flat_orientation_l2: -0.0239
  Episode_Termination/base_contact: 1.5000
      Episode_Termination/time_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 1.10s
                      Time elapsed: 00:02:53
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 162/1000 [0m                      

                       Computation: 10727 steps/s (collection: 1.094s, learning 0.051s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 7.7320
                       Mean reward: 0.93
               Mean episode length: 632.13
Episode_Reward/track_lin_vel_xy_exp: 0.2106
Episode_Reward/track_ang_vel_z_exp: 0.2164
       Episode_Reward/lin_vel_z_l2: -0.0201
      Episode_Reward/ang_vel_xy_l2: -0.0369
     Episode_Reward/dof_torques_l2: -0.0981
         Episode_Reward/dof_acc_l2: -0.0578
     Episode_Reward/action_rate_l2: -0.0546
      Episode_Reward/feet_air_time: -0.0474
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: -0.0257
  Episode_Termination/base_contact: 0.8750
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2002944
                    Iteration time: 1.15s
                      Time elapsed: 00:02:54
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 163/1000 [0m                      

                       Computation: 11284 steps/s (collection: 1.045s, learning 0.044s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 7.6702
                       Mean reward: 0.82
               Mean episode length: 636.83
Episode_Reward/track_lin_vel_xy_exp: 0.1169
Episode_Reward/track_ang_vel_z_exp: 0.1367
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0290
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.0436
     Episode_Reward/action_rate_l2: -0.0418
      Episode_Reward/feet_air_time: -0.0341
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: -0.0265
  Episode_Termination/base_contact: 1.3750
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 1.09s
                      Time elapsed: 00:02:55
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 164/1000 [0m                      

                       Computation: 11016 steps/s (collection: 1.026s, learning 0.089s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 7.5794
                       Mean reward: 0.87
               Mean episode length: 653.54
Episode_Reward/track_lin_vel_xy_exp: 0.1007
Episode_Reward/track_ang_vel_z_exp: 0.3184
       Episode_Reward/lin_vel_z_l2: -0.0224
      Episode_Reward/ang_vel_xy_l2: -0.0422
     Episode_Reward/dof_torques_l2: -0.1725
         Episode_Reward/dof_acc_l2: -0.0514
     Episode_Reward/action_rate_l2: -0.0746
      Episode_Reward/feet_air_time: -0.0432
 Episode_Reward/undesired_contacts: -0.0603
Episode_Reward/flat_orientation_l2: -0.0319
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2027520
                    Iteration time: 1.12s
                      Time elapsed: 00:02:56
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 165/1000 [0m                      

                       Computation: 11173 steps/s (collection: 1.057s, learning 0.043s)
             Mean action noise std: 0.46
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 7.5255
                       Mean reward: 0.35
               Mean episode length: 681.82
Episode_Reward/track_lin_vel_xy_exp: 0.1707
Episode_Reward/track_ang_vel_z_exp: 0.2993
       Episode_Reward/lin_vel_z_l2: -0.0233
      Episode_Reward/ang_vel_xy_l2: -0.0455
     Episode_Reward/dof_torques_l2: -0.1371
         Episode_Reward/dof_acc_l2: -0.0711
     Episode_Reward/action_rate_l2: -0.0719
      Episode_Reward/feet_air_time: -0.0579
 Episode_Reward/undesired_contacts: -0.0120
Episode_Reward/flat_orientation_l2: -0.0265
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 1.10s
                      Time elapsed: 00:02:57
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 166/1000 [0m                      

                       Computation: 11724 steps/s (collection: 1.002s, learning 0.046s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 7.4498
                       Mean reward: 0.22
               Mean episode length: 647.46
Episode_Reward/track_lin_vel_xy_exp: 0.0856
Episode_Reward/track_ang_vel_z_exp: 0.0701
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.0654
         Episode_Reward/dof_acc_l2: -0.0371
     Episode_Reward/action_rate_l2: -0.0305
      Episode_Reward/feet_air_time: -0.0264
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: -0.0207
  Episode_Termination/base_contact: 0.9167
      Episode_Termination/time_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2052096
                    Iteration time: 1.05s
                      Time elapsed: 00:02:58
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 167/1000 [0m                      

                       Computation: 11087 steps/s (collection: 1.061s, learning 0.047s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 7.3793
                       Mean reward: 0.10
               Mean episode length: 703.69
Episode_Reward/track_lin_vel_xy_exp: 0.1610
Episode_Reward/track_ang_vel_z_exp: 0.2437
       Episode_Reward/lin_vel_z_l2: -0.0250
      Episode_Reward/ang_vel_xy_l2: -0.0494
     Episode_Reward/dof_torques_l2: -0.1587
         Episode_Reward/dof_acc_l2: -0.0696
     Episode_Reward/action_rate_l2: -0.0800
      Episode_Reward/feet_air_time: -0.0626
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: -0.0296
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.11s
                      Time elapsed: 00:03:00
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 168/1000 [0m                      

                       Computation: 10756 steps/s (collection: 1.094s, learning 0.048s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 7.3240
                       Mean reward: 0.07
               Mean episode length: 733.13
Episode_Reward/track_lin_vel_xy_exp: 0.3880
Episode_Reward/track_ang_vel_z_exp: 0.2182
       Episode_Reward/lin_vel_z_l2: -0.0245
      Episode_Reward/ang_vel_xy_l2: -0.0452
     Episode_Reward/dof_torques_l2: -0.1429
         Episode_Reward/dof_acc_l2: -0.0691
     Episode_Reward/action_rate_l2: -0.0738
      Episode_Reward/feet_air_time: -0.0599
 Episode_Reward/undesired_contacts: -0.0121
Episode_Reward/flat_orientation_l2: -0.0250
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2076672
                    Iteration time: 1.14s
                      Time elapsed: 00:03:01
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 169/1000 [0m                      

                       Computation: 11249 steps/s (collection: 1.046s, learning 0.046s)
             Mean action noise std: 0.45
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 7.2686
                       Mean reward: 0.36
               Mean episode length: 759.13
Episode_Reward/track_lin_vel_xy_exp: 0.3083
Episode_Reward/track_ang_vel_z_exp: 0.2909
       Episode_Reward/lin_vel_z_l2: -0.0216
      Episode_Reward/ang_vel_xy_l2: -0.0428
     Episode_Reward/dof_torques_l2: -0.1479
         Episode_Reward/dof_acc_l2: -0.0607
     Episode_Reward/action_rate_l2: -0.0697
      Episode_Reward/feet_air_time: -0.0523
 Episode_Reward/undesired_contacts: -0.0099
Episode_Reward/flat_orientation_l2: -0.0226
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 1.09s
                      Time elapsed: 00:03:02
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 170/1000 [0m                      

                       Computation: 11401 steps/s (collection: 1.014s, learning 0.064s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 7.2131
                       Mean reward: 0.80
               Mean episode length: 771.44
Episode_Reward/track_lin_vel_xy_exp: 0.2070
Episode_Reward/track_ang_vel_z_exp: 0.1766
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0362
     Episode_Reward/dof_torques_l2: -0.1043
         Episode_Reward/dof_acc_l2: -0.0561
     Episode_Reward/action_rate_l2: -0.0538
      Episode_Reward/feet_air_time: -0.0457
 Episode_Reward/undesired_contacts: -0.0139
Episode_Reward/flat_orientation_l2: -0.0277
  Episode_Termination/base_contact: 0.8750
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2101248
                    Iteration time: 1.08s
                      Time elapsed: 00:03:03
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 171/1000 [0m                      

                       Computation: 11839 steps/s (collection: 0.987s, learning 0.051s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 7.1437
                       Mean reward: 1.07
               Mean episode length: 787.15
Episode_Reward/track_lin_vel_xy_exp: 0.2463
Episode_Reward/track_ang_vel_z_exp: 0.2629
       Episode_Reward/lin_vel_z_l2: -0.0205
      Episode_Reward/ang_vel_xy_l2: -0.0404
     Episode_Reward/dof_torques_l2: -0.1309
         Episode_Reward/dof_acc_l2: -0.0571
     Episode_Reward/action_rate_l2: -0.0622
      Episode_Reward/feet_air_time: -0.0494
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: -0.0246
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 1.04s
                      Time elapsed: 00:03:04
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 172/1000 [0m                      

                       Computation: 11931 steps/s (collection: 0.985s, learning 0.044s)
             Mean action noise std: 0.44
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 7.0253
                       Mean reward: 1.28
               Mean episode length: 758.37
Episode_Reward/track_lin_vel_xy_exp: 0.1878
Episode_Reward/track_ang_vel_z_exp: 0.1572
       Episode_Reward/lin_vel_z_l2: -0.0190
      Episode_Reward/ang_vel_xy_l2: -0.0342
     Episode_Reward/dof_torques_l2: -0.1035
         Episode_Reward/dof_acc_l2: -0.0514
     Episode_Reward/action_rate_l2: -0.0511
      Episode_Reward/feet_air_time: -0.0428
 Episode_Reward/undesired_contacts: -0.0060
Episode_Reward/flat_orientation_l2: -0.0297
  Episode_Termination/base_contact: 0.8333
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2125824
                    Iteration time: 1.03s
                      Time elapsed: 00:03:05
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 173/1000 [0m                      

                       Computation: 11639 steps/s (collection: 1.012s, learning 0.044s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 6.9482
                       Mean reward: 1.27
               Mean episode length: 745.43
Episode_Reward/track_lin_vel_xy_exp: 0.2594
Episode_Reward/track_ang_vel_z_exp: 0.2165
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0374
     Episode_Reward/dof_torques_l2: -0.1173
         Episode_Reward/dof_acc_l2: -0.0531
     Episode_Reward/action_rate_l2: -0.0567
      Episode_Reward/feet_air_time: -0.0461
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: -0.0230
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 1.06s
                      Time elapsed: 00:03:06
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 174/1000 [0m                      

                       Computation: 11197 steps/s (collection: 1.050s, learning 0.048s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 6.8762
                       Mean reward: 1.24
               Mean episode length: 707.40
Episode_Reward/track_lin_vel_xy_exp: 0.0914
Episode_Reward/track_ang_vel_z_exp: 0.2087
       Episode_Reward/lin_vel_z_l2: -0.0200
      Episode_Reward/ang_vel_xy_l2: -0.0315
     Episode_Reward/dof_torques_l2: -0.0841
         Episode_Reward/dof_acc_l2: -0.0502
     Episode_Reward/action_rate_l2: -0.0454
      Episode_Reward/feet_air_time: -0.0427
 Episode_Reward/undesired_contacts: -0.0080
Episode_Reward/flat_orientation_l2: -0.0247
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2150400
                    Iteration time: 1.10s
                      Time elapsed: 00:03:07
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 175/1000 [0m                      

                       Computation: 11741 steps/s (collection: 1.001s, learning 0.046s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 6.7965
                       Mean reward: 0.94
               Mean episode length: 675.83
Episode_Reward/track_lin_vel_xy_exp: 0.1856
Episode_Reward/track_ang_vel_z_exp: 0.1782
       Episode_Reward/lin_vel_z_l2: -0.0159
      Episode_Reward/ang_vel_xy_l2: -0.0273
     Episode_Reward/dof_torques_l2: -0.0847
         Episode_Reward/dof_acc_l2: -0.0427
     Episode_Reward/action_rate_l2: -0.0413
      Episode_Reward/feet_air_time: -0.0346
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: -0.0211
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.05s
                      Time elapsed: 00:03:08
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 176/1000 [0m                      

                       Computation: 11968 steps/s (collection: 0.980s, learning 0.047s)
             Mean action noise std: 0.43
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 6.7400
                       Mean reward: 0.96
               Mean episode length: 650.87
Episode_Reward/track_lin_vel_xy_exp: 0.1856
Episode_Reward/track_ang_vel_z_exp: 0.1975
       Episode_Reward/lin_vel_z_l2: -0.0180
      Episode_Reward/ang_vel_xy_l2: -0.0332
     Episode_Reward/dof_torques_l2: -0.1005
         Episode_Reward/dof_acc_l2: -0.0514
     Episode_Reward/action_rate_l2: -0.0498
      Episode_Reward/feet_air_time: -0.0425
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: -0.0225
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2174976
                    Iteration time: 1.03s
                      Time elapsed: 00:03:09
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 177/1000 [0m                      

                       Computation: 11741 steps/s (collection: 1.002s, learning 0.044s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 6.6792
                       Mean reward: 0.87
               Mean episode length: 646.31
Episode_Reward/track_lin_vel_xy_exp: 0.2275
Episode_Reward/track_ang_vel_z_exp: 0.1573
       Episode_Reward/lin_vel_z_l2: -0.0184
      Episode_Reward/ang_vel_xy_l2: -0.0312
     Episode_Reward/dof_torques_l2: -0.1045
         Episode_Reward/dof_acc_l2: -0.0492
     Episode_Reward/action_rate_l2: -0.0484
      Episode_Reward/feet_air_time: -0.0396
 Episode_Reward/undesired_contacts: -0.0125
Episode_Reward/flat_orientation_l2: -0.0220
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 1.05s
                      Time elapsed: 00:03:10
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 178/1000 [0m                      

                       Computation: 11556 steps/s (collection: 1.015s, learning 0.048s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 6.5979
                       Mean reward: 0.79
               Mean episode length: 636.02
Episode_Reward/track_lin_vel_xy_exp: 0.1318
Episode_Reward/track_ang_vel_z_exp: 0.1408
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0291
     Episode_Reward/dof_torques_l2: -0.0729
         Episode_Reward/dof_acc_l2: -0.0444
     Episode_Reward/action_rate_l2: -0.0382
      Episode_Reward/feet_air_time: -0.0355
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: -0.0255
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2199552
                    Iteration time: 1.06s
                      Time elapsed: 00:03:11
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 179/1000 [0m                      

                       Computation: 11463 steps/s (collection: 1.027s, learning 0.045s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 6.5341
                       Mean reward: 1.15
               Mean episode length: 626.06
Episode_Reward/track_lin_vel_xy_exp: 0.2175
Episode_Reward/track_ang_vel_z_exp: 0.1823
       Episode_Reward/lin_vel_z_l2: -0.0167
      Episode_Reward/ang_vel_xy_l2: -0.0307
     Episode_Reward/dof_torques_l2: -0.1012
         Episode_Reward/dof_acc_l2: -0.0442
     Episode_Reward/action_rate_l2: -0.0457
      Episode_Reward/feet_air_time: -0.0372
 Episode_Reward/undesired_contacts: -0.0130
Episode_Reward/flat_orientation_l2: -0.0235
  Episode_Termination/base_contact: 1.0417
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 1.07s
                      Time elapsed: 00:03:12
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 180/1000 [0m                      

                       Computation: 10960 steps/s (collection: 1.069s, learning 0.052s)
             Mean action noise std: 0.42
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 6.4809
                       Mean reward: 1.18
               Mean episode length: 620.90
Episode_Reward/track_lin_vel_xy_exp: 0.2795
Episode_Reward/track_ang_vel_z_exp: 0.2562
       Episode_Reward/lin_vel_z_l2: -0.0177
      Episode_Reward/ang_vel_xy_l2: -0.0343
     Episode_Reward/dof_torques_l2: -0.1210
         Episode_Reward/dof_acc_l2: -0.0452
     Episode_Reward/action_rate_l2: -0.0529
      Episode_Reward/feet_air_time: -0.0380
 Episode_Reward/undesired_contacts: -0.0108
Episode_Reward/flat_orientation_l2: -0.0243
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 2224128
                    Iteration time: 1.12s
                      Time elapsed: 00:03:13
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 181/1000 [0m                      

                       Computation: 11209 steps/s (collection: 1.052s, learning 0.044s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 6.3684
                       Mean reward: 1.20
               Mean episode length: 642.01
Episode_Reward/track_lin_vel_xy_exp: 0.2127
Episode_Reward/track_ang_vel_z_exp: 0.1999
       Episode_Reward/lin_vel_z_l2: -0.0196
      Episode_Reward/ang_vel_xy_l2: -0.0372
     Episode_Reward/dof_torques_l2: -0.1024
         Episode_Reward/dof_acc_l2: -0.0600
     Episode_Reward/action_rate_l2: -0.0509
      Episode_Reward/feet_air_time: -0.0461
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: -0.0277
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 1.10s
                      Time elapsed: 00:03:15
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 182/1000 [0m                      

                       Computation: 11483 steps/s (collection: 1.022s, learning 0.048s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 6.2892
                       Mean reward: 1.43
               Mean episode length: 668.49
Episode_Reward/track_lin_vel_xy_exp: 0.2741
Episode_Reward/track_ang_vel_z_exp: 0.2387
       Episode_Reward/lin_vel_z_l2: -0.0201
      Episode_Reward/ang_vel_xy_l2: -0.0388
     Episode_Reward/dof_torques_l2: -0.1220
         Episode_Reward/dof_acc_l2: -0.0625
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0513
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: -0.0217
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2248704
                    Iteration time: 1.07s
                      Time elapsed: 00:03:16
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 183/1000 [0m                      

                       Computation: 10832 steps/s (collection: 1.088s, learning 0.046s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 6.2500
                       Mean reward: 1.02
               Mean episode length: 637.25
Episode_Reward/track_lin_vel_xy_exp: 0.0797
Episode_Reward/track_ang_vel_z_exp: 0.0661
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.0498
         Episode_Reward/dof_acc_l2: -0.0293
     Episode_Reward/action_rate_l2: -0.0222
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: -0.0225
  Episode_Termination/base_contact: 0.9167
      Episode_Termination/time_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.13s
                      Time elapsed: 00:03:17
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 184/1000 [0m                      

                       Computation: 11401 steps/s (collection: 1.033s, learning 0.044s)
             Mean action noise std: 0.41
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 6.2005
                       Mean reward: 1.26
               Mean episode length: 645.78
Episode_Reward/track_lin_vel_xy_exp: 0.1285
Episode_Reward/track_ang_vel_z_exp: 0.2072
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0290
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.0445
     Episode_Reward/action_rate_l2: -0.0406
      Episode_Reward/feet_air_time: -0.0366
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: -0.0213
  Episode_Termination/base_contact: 0.8333
      Episode_Termination/time_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2273280
                    Iteration time: 1.08s
                      Time elapsed: 00:03:18
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 185/1000 [0m                      

                       Computation: 10888 steps/s (collection: 1.076s, learning 0.052s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 6.1081
                       Mean reward: 1.36
               Mean episode length: 661.86
Episode_Reward/track_lin_vel_xy_exp: 0.1519
Episode_Reward/track_ang_vel_z_exp: 0.2813
       Episode_Reward/lin_vel_z_l2: -0.0227
      Episode_Reward/ang_vel_xy_l2: -0.0400
     Episode_Reward/dof_torques_l2: -0.1187
         Episode_Reward/dof_acc_l2: -0.0660
     Episode_Reward/action_rate_l2: -0.0584
      Episode_Reward/feet_air_time: -0.0546
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: -0.0209
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 1.13s
                      Time elapsed: 00:03:19
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 186/1000 [0m                      

                       Computation: 11170 steps/s (collection: 1.050s, learning 0.050s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 6.0348
                       Mean reward: 1.06
               Mean episode length: 629.97
Episode_Reward/track_lin_vel_xy_exp: 0.0708
Episode_Reward/track_ang_vel_z_exp: 0.1006
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.0574
         Episode_Reward/dof_acc_l2: -0.0321
     Episode_Reward/action_rate_l2: -0.0255
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: -0.0220
  Episode_Termination/base_contact: 1.2083
      Episode_Termination/time_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2297856
                    Iteration time: 1.10s
                      Time elapsed: 00:03:20
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 187/1000 [0m                      

                       Computation: 11424 steps/s (collection: 1.031s, learning 0.045s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 5.9931
                       Mean reward: 0.80
               Mean episode length: 574.48
Episode_Reward/track_lin_vel_xy_exp: 0.1902
Episode_Reward/track_ang_vel_z_exp: 0.1425
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0242
     Episode_Reward/dof_torques_l2: -0.0622
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0304
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0271
  Episode_Termination/base_contact: 1.2500
      Episode_Termination/time_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 1.08s
                      Time elapsed: 00:03:21
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 188/1000 [0m                      

                       Computation: 11280 steps/s (collection: 1.039s, learning 0.051s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 5.9192
                       Mean reward: 0.72
               Mean episode length: 574.17
Episode_Reward/track_lin_vel_xy_exp: 0.2620
Episode_Reward/track_ang_vel_z_exp: 0.2945
       Episode_Reward/lin_vel_z_l2: -0.0215
      Episode_Reward/ang_vel_xy_l2: -0.0395
     Episode_Reward/dof_torques_l2: -0.1186
         Episode_Reward/dof_acc_l2: -0.0604
     Episode_Reward/action_rate_l2: -0.0583
      Episode_Reward/feet_air_time: -0.0538
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2322432
                    Iteration time: 1.09s
                      Time elapsed: 00:03:22
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 189/1000 [0m                      

                       Computation: 11471 steps/s (collection: 1.026s, learning 0.045s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 5.8556
                       Mean reward: 0.72
               Mean episode length: 597.65
Episode_Reward/track_lin_vel_xy_exp: 0.1916
Episode_Reward/track_ang_vel_z_exp: 0.1674
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0306
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.0527
     Episode_Reward/action_rate_l2: -0.0417
      Episode_Reward/feet_air_time: -0.0416
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: -0.0221
  Episode_Termination/base_contact: 1.2917
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 1.07s
                      Time elapsed: 00:03:23
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 190/1000 [0m                      

                       Computation: 10882 steps/s (collection: 1.078s, learning 0.051s)
             Mean action noise std: 0.40
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 5.8233
                       Mean reward: 0.53
               Mean episode length: 574.62
Episode_Reward/track_lin_vel_xy_exp: 0.1455
Episode_Reward/track_ang_vel_z_exp: 0.1554
       Episode_Reward/lin_vel_z_l2: -0.0163
      Episode_Reward/ang_vel_xy_l2: -0.0290
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.0458
     Episode_Reward/action_rate_l2: -0.0391
      Episode_Reward/feet_air_time: -0.0312
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0220
  Episode_Termination/base_contact: 1.5417
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2347008
                    Iteration time: 1.13s
                      Time elapsed: 00:03:24
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 191/1000 [0m                      

                       Computation: 10760 steps/s (collection: 1.093s, learning 0.049s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 5.7798
                       Mean reward: 1.14
               Mean episode length: 665.22
Episode_Reward/track_lin_vel_xy_exp: 0.2287
Episode_Reward/track_ang_vel_z_exp: 0.1946
       Episode_Reward/lin_vel_z_l2: -0.0204
      Episode_Reward/ang_vel_xy_l2: -0.0337
     Episode_Reward/dof_torques_l2: -0.0981
         Episode_Reward/dof_acc_l2: -0.0502
     Episode_Reward/action_rate_l2: -0.0451
      Episode_Reward/feet_air_time: -0.0408
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: -0.0219
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.14s
                      Time elapsed: 00:03:26
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 192/1000 [0m                      

                       Computation: 10715 steps/s (collection: 1.095s, learning 0.052s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 5.6925
                       Mean reward: 0.87
               Mean episode length: 689.54
Episode_Reward/track_lin_vel_xy_exp: 0.1542
Episode_Reward/track_ang_vel_z_exp: 0.1921
       Episode_Reward/lin_vel_z_l2: -0.0204
      Episode_Reward/ang_vel_xy_l2: -0.0356
     Episode_Reward/dof_torques_l2: -0.1188
         Episode_Reward/dof_acc_l2: -0.0581
     Episode_Reward/action_rate_l2: -0.0526
      Episode_Reward/feet_air_time: -0.0480
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0227
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2371584
                    Iteration time: 1.15s
                      Time elapsed: 00:03:27
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 193/1000 [0m                      

                       Computation: 10742 steps/s (collection: 1.091s, learning 0.053s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 5.6634
                       Mean reward: 0.96
               Mean episode length: 698.48
Episode_Reward/track_lin_vel_xy_exp: 0.0898
Episode_Reward/track_ang_vel_z_exp: 0.2217
       Episode_Reward/lin_vel_z_l2: -0.0208
      Episode_Reward/ang_vel_xy_l2: -0.0339
     Episode_Reward/dof_torques_l2: -0.1021
         Episode_Reward/dof_acc_l2: -0.0520
     Episode_Reward/action_rate_l2: -0.0471
      Episode_Reward/feet_air_time: -0.0452
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: -0.0193
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 1.14s
                      Time elapsed: 00:03:28
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 194/1000 [0m                      

                       Computation: 10898 steps/s (collection: 1.079s, learning 0.049s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 5.6418
                       Mean reward: 0.96
               Mean episode length: 726.73
Episode_Reward/track_lin_vel_xy_exp: 0.1039
Episode_Reward/track_ang_vel_z_exp: 0.2569
       Episode_Reward/lin_vel_z_l2: -0.0203
      Episode_Reward/ang_vel_xy_l2: -0.0344
     Episode_Reward/dof_torques_l2: -0.1096
         Episode_Reward/dof_acc_l2: -0.0554
     Episode_Reward/action_rate_l2: -0.0508
      Episode_Reward/feet_air_time: -0.0472
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2396160
                    Iteration time: 1.13s
                      Time elapsed: 00:03:29
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 195/1000 [0m                      

                       Computation: 11179 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 5.6059
                       Mean reward: 0.84
               Mean episode length: 728.40
Episode_Reward/track_lin_vel_xy_exp: 0.1324
Episode_Reward/track_ang_vel_z_exp: 0.2711
       Episode_Reward/lin_vel_z_l2: -0.0220
      Episode_Reward/ang_vel_xy_l2: -0.0358
     Episode_Reward/dof_torques_l2: -0.1206
         Episode_Reward/dof_acc_l2: -0.0553
     Episode_Reward/action_rate_l2: -0.0528
      Episode_Reward/feet_air_time: -0.0489
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0202
  Episode_Termination/base_contact: 0.8750
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 1.10s
                      Time elapsed: 00:03:30
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 196/1000 [0m                      

                       Computation: 10827 steps/s (collection: 1.087s, learning 0.047s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 5.5741
                       Mean reward: 0.78
               Mean episode length: 713.87
Episode_Reward/track_lin_vel_xy_exp: 0.0854
Episode_Reward/track_ang_vel_z_exp: 0.1818
       Episode_Reward/lin_vel_z_l2: -0.0168
      Episode_Reward/ang_vel_xy_l2: -0.0276
     Episode_Reward/dof_torques_l2: -0.0916
         Episode_Reward/dof_acc_l2: -0.0418
     Episode_Reward/action_rate_l2: -0.0390
      Episode_Reward/feet_air_time: -0.0361
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0171
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2420736
                    Iteration time: 1.13s
                      Time elapsed: 00:03:31
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 197/1000 [0m                      

                       Computation: 11285 steps/s (collection: 1.043s, learning 0.046s)
             Mean action noise std: 0.39
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 5.5288
                       Mean reward: 1.10
               Mean episode length: 675.93
Episode_Reward/track_lin_vel_xy_exp: 0.1973
Episode_Reward/track_ang_vel_z_exp: 0.1070
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0216
     Episode_Reward/dof_torques_l2: -0.0679
         Episode_Reward/dof_acc_l2: -0.0366
     Episode_Reward/action_rate_l2: -0.0299
      Episode_Reward/feet_air_time: -0.0291
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.8333
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 1.09s
                      Time elapsed: 00:03:32
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 198/1000 [0m                      

                       Computation: 11550 steps/s (collection: 1.016s, learning 0.048s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 5.4635
                       Mean reward: 1.30
               Mean episode length: 664.40
Episode_Reward/track_lin_vel_xy_exp: 0.1160
Episode_Reward/track_ang_vel_z_exp: 0.1389
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.0633
         Episode_Reward/dof_acc_l2: -0.0344
     Episode_Reward/action_rate_l2: -0.0264
      Episode_Reward/feet_air_time: -0.0273
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 1.0000
      Episode_Termination/time_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2445312
                    Iteration time: 1.06s
                      Time elapsed: 00:03:33
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 199/1000 [0m                      

                       Computation: 11637 steps/s (collection: 1.010s, learning 0.046s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 5.3921
                       Mean reward: 1.43
               Mean episode length: 628.03
Episode_Reward/track_lin_vel_xy_exp: 0.1643
Episode_Reward/track_ang_vel_z_exp: 0.1834
       Episode_Reward/lin_vel_z_l2: -0.0167
      Episode_Reward/ang_vel_xy_l2: -0.0268
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.0446
     Episode_Reward/action_rate_l2: -0.0374
      Episode_Reward/feet_air_time: -0.0375
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0159
  Episode_Termination/base_contact: 1.0000
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.06s
                      Time elapsed: 00:03:34
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 200/1000 [0m                      

                       Computation: 11486 steps/s (collection: 1.011s, learning 0.059s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 5.3444
                       Mean reward: 1.12
               Mean episode length: 579.69
Episode_Reward/track_lin_vel_xy_exp: 0.0773
Episode_Reward/track_ang_vel_z_exp: 0.1187
       Episode_Reward/lin_vel_z_l2: -0.0151
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.0608
         Episode_Reward/dof_acc_l2: -0.0345
     Episode_Reward/action_rate_l2: -0.0246
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0174
  Episode_Termination/base_contact: 1.0833
      Episode_Termination/time_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2469888
                    Iteration time: 1.07s
                      Time elapsed: 00:03:35
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 201/1000 [0m                      

                       Computation: 11168 steps/s (collection: 1.037s, learning 0.063s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 5.2850
                       Mean reward: 1.37
               Mean episode length: 580.47
Episode_Reward/track_lin_vel_xy_exp: 0.1213
Episode_Reward/track_ang_vel_z_exp: 0.2548
       Episode_Reward/lin_vel_z_l2: -0.0180
      Episode_Reward/ang_vel_xy_l2: -0.0295
     Episode_Reward/dof_torques_l2: -0.0980
         Episode_Reward/dof_acc_l2: -0.0423
     Episode_Reward/action_rate_l2: -0.0419
      Episode_Reward/feet_air_time: -0.0394
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0127
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 1.10s
                      Time elapsed: 00:03:37
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 202/1000 [0m                      

                       Computation: 10978 steps/s (collection: 1.073s, learning 0.046s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 5.2335
                       Mean reward: 1.35
               Mean episode length: 604.70
Episode_Reward/track_lin_vel_xy_exp: 0.1668
Episode_Reward/track_ang_vel_z_exp: 0.2055
       Episode_Reward/lin_vel_z_l2: -0.0170
      Episode_Reward/ang_vel_xy_l2: -0.0284
     Episode_Reward/dof_torques_l2: -0.0959
         Episode_Reward/dof_acc_l2: -0.0433
     Episode_Reward/action_rate_l2: -0.0404
      Episode_Reward/feet_air_time: -0.0368
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0140
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2494464
                    Iteration time: 1.12s
                      Time elapsed: 00:03:38
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 203/1000 [0m                      

                       Computation: 11121 steps/s (collection: 1.056s, learning 0.049s)
             Mean action noise std: 0.38
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 5.1892
                       Mean reward: 1.34
               Mean episode length: 615.53
Episode_Reward/track_lin_vel_xy_exp: 0.1694
Episode_Reward/track_ang_vel_z_exp: 0.1773
       Episode_Reward/lin_vel_z_l2: -0.0160
      Episode_Reward/ang_vel_xy_l2: -0.0252
     Episode_Reward/dof_torques_l2: -0.0775
         Episode_Reward/dof_acc_l2: -0.0391
     Episode_Reward/action_rate_l2: -0.0342
      Episode_Reward/feet_air_time: -0.0352
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0117
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 1.10s
                      Time elapsed: 00:03:39
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 204/1000 [0m                      

                       Computation: 11464 steps/s (collection: 1.018s, learning 0.054s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 5.1411
                       Mean reward: 1.53
               Mean episode length: 646.11
Episode_Reward/track_lin_vel_xy_exp: 0.1687
Episode_Reward/track_ang_vel_z_exp: 0.2176
       Episode_Reward/lin_vel_z_l2: -0.0196
      Episode_Reward/ang_vel_xy_l2: -0.0321
     Episode_Reward/dof_torques_l2: -0.1029
         Episode_Reward/dof_acc_l2: -0.0496
     Episode_Reward/action_rate_l2: -0.0444
      Episode_Reward/feet_air_time: -0.0430
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0139
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2519040
                    Iteration time: 1.07s
                      Time elapsed: 00:03:40
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 205/1000 [0m                      

                       Computation: 10845 steps/s (collection: 1.068s, learning 0.065s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 5.0839
                       Mean reward: 1.92
               Mean episode length: 667.87
Episode_Reward/track_lin_vel_xy_exp: 0.3227
Episode_Reward/track_ang_vel_z_exp: 0.2364
       Episode_Reward/lin_vel_z_l2: -0.0234
      Episode_Reward/ang_vel_xy_l2: -0.0358
     Episode_Reward/dof_torques_l2: -0.1224
         Episode_Reward/dof_acc_l2: -0.0567
     Episode_Reward/action_rate_l2: -0.0501
      Episode_Reward/feet_air_time: -0.0477
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 1.13s
                      Time elapsed: 00:03:41
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 206/1000 [0m                      

                       Computation: 11408 steps/s (collection: 1.023s, learning 0.054s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 5.0021
                       Mean reward: 2.03
               Mean episode length: 705.88
Episode_Reward/track_lin_vel_xy_exp: 0.3586
Episode_Reward/track_ang_vel_z_exp: 0.2122
       Episode_Reward/lin_vel_z_l2: -0.0169
      Episode_Reward/ang_vel_xy_l2: -0.0295
     Episode_Reward/dof_torques_l2: -0.1039
         Episode_Reward/dof_acc_l2: -0.0454
     Episode_Reward/action_rate_l2: -0.0425
      Episode_Reward/feet_air_time: -0.0404
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0145
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2543616
                    Iteration time: 1.08s
                      Time elapsed: 00:03:42
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 207/1000 [0m                      

                       Computation: 11019 steps/s (collection: 1.067s, learning 0.048s)
             Mean action noise std: 0.37
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 4.9279
                       Mean reward: 2.20
               Mean episode length: 688.76
Episode_Reward/track_lin_vel_xy_exp: 0.0945
Episode_Reward/track_ang_vel_z_exp: 0.2245
       Episode_Reward/lin_vel_z_l2: -0.0179
      Episode_Reward/ang_vel_xy_l2: -0.0269
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.0469
     Episode_Reward/action_rate_l2: -0.0378
      Episode_Reward/feet_air_time: -0.0424
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.12s
                      Time elapsed: 00:03:43
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 208/1000 [0m                      

                       Computation: 11413 steps/s (collection: 1.030s, learning 0.047s)
             Mean action noise std: 0.36
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 4.8215
                       Mean reward: 2.35
               Mean episode length: 690.70
Episode_Reward/track_lin_vel_xy_exp: 0.1812
Episode_Reward/track_ang_vel_z_exp: 0.2457
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0256
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.0343
     Episode_Reward/action_rate_l2: -0.0348
      Episode_Reward/feet_air_time: -0.0317
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0149
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2568192
                    Iteration time: 1.08s
                      Time elapsed: 00:03:44
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 209/1000 [0m                      

                       Computation: 10967 steps/s (collection: 1.075s, learning 0.045s)
             Mean action noise std: 0.36
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 4.7446
                       Mean reward: 2.26
               Mean episode length: 681.95
Episode_Reward/track_lin_vel_xy_exp: 0.0999
Episode_Reward/track_ang_vel_z_exp: 0.1342
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.0740
         Episode_Reward/dof_acc_l2: -0.0320
     Episode_Reward/action_rate_l2: -0.0272
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 1.2917
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 1.12s
                      Time elapsed: 00:03:45
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 210/1000 [0m                      

                       Computation: 10778 steps/s (collection: 1.093s, learning 0.047s)
             Mean action noise std: 0.36
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 4.6695
                       Mean reward: 2.62
               Mean episode length: 706.01
Episode_Reward/track_lin_vel_xy_exp: 0.3263
Episode_Reward/track_ang_vel_z_exp: 0.2963
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0346
     Episode_Reward/dof_torques_l2: -0.1216
         Episode_Reward/dof_acc_l2: -0.0483
     Episode_Reward/action_rate_l2: -0.0484
      Episode_Reward/feet_air_time: -0.0449
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2592768
                    Iteration time: 1.14s
                      Time elapsed: 00:03:47
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 211/1000 [0m                      

                       Computation: 11023 steps/s (collection: 1.058s, learning 0.057s)
             Mean action noise std: 0.36
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 4.6158
                       Mean reward: 2.60
               Mean episode length: 737.13
Episode_Reward/track_lin_vel_xy_exp: 0.1831
Episode_Reward/track_ang_vel_z_exp: 0.2911
       Episode_Reward/lin_vel_z_l2: -0.0239
      Episode_Reward/ang_vel_xy_l2: -0.0361
     Episode_Reward/dof_torques_l2: -0.1188
         Episode_Reward/dof_acc_l2: -0.0534
     Episode_Reward/action_rate_l2: -0.0462
      Episode_Reward/feet_air_time: -0.0461
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 1.11s
                      Time elapsed: 00:03:48
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 212/1000 [0m                      

                       Computation: 10500 steps/s (collection: 1.118s, learning 0.053s)
             Mean action noise std: 0.36
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 4.5548
                       Mean reward: 2.42
               Mean episode length: 721.95
Episode_Reward/track_lin_vel_xy_exp: 0.1924
Episode_Reward/track_ang_vel_z_exp: 0.2721
       Episode_Reward/lin_vel_z_l2: -0.0203
      Episode_Reward/ang_vel_xy_l2: -0.0346
     Episode_Reward/dof_torques_l2: -0.1139
         Episode_Reward/dof_acc_l2: -0.0510
     Episode_Reward/action_rate_l2: -0.0461
      Episode_Reward/feet_air_time: -0.0485
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0140
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2617344
                    Iteration time: 1.17s
                      Time elapsed: 00:03:49
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 213/1000 [0m                      

                       Computation: 11148 steps/s (collection: 1.050s, learning 0.052s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 4.4957
                       Mean reward: 2.48
               Mean episode length: 729.76
Episode_Reward/track_lin_vel_xy_exp: 0.4282
Episode_Reward/track_ang_vel_z_exp: 0.1732
       Episode_Reward/lin_vel_z_l2: -0.0172
      Episode_Reward/ang_vel_xy_l2: -0.0321
     Episode_Reward/dof_torques_l2: -0.1152
         Episode_Reward/dof_acc_l2: -0.0456
     Episode_Reward/action_rate_l2: -0.0425
      Episode_Reward/feet_air_time: -0.0328
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0188
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 1.10s
                      Time elapsed: 00:03:50
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 214/1000 [0m                      

                       Computation: 11258 steps/s (collection: 1.044s, learning 0.047s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 4.4014
                       Mean reward: 2.78
               Mean episode length: 744.51
Episode_Reward/track_lin_vel_xy_exp: 0.1780
Episode_Reward/track_ang_vel_z_exp: 0.1656
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0262
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.0372
     Episode_Reward/action_rate_l2: -0.0352
      Episode_Reward/feet_air_time: -0.0343
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2641920
                    Iteration time: 1.09s
                      Time elapsed: 00:03:51
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 215/1000 [0m                      

                       Computation: 11576 steps/s (collection: 1.011s, learning 0.051s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 4.3340
                       Mean reward: 2.71
               Mean episode length: 735.94
Episode_Reward/track_lin_vel_xy_exp: 0.1101
Episode_Reward/track_ang_vel_z_exp: 0.1391
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.0249
     Episode_Reward/action_rate_l2: -0.0215
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0105
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.06s
                      Time elapsed: 00:03:52
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 216/1000 [0m                      

                       Computation: 11080 steps/s (collection: 1.062s, learning 0.047s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 4.2790
                       Mean reward: 3.06
               Mean episode length: 751.30
Episode_Reward/track_lin_vel_xy_exp: 0.1599
Episode_Reward/track_ang_vel_z_exp: 0.2353
       Episode_Reward/lin_vel_z_l2: -0.0159
      Episode_Reward/ang_vel_xy_l2: -0.0257
     Episode_Reward/dof_torques_l2: -0.0823
         Episode_Reward/dof_acc_l2: -0.0394
     Episode_Reward/action_rate_l2: -0.0335
      Episode_Reward/feet_air_time: -0.0370
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0130
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 2666496
                    Iteration time: 1.11s
                      Time elapsed: 00:03:53
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 217/1000 [0m                      

                       Computation: 11175 steps/s (collection: 1.055s, learning 0.045s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 4.2369
                       Mean reward: 2.88
               Mean episode length: 734.93
Episode_Reward/track_lin_vel_xy_exp: 0.0897
Episode_Reward/track_ang_vel_z_exp: 0.3087
       Episode_Reward/lin_vel_z_l2: -0.0195
      Episode_Reward/ang_vel_xy_l2: -0.0334
     Episode_Reward/dof_torques_l2: -0.1118
         Episode_Reward/dof_acc_l2: -0.0432
     Episode_Reward/action_rate_l2: -0.0413
      Episode_Reward/feet_air_time: -0.0397
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 1.10s
                      Time elapsed: 00:03:54
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 218/1000 [0m                      

                       Computation: 11631 steps/s (collection: 1.004s, learning 0.053s)
             Mean action noise std: 0.35
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 4.1838
                       Mean reward: 2.42
               Mean episode length: 722.44
Episode_Reward/track_lin_vel_xy_exp: 0.1149
Episode_Reward/track_ang_vel_z_exp: 0.2255
       Episode_Reward/lin_vel_z_l2: -0.0185
      Episode_Reward/ang_vel_xy_l2: -0.0304
     Episode_Reward/dof_torques_l2: -0.1149
         Episode_Reward/dof_acc_l2: -0.0397
     Episode_Reward/action_rate_l2: -0.0391
      Episode_Reward/feet_air_time: -0.0371
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2691072
                    Iteration time: 1.06s
                      Time elapsed: 00:03:55
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 219/1000 [0m                      

                       Computation: 11130 steps/s (collection: 1.060s, learning 0.044s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 4.0993
                       Mean reward: 2.62
               Mean episode length: 710.37
Episode_Reward/track_lin_vel_xy_exp: 0.2312
Episode_Reward/track_ang_vel_z_exp: 0.3057
       Episode_Reward/lin_vel_z_l2: -0.0214
      Episode_Reward/ang_vel_xy_l2: -0.0371
     Episode_Reward/dof_torques_l2: -0.1252
         Episode_Reward/dof_acc_l2: -0.0502
     Episode_Reward/action_rate_l2: -0.0474
      Episode_Reward/feet_air_time: -0.0484
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 1.10s
                      Time elapsed: 00:03:56
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 220/1000 [0m                      

                       Computation: 11465 steps/s (collection: 1.025s, learning 0.047s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 4.0296
                       Mean reward: 2.41
               Mean episode length: 740.32
Episode_Reward/track_lin_vel_xy_exp: 0.1610
Episode_Reward/track_ang_vel_z_exp: 0.2795
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0337
     Episode_Reward/dof_torques_l2: -0.1195
         Episode_Reward/dof_acc_l2: -0.0477
     Episode_Reward/action_rate_l2: -0.0435
      Episode_Reward/feet_air_time: -0.0439
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0146
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2715648
                    Iteration time: 1.07s
                      Time elapsed: 00:03:58
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 221/1000 [0m                      

                       Computation: 11077 steps/s (collection: 1.065s, learning 0.044s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 3.9478
                       Mean reward: 2.73
               Mean episode length: 775.16
Episode_Reward/track_lin_vel_xy_exp: 0.2892
Episode_Reward/track_ang_vel_z_exp: 0.2343
       Episode_Reward/lin_vel_z_l2: -0.0213
      Episode_Reward/ang_vel_xy_l2: -0.0308
     Episode_Reward/dof_torques_l2: -0.1149
         Episode_Reward/dof_acc_l2: -0.0418
     Episode_Reward/action_rate_l2: -0.0369
      Episode_Reward/feet_air_time: -0.0342
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: -0.0266
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 1.11s
                      Time elapsed: 00:03:59
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 222/1000 [0m                      

                       Computation: 11290 steps/s (collection: 1.023s, learning 0.066s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 3.9024
                       Mean reward: 2.95
               Mean episode length: 792.57
Episode_Reward/track_lin_vel_xy_exp: 0.2708
Episode_Reward/track_ang_vel_z_exp: 0.3049
       Episode_Reward/lin_vel_z_l2: -0.0195
      Episode_Reward/ang_vel_xy_l2: -0.0311
     Episode_Reward/dof_torques_l2: -0.1200
         Episode_Reward/dof_acc_l2: -0.0430
     Episode_Reward/action_rate_l2: -0.0415
      Episode_Reward/feet_air_time: -0.0385
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2740224
                    Iteration time: 1.09s
                      Time elapsed: 00:04:00
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 223/1000 [0m                      

                       Computation: 10861 steps/s (collection: 1.084s, learning 0.047s)
             Mean action noise std: 0.34
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 3.8435
                       Mean reward: 2.75
               Mean episode length: 776.02
Episode_Reward/track_lin_vel_xy_exp: 0.1477
Episode_Reward/track_ang_vel_z_exp: 0.2221
       Episode_Reward/lin_vel_z_l2: -0.0186
      Episode_Reward/ang_vel_xy_l2: -0.0239
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.0394
     Episode_Reward/action_rate_l2: -0.0298
      Episode_Reward/feet_air_time: -0.0327
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.13s
                      Time elapsed: 00:04:01
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 224/1000 [0m                      

                       Computation: 11244 steps/s (collection: 1.046s, learning 0.047s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 3.7908
                       Mean reward: 2.45
               Mean episode length: 760.53
Episode_Reward/track_lin_vel_xy_exp: 0.0590
Episode_Reward/track_ang_vel_z_exp: 0.1810
       Episode_Reward/lin_vel_z_l2: -0.0177
      Episode_Reward/ang_vel_xy_l2: -0.0275
     Episode_Reward/dof_torques_l2: -0.1005
         Episode_Reward/dof_acc_l2: -0.0397
     Episode_Reward/action_rate_l2: -0.0330
      Episode_Reward/feet_air_time: -0.0356
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0221
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2764800
                    Iteration time: 1.09s
                      Time elapsed: 00:04:02
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 225/1000 [0m                      

                       Computation: 11443 steps/s (collection: 1.029s, learning 0.045s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 3.7201
                       Mean reward: 2.56
               Mean episode length: 734.14
Episode_Reward/track_lin_vel_xy_exp: 0.1124
Episode_Reward/track_ang_vel_z_exp: 0.1426
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0193
     Episode_Reward/dof_torques_l2: -0.0572
         Episode_Reward/dof_acc_l2: -0.0257
     Episode_Reward/action_rate_l2: -0.0213
      Episode_Reward/feet_air_time: -0.0260
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0167
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 1.07s
                      Time elapsed: 00:04:03
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 226/1000 [0m                      

                       Computation: 10973 steps/s (collection: 1.067s, learning 0.053s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 3.6469
                       Mean reward: 2.50
               Mean episode length: 722.51
Episode_Reward/track_lin_vel_xy_exp: 0.1303
Episode_Reward/track_ang_vel_z_exp: 0.1709
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0253
     Episode_Reward/dof_torques_l2: -0.0895
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0311
      Episode_Reward/feet_air_time: -0.0352
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0175
  Episode_Termination/base_contact: 0.8750
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2789376
                    Iteration time: 1.12s
                      Time elapsed: 00:04:04
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 227/1000 [0m                      

                       Computation: 11193 steps/s (collection: 1.049s, learning 0.049s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 3.5461
                       Mean reward: 2.76
               Mean episode length: 725.84
Episode_Reward/track_lin_vel_xy_exp: 0.1604
Episode_Reward/track_ang_vel_z_exp: 0.2886
       Episode_Reward/lin_vel_z_l2: -0.0193
      Episode_Reward/ang_vel_xy_l2: -0.0317
     Episode_Reward/dof_torques_l2: -0.1234
         Episode_Reward/dof_acc_l2: -0.0409
     Episode_Reward/action_rate_l2: -0.0403
      Episode_Reward/feet_air_time: -0.0387
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0140
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 1.10s
                      Time elapsed: 00:04:05
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 228/1000 [0m                      

                       Computation: 10879 steps/s (collection: 1.076s, learning 0.053s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 3.4999
                       Mean reward: 2.50
               Mean episode length: 659.72
Episode_Reward/track_lin_vel_xy_exp: 0.1491
Episode_Reward/track_ang_vel_z_exp: 0.2002
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0223
     Episode_Reward/dof_torques_l2: -0.0821
         Episode_Reward/dof_acc_l2: -0.0315
     Episode_Reward/action_rate_l2: -0.0273
      Episode_Reward/feet_air_time: -0.0292
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0143
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2813952
                    Iteration time: 1.13s
                      Time elapsed: 00:04:06
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 229/1000 [0m                      

                       Computation: 10504 steps/s (collection: 1.110s, learning 0.060s)
             Mean action noise std: 0.33
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 3.4552
                       Mean reward: 2.85
               Mean episode length: 676.64
Episode_Reward/track_lin_vel_xy_exp: 0.2686
Episode_Reward/track_ang_vel_z_exp: 0.2535
       Episode_Reward/lin_vel_z_l2: -0.0161
      Episode_Reward/ang_vel_xy_l2: -0.0250
     Episode_Reward/dof_torques_l2: -0.0942
         Episode_Reward/dof_acc_l2: -0.0330
     Episode_Reward/action_rate_l2: -0.0314
      Episode_Reward/feet_air_time: -0.0301
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 1.17s
                      Time elapsed: 00:04:08
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 230/1000 [0m                      

                       Computation: 11644 steps/s (collection: 1.008s, learning 0.048s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 3.4004
                       Mean reward: 2.93
               Mean episode length: 713.16
Episode_Reward/track_lin_vel_xy_exp: 0.0854
Episode_Reward/track_ang_vel_z_exp: 0.2294
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.0947
         Episode_Reward/dof_acc_l2: -0.0347
     Episode_Reward/action_rate_l2: -0.0300
      Episode_Reward/feet_air_time: -0.0303
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2838528
                    Iteration time: 1.06s
                      Time elapsed: 00:04:09
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 231/1000 [0m                      

                       Computation: 11443 steps/s (collection: 1.027s, learning 0.047s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 3.3611
                       Mean reward: 3.19
               Mean episode length: 747.24
Episode_Reward/track_lin_vel_xy_exp: 0.1504
Episode_Reward/track_ang_vel_z_exp: 0.2539
       Episode_Reward/lin_vel_z_l2: -0.0193
      Episode_Reward/ang_vel_xy_l2: -0.0271
     Episode_Reward/dof_torques_l2: -0.0973
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0331
      Episode_Reward/feet_air_time: -0.0354
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: -0.0143
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.07s
                      Time elapsed: 00:04:10
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 232/1000 [0m                      

                       Computation: 11338 steps/s (collection: 1.039s, learning 0.045s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 3.3458
                       Mean reward: 3.23
               Mean episode length: 728.12
Episode_Reward/track_lin_vel_xy_exp: 0.2098
Episode_Reward/track_ang_vel_z_exp: 0.2512
       Episode_Reward/lin_vel_z_l2: -0.0161
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.0920
         Episode_Reward/dof_acc_l2: -0.0306
     Episode_Reward/action_rate_l2: -0.0287
      Episode_Reward/feet_air_time: -0.0269
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2863104
                    Iteration time: 1.08s
                      Time elapsed: 00:04:11
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 233/1000 [0m                      

                       Computation: 11139 steps/s (collection: 1.057s, learning 0.046s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 3.3249
                       Mean reward: 3.16
               Mean episode length: 722.89
Episode_Reward/track_lin_vel_xy_exp: 0.1240
Episode_Reward/track_ang_vel_z_exp: 0.2652
       Episode_Reward/lin_vel_z_l2: -0.0188
      Episode_Reward/ang_vel_xy_l2: -0.0268
     Episode_Reward/dof_torques_l2: -0.0989
         Episode_Reward/dof_acc_l2: -0.0365
     Episode_Reward/action_rate_l2: -0.0334
      Episode_Reward/feet_air_time: -0.0354
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0136
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 1.10s
                      Time elapsed: 00:04:12
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 234/1000 [0m                      

                       Computation: 11409 steps/s (collection: 1.021s, learning 0.056s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 3.2661
                       Mean reward: 3.49
               Mean episode length: 701.58
Episode_Reward/track_lin_vel_xy_exp: 0.1970
Episode_Reward/track_ang_vel_z_exp: 0.2612
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0244
     Episode_Reward/dof_torques_l2: -0.0919
         Episode_Reward/dof_acc_l2: -0.0334
     Episode_Reward/action_rate_l2: -0.0320
      Episode_Reward/feet_air_time: -0.0341
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: -0.0134
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2887680
                    Iteration time: 1.08s
                      Time elapsed: 00:04:13
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 235/1000 [0m                      

                       Computation: 11388 steps/s (collection: 1.035s, learning 0.044s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 3.2184
                       Mean reward: 2.92
               Mean episode length: 695.88
Episode_Reward/track_lin_vel_xy_exp: 0.0840
Episode_Reward/track_ang_vel_z_exp: 0.1977
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0242
     Episode_Reward/dof_torques_l2: -0.1023
         Episode_Reward/dof_acc_l2: -0.0362
     Episode_Reward/action_rate_l2: -0.0320
      Episode_Reward/feet_air_time: -0.0354
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0136
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 1.08s
                      Time elapsed: 00:04:14
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 236/1000 [0m                      

                       Computation: 11338 steps/s (collection: 1.034s, learning 0.049s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 3.1780
                       Mean reward: 2.86
               Mean episode length: 713.94
Episode_Reward/track_lin_vel_xy_exp: 0.1985
Episode_Reward/track_ang_vel_z_exp: 0.2824
       Episode_Reward/lin_vel_z_l2: -0.0191
      Episode_Reward/ang_vel_xy_l2: -0.0277
     Episode_Reward/dof_torques_l2: -0.1142
         Episode_Reward/dof_acc_l2: -0.0365
     Episode_Reward/action_rate_l2: -0.0353
      Episode_Reward/feet_air_time: -0.0352
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2912256
                    Iteration time: 1.08s
                      Time elapsed: 00:04:15
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 237/1000 [0m                      

                       Computation: 11339 steps/s (collection: 1.026s, learning 0.058s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 3.1167
                       Mean reward: 2.93
               Mean episode length: 741.42
Episode_Reward/track_lin_vel_xy_exp: 0.1084
Episode_Reward/track_ang_vel_z_exp: 0.2345
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.0982
         Episode_Reward/dof_acc_l2: -0.0255
     Episode_Reward/action_rate_l2: -0.0276
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 1.08s
                      Time elapsed: 00:04:16
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 238/1000 [0m                      

                       Computation: 11535 steps/s (collection: 1.016s, learning 0.049s)
             Mean action noise std: 0.32
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 3.0303
                       Mean reward: 2.81
               Mean episode length: 730.79
Episode_Reward/track_lin_vel_xy_exp: 0.1691
Episode_Reward/track_ang_vel_z_exp: 0.2194
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0215
     Episode_Reward/dof_torques_l2: -0.0962
         Episode_Reward/dof_acc_l2: -0.0284
     Episode_Reward/action_rate_l2: -0.0274
      Episode_Reward/feet_air_time: -0.0268
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2936832
                    Iteration time: 1.07s
                      Time elapsed: 00:04:17
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 239/1000 [0m                      

                       Computation: 11581 steps/s (collection: 1.017s, learning 0.044s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 2.9747
                       Mean reward: 2.94
               Mean episode length: 707.22
Episode_Reward/track_lin_vel_xy_exp: 0.1363
Episode_Reward/track_ang_vel_z_exp: 0.1982
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0803
         Episode_Reward/dof_acc_l2: -0.0247
     Episode_Reward/action_rate_l2: -0.0231
      Episode_Reward/feet_air_time: -0.0218
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0123
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.06s
                      Time elapsed: 00:04:18
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 240/1000 [0m                      

                       Computation: 11046 steps/s (collection: 1.063s, learning 0.050s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 2.9309
                       Mean reward: 2.55
               Mean episode length: 638.45
Episode_Reward/track_lin_vel_xy_exp: 0.0778
Episode_Reward/track_ang_vel_z_exp: 0.1641
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.0775
         Episode_Reward/dof_acc_l2: -0.0279
     Episode_Reward/action_rate_l2: -0.0207
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 0.9167
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 2961408
                    Iteration time: 1.11s
                      Time elapsed: 00:04:19
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 241/1000 [0m                      

                       Computation: 11175 steps/s (collection: 1.050s, learning 0.050s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 2.8823
                       Mean reward: 2.89
               Mean episode length: 653.92
Episode_Reward/track_lin_vel_xy_exp: 0.2211
Episode_Reward/track_ang_vel_z_exp: 0.3498
       Episode_Reward/lin_vel_z_l2: -0.0163
      Episode_Reward/ang_vel_xy_l2: -0.0274
     Episode_Reward/dof_torques_l2: -0.1180
         Episode_Reward/dof_acc_l2: -0.0331
     Episode_Reward/action_rate_l2: -0.0367
      Episode_Reward/feet_air_time: -0.0363
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0113
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 1.10s
                      Time elapsed: 00:04:21
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 242/1000 [0m                      

                       Computation: 11353 steps/s (collection: 1.038s, learning 0.044s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 2.8045
                       Mean reward: 3.00
               Mean episode length: 652.15
Episode_Reward/track_lin_vel_xy_exp: 0.1479
Episode_Reward/track_ang_vel_z_exp: 0.2645
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0214
     Episode_Reward/dof_torques_l2: -0.0806
         Episode_Reward/dof_acc_l2: -0.0270
     Episode_Reward/action_rate_l2: -0.0271
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0130
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2985984
                    Iteration time: 1.08s
                      Time elapsed: 00:04:22
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 243/1000 [0m                      

                       Computation: 10589 steps/s (collection: 1.114s, learning 0.046s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 2.7456
                       Mean reward: 3.23
               Mean episode length: 660.58
Episode_Reward/track_lin_vel_xy_exp: 0.1398
Episode_Reward/track_ang_vel_z_exp: 0.2490
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0232
     Episode_Reward/dof_torques_l2: -0.0972
         Episode_Reward/dof_acc_l2: -0.0294
     Episode_Reward/action_rate_l2: -0.0297
      Episode_Reward/feet_air_time: -0.0292
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0141
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 1.16s
                      Time elapsed: 00:04:23
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 244/1000 [0m                      

                       Computation: 11203 steps/s (collection: 1.051s, learning 0.046s)
             Mean action noise std: 0.31
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 2.6780
                       Mean reward: 3.28
               Mean episode length: 705.06
Episode_Reward/track_lin_vel_xy_exp: 0.0926
Episode_Reward/track_ang_vel_z_exp: 0.2356
       Episode_Reward/lin_vel_z_l2: -0.0138
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.0296
     Episode_Reward/action_rate_l2: -0.0277
      Episode_Reward/feet_air_time: -0.0305
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0131
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3010560
                    Iteration time: 1.10s
                      Time elapsed: 00:04:24
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 245/1000 [0m                      

                       Computation: 11348 steps/s (collection: 1.033s, learning 0.050s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 2.6092
                       Mean reward: 3.58
               Mean episode length: 707.82
Episode_Reward/track_lin_vel_xy_exp: 0.1086
Episode_Reward/track_ang_vel_z_exp: 0.3062
       Episode_Reward/lin_vel_z_l2: -0.0152
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.0279
     Episode_Reward/action_rate_l2: -0.0294
      Episode_Reward/feet_air_time: -0.0304
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0118
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 1.08s
                      Time elapsed: 00:04:25
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 246/1000 [0m                      

                       Computation: 10848 steps/s (collection: 1.089s, learning 0.044s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 2.5471
                       Mean reward: 3.88
               Mean episode length: 729.06
Episode_Reward/track_lin_vel_xy_exp: 0.1497
Episode_Reward/track_ang_vel_z_exp: 0.2833
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0220
     Episode_Reward/dof_torques_l2: -0.0908
         Episode_Reward/dof_acc_l2: -0.0252
     Episode_Reward/action_rate_l2: -0.0297
      Episode_Reward/feet_air_time: -0.0283
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0091
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3035136
                    Iteration time: 1.13s
                      Time elapsed: 00:04:26
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 247/1000 [0m                      

                       Computation: 10689 steps/s (collection: 1.097s, learning 0.052s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 2.4814
                       Mean reward: 3.59
               Mean episode length: 692.36
Episode_Reward/track_lin_vel_xy_exp: 0.1573
Episode_Reward/track_ang_vel_z_exp: 0.2201
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0725
         Episode_Reward/dof_acc_l2: -0.0235
     Episode_Reward/action_rate_l2: -0.0219
      Episode_Reward/feet_air_time: -0.0233
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0110
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.15s
                      Time elapsed: 00:04:27
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 248/1000 [0m                      

                       Computation: 10837 steps/s (collection: 1.089s, learning 0.044s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 2.4312
                       Mean reward: 3.40
               Mean episode length: 671.46
Episode_Reward/track_lin_vel_xy_exp: 0.1537
Episode_Reward/track_ang_vel_z_exp: 0.1821
       Episode_Reward/lin_vel_z_l2: -0.0169
      Episode_Reward/ang_vel_xy_l2: -0.0169
     Episode_Reward/dof_torques_l2: -0.0736
         Episode_Reward/dof_acc_l2: -0.0312
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0242
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0127
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3059712
                    Iteration time: 1.13s
                      Time elapsed: 00:04:28
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 249/1000 [0m                      

                       Computation: 10831 steps/s (collection: 1.078s, learning 0.056s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 2.3875
                       Mean reward: 3.72
               Mean episode length: 698.32
Episode_Reward/track_lin_vel_xy_exp: 0.3001
Episode_Reward/track_ang_vel_z_exp: 0.3332
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0262
     Episode_Reward/dof_torques_l2: -0.1139
         Episode_Reward/dof_acc_l2: -0.0357
     Episode_Reward/action_rate_l2: -0.0323
      Episode_Reward/feet_air_time: -0.0336
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0135
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 1.13s
                      Time elapsed: 00:04:29
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 250/1000 [0m                      

                       Computation: 11099 steps/s (collection: 1.048s, learning 0.059s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 2.3563
                       Mean reward: 3.82
               Mean episode length: 697.38
Episode_Reward/track_lin_vel_xy_exp: 0.2429
Episode_Reward/track_ang_vel_z_exp: 0.2841
       Episode_Reward/lin_vel_z_l2: -0.0165
      Episode_Reward/ang_vel_xy_l2: -0.0238
     Episode_Reward/dof_torques_l2: -0.1123
         Episode_Reward/dof_acc_l2: -0.0374
     Episode_Reward/action_rate_l2: -0.0306
      Episode_Reward/feet_air_time: -0.0338
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3084288
                    Iteration time: 1.11s
                      Time elapsed: 00:04:31
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 251/1000 [0m                      

                       Computation: 11230 steps/s (collection: 1.043s, learning 0.051s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 2.3329
                       Mean reward: 3.82
               Mean episode length: 680.96
Episode_Reward/track_lin_vel_xy_exp: 0.0869
Episode_Reward/track_ang_vel_z_exp: 0.2808
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1006
         Episode_Reward/dof_acc_l2: -0.0294
     Episode_Reward/action_rate_l2: -0.0267
      Episode_Reward/feet_air_time: -0.0275
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 1.09s
                      Time elapsed: 00:04:32
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 252/1000 [0m                      

                       Computation: 11188 steps/s (collection: 1.046s, learning 0.053s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 2.2960
                       Mean reward: 3.79
               Mean episode length: 674.42
Episode_Reward/track_lin_vel_xy_exp: 0.2666
Episode_Reward/track_ang_vel_z_exp: 0.2848
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0223
     Episode_Reward/dof_torques_l2: -0.0974
         Episode_Reward/dof_acc_l2: -0.0280
     Episode_Reward/action_rate_l2: -0.0273
      Episode_Reward/feet_air_time: -0.0273
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3108864
                    Iteration time: 1.10s
                      Time elapsed: 00:04:33
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 253/1000 [0m                      

                       Computation: 11321 steps/s (collection: 1.030s, learning 0.056s)
             Mean action noise std: 0.30
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 2.2394
                       Mean reward: 3.75
               Mean episode length: 681.24
Episode_Reward/track_lin_vel_xy_exp: 0.2452
Episode_Reward/track_ang_vel_z_exp: 0.2449
       Episode_Reward/lin_vel_z_l2: -0.0172
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.0926
         Episode_Reward/dof_acc_l2: -0.0330
     Episode_Reward/action_rate_l2: -0.0272
      Episode_Reward/feet_air_time: -0.0292
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0140
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 1.09s
                      Time elapsed: 00:04:34
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 254/1000 [0m                      

                       Computation: 10503 steps/s (collection: 1.103s, learning 0.067s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 2.1979
                       Mean reward: 3.77
               Mean episode length: 666.72
Episode_Reward/track_lin_vel_xy_exp: 0.1519
Episode_Reward/track_ang_vel_z_exp: 0.2877
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.0943
         Episode_Reward/dof_acc_l2: -0.0261
     Episode_Reward/action_rate_l2: -0.0264
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 3133440
                    Iteration time: 1.17s
                      Time elapsed: 00:04:35
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 255/1000 [0m                      

                       Computation: 10875 steps/s (collection: 1.082s, learning 0.048s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 2.1599
                       Mean reward: 3.36
               Mean episode length: 707.83
Episode_Reward/track_lin_vel_xy_exp: 0.0596
Episode_Reward/track_ang_vel_z_exp: 0.3201
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.1015
         Episode_Reward/dof_acc_l2: -0.0303
     Episode_Reward/action_rate_l2: -0.0288
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.13s
                      Time elapsed: 00:04:36
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 256/1000 [0m                      

                       Computation: 10928 steps/s (collection: 1.073s, learning 0.051s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 2.1249
                       Mean reward: 3.13
               Mean episode length: 656.39
Episode_Reward/track_lin_vel_xy_exp: 0.1180
Episode_Reward/track_ang_vel_z_exp: 0.2095
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.0281
     Episode_Reward/action_rate_l2: -0.0211
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0170
  Episode_Termination/base_contact: 1.0417
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3158016
                    Iteration time: 1.12s
                      Time elapsed: 00:04:37
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 257/1000 [0m                      

                       Computation: 11128 steps/s (collection: 1.053s, learning 0.051s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 2.0507
                       Mean reward: 3.47
               Mean episode length: 624.72
Episode_Reward/track_lin_vel_xy_exp: 0.2109
Episode_Reward/track_ang_vel_z_exp: 0.2389
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.0239
     Episode_Reward/action_rate_l2: -0.0215
      Episode_Reward/feet_air_time: -0.0220
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0123
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 1.10s
                      Time elapsed: 00:04:38
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 258/1000 [0m                      

                       Computation: 11022 steps/s (collection: 1.068s, learning 0.047s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 1.9973
                       Mean reward: 3.75
               Mean episode length: 609.26
Episode_Reward/track_lin_vel_xy_exp: 0.2228
Episode_Reward/track_ang_vel_z_exp: 0.2296
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.0698
         Episode_Reward/dof_acc_l2: -0.0264
     Episode_Reward/action_rate_l2: -0.0223
      Episode_Reward/feet_air_time: -0.0281
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3182592
                    Iteration time: 1.11s
                      Time elapsed: 00:04:40
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 259/1000 [0m                      

                       Computation: 11400 steps/s (collection: 1.034s, learning 0.044s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 1.9504
                       Mean reward: 3.87
               Mean episode length: 630.01
Episode_Reward/track_lin_vel_xy_exp: 0.1381
Episode_Reward/track_ang_vel_z_exp: 0.2627
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.0287
     Episode_Reward/action_rate_l2: -0.0248
      Episode_Reward/feet_air_time: -0.0280
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0129
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 1.08s
                      Time elapsed: 00:04:41
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 260/1000 [0m                      

                       Computation: 11290 steps/s (collection: 1.044s, learning 0.044s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 1.8886
                       Mean reward: 4.29
               Mean episode length: 624.82
Episode_Reward/track_lin_vel_xy_exp: 0.1101
Episode_Reward/track_ang_vel_z_exp: 0.2660
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.0237
     Episode_Reward/action_rate_l2: -0.0236
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3207168
                    Iteration time: 1.09s
                      Time elapsed: 00:04:42
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 261/1000 [0m                      

                       Computation: 11115 steps/s (collection: 1.061s, learning 0.045s)
             Mean action noise std: 0.29
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 1.8127
                       Mean reward: 4.47
               Mean episode length: 606.33
Episode_Reward/track_lin_vel_xy_exp: 0.1744
Episode_Reward/track_ang_vel_z_exp: 0.1912
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0154
     Episode_Reward/dof_torques_l2: -0.0699
         Episode_Reward/dof_acc_l2: -0.0246
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0156
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 1.11s
                      Time elapsed: 00:04:43
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 262/1000 [0m                      

                       Computation: 10689 steps/s (collection: 1.106s, learning 0.044s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 1.7533
                       Mean reward: 4.83
               Mean episode length: 675.38
Episode_Reward/track_lin_vel_xy_exp: 0.1341
Episode_Reward/track_ang_vel_z_exp: 0.2953
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.0277
     Episode_Reward/action_rate_l2: -0.0257
      Episode_Reward/feet_air_time: -0.0310
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0112
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3231744
                    Iteration time: 1.15s
                      Time elapsed: 00:04:44
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 263/1000 [0m                      

                       Computation: 11408 steps/s (collection: 1.028s, learning 0.050s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 1.7086
                       Mean reward: 4.37
               Mean episode length: 669.07
Episode_Reward/track_lin_vel_xy_exp: 0.1381
Episode_Reward/track_ang_vel_z_exp: 0.2345
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.0244
     Episode_Reward/action_rate_l2: -0.0234
      Episode_Reward/feet_air_time: -0.0308
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0165
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.08s
                      Time elapsed: 00:04:45
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 264/1000 [0m                      

                       Computation: 11414 steps/s (collection: 1.032s, learning 0.045s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 1.6723
                       Mean reward: 4.44
               Mean episode length: 682.61
Episode_Reward/track_lin_vel_xy_exp: 0.0759
Episode_Reward/track_ang_vel_z_exp: 0.2105
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0158
     Episode_Reward/dof_torques_l2: -0.0674
         Episode_Reward/dof_acc_l2: -0.0234
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0261
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0136
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3256320
                    Iteration time: 1.08s
                      Time elapsed: 00:04:46
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 265/1000 [0m                      

                       Computation: 10856 steps/s (collection: 1.084s, learning 0.048s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 1.6577
                       Mean reward: 4.36
               Mean episode length: 660.57
Episode_Reward/track_lin_vel_xy_exp: 0.1209
Episode_Reward/track_ang_vel_z_exp: 0.2304
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.0845
         Episode_Reward/dof_acc_l2: -0.0227
     Episode_Reward/action_rate_l2: -0.0207
      Episode_Reward/feet_air_time: -0.0208
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 1.13s
                      Time elapsed: 00:04:47
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 266/1000 [0m                      

                       Computation: 11109 steps/s (collection: 1.057s, learning 0.049s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 1.6493
                       Mean reward: 3.71
               Mean episode length: 659.76
Episode_Reward/track_lin_vel_xy_exp: 0.1508
Episode_Reward/track_ang_vel_z_exp: 0.3265
       Episode_Reward/lin_vel_z_l2: -0.0170
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.0952
         Episode_Reward/dof_acc_l2: -0.0311
     Episode_Reward/action_rate_l2: -0.0267
      Episode_Reward/feet_air_time: -0.0273
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0197
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3280896
                    Iteration time: 1.11s
                      Time elapsed: 00:04:48
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 267/1000 [0m                      

                       Computation: 11837 steps/s (collection: 0.993s, learning 0.045s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 1.6286
                       Mean reward: 3.71
               Mean episode length: 656.51
Episode_Reward/track_lin_vel_xy_exp: 0.1338
Episode_Reward/track_ang_vel_z_exp: 0.2795
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0193
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.0268
     Episode_Reward/action_rate_l2: -0.0235
      Episode_Reward/feet_air_time: -0.0277
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0198
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 1.04s
                      Time elapsed: 00:04:49
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 268/1000 [0m                      

                       Computation: 11444 steps/s (collection: 1.028s, learning 0.045s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 1.5871
                       Mean reward: 3.90
               Mean episode length: 659.54
Episode_Reward/track_lin_vel_xy_exp: 0.2632
Episode_Reward/track_ang_vel_z_exp: 0.2743
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0212
     Episode_Reward/dof_torques_l2: -0.0811
         Episode_Reward/dof_acc_l2: -0.0251
     Episode_Reward/action_rate_l2: -0.0256
      Episode_Reward/feet_air_time: -0.0299
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0165
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3305472
                    Iteration time: 1.07s
                      Time elapsed: 00:04:50
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 269/1000 [0m                      

                       Computation: 11394 steps/s (collection: 1.016s, learning 0.062s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 1.5577
                       Mean reward: 3.77
               Mean episode length: 651.33
Episode_Reward/track_lin_vel_xy_exp: 0.2445
Episode_Reward/track_ang_vel_z_exp: 0.3103
       Episode_Reward/lin_vel_z_l2: -0.0137
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.0884
         Episode_Reward/dof_acc_l2: -0.0263
     Episode_Reward/action_rate_l2: -0.0254
      Episode_Reward/feet_air_time: -0.0270
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0191
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 1.08s
                      Time elapsed: 00:04:52
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 270/1000 [0m                      

                       Computation: 10911 steps/s (collection: 1.062s, learning 0.064s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 1.5559
                       Mean reward: 4.03
               Mean episode length: 693.05
Episode_Reward/track_lin_vel_xy_exp: 0.2136
Episode_Reward/track_ang_vel_z_exp: 0.3500
       Episode_Reward/lin_vel_z_l2: -0.0165
      Episode_Reward/ang_vel_xy_l2: -0.0229
     Episode_Reward/dof_torques_l2: -0.1067
         Episode_Reward/dof_acc_l2: -0.0323
     Episode_Reward/action_rate_l2: -0.0288
      Episode_Reward/feet_air_time: -0.0329
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0156
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3330048
                    Iteration time: 1.13s
                      Time elapsed: 00:04:53
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 271/1000 [0m                      

                       Computation: 11146 steps/s (collection: 1.042s, learning 0.060s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 1.5448
                       Mean reward: 4.54
               Mean episode length: 713.94
Episode_Reward/track_lin_vel_xy_exp: 0.1959
Episode_Reward/track_ang_vel_z_exp: 0.3130
       Episode_Reward/lin_vel_z_l2: -0.0179
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.0989
         Episode_Reward/dof_acc_l2: -0.0326
     Episode_Reward/action_rate_l2: -0.0250
      Episode_Reward/feet_air_time: -0.0268
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0167
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.10s
                      Time elapsed: 00:04:54
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 272/1000 [0m                      

                       Computation: 10603 steps/s (collection: 1.066s, learning 0.093s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 1.5084
                       Mean reward: 5.09
               Mean episode length: 735.17
Episode_Reward/track_lin_vel_xy_exp: 0.1548
Episode_Reward/track_ang_vel_z_exp: 0.2715
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.0925
         Episode_Reward/dof_acc_l2: -0.0321
     Episode_Reward/action_rate_l2: -0.0238
      Episode_Reward/feet_air_time: -0.0266
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 0.9583
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3354624
                    Iteration time: 1.16s
                      Time elapsed: 00:04:55
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 273/1000 [0m                      

                       Computation: 11567 steps/s (collection: 1.017s, learning 0.045s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 1.4834
                       Mean reward: 4.84
               Mean episode length: 735.65
Episode_Reward/track_lin_vel_xy_exp: 0.1347
Episode_Reward/track_ang_vel_z_exp: 0.2966
       Episode_Reward/lin_vel_z_l2: -0.0183
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.0997
         Episode_Reward/dof_acc_l2: -0.0385
     Episode_Reward/action_rate_l2: -0.0250
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: -0.0223
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 1.06s
                      Time elapsed: 00:04:56
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 274/1000 [0m                      

                       Computation: 10780 steps/s (collection: 1.083s, learning 0.056s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 1.4433
                       Mean reward: 5.37
               Mean episode length: 756.92
Episode_Reward/track_lin_vel_xy_exp: 0.2334
Episode_Reward/track_ang_vel_z_exp: 0.2566
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.0260
     Episode_Reward/action_rate_l2: -0.0227
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0165
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3379200
                    Iteration time: 1.14s
                      Time elapsed: 00:04:57
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 275/1000 [0m                      

                       Computation: 11042 steps/s (collection: 1.067s, learning 0.046s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 1.4116
                       Mean reward: 5.28
               Mean episode length: 740.43
Episode_Reward/track_lin_vel_xy_exp: 0.1647
Episode_Reward/track_ang_vel_z_exp: 0.2787
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.0269
     Episode_Reward/action_rate_l2: -0.0239
      Episode_Reward/feet_air_time: -0.0288
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0116
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 1.11s
                      Time elapsed: 00:04:58
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 276/1000 [0m                      

                       Computation: 11283 steps/s (collection: 1.040s, learning 0.049s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 1.3896
                       Mean reward: 5.43
               Mean episode length: 768.27
Episode_Reward/track_lin_vel_xy_exp: 0.2419
Episode_Reward/track_ang_vel_z_exp: 0.3383
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0232
     Episode_Reward/dof_torques_l2: -0.0962
         Episode_Reward/dof_acc_l2: -0.0271
     Episode_Reward/action_rate_l2: -0.0294
      Episode_Reward/feet_air_time: -0.0319
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0167
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3403776
                    Iteration time: 1.09s
                      Time elapsed: 00:04:59
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 277/1000 [0m                      

                       Computation: 11847 steps/s (collection: 0.989s, learning 0.049s)
             Mean action noise std: 0.28
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 1.3725
                       Mean reward: 5.28
               Mean episode length: 745.46
Episode_Reward/track_lin_vel_xy_exp: 0.1275
Episode_Reward/track_ang_vel_z_exp: 0.2522
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0164
     Episode_Reward/dof_torques_l2: -0.0723
         Episode_Reward/dof_acc_l2: -0.0207
     Episode_Reward/action_rate_l2: -0.0208
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0128
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 1.04s
                      Time elapsed: 00:05:00
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 278/1000 [0m                      

                       Computation: 11283 steps/s (collection: 1.040s, learning 0.049s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 1.3246
                       Mean reward: 5.40
               Mean episode length: 745.71
Episode_Reward/track_lin_vel_xy_exp: 0.0832
Episode_Reward/track_ang_vel_z_exp: 0.3473
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0227
     Episode_Reward/dof_torques_l2: -0.0976
         Episode_Reward/dof_acc_l2: -0.0317
     Episode_Reward/action_rate_l2: -0.0264
      Episode_Reward/feet_air_time: -0.0323
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0332
  Episode_Termination/base_contact: 0.8333
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3428352
                    Iteration time: 1.09s
                      Time elapsed: 00:05:01
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 279/1000 [0m                      

                       Computation: 11776 steps/s (collection: 0.984s, learning 0.059s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 1.2859
                       Mean reward: 5.08
               Mean episode length: 748.70
Episode_Reward/track_lin_vel_xy_exp: 0.1010
Episode_Reward/track_ang_vel_z_exp: 0.3408
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.0976
         Episode_Reward/dof_acc_l2: -0.0226
     Episode_Reward/action_rate_l2: -0.0264
      Episode_Reward/feet_air_time: -0.0233
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.04s
                      Time elapsed: 00:05:02
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 280/1000 [0m                      

                       Computation: 10598 steps/s (collection: 1.114s, learning 0.045s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 1.2700
                       Mean reward: 4.87
               Mean episode length: 727.07
Episode_Reward/track_lin_vel_xy_exp: 0.1482
Episode_Reward/track_ang_vel_z_exp: 0.2328
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0154
     Episode_Reward/dof_torques_l2: -0.0755
         Episode_Reward/dof_acc_l2: -0.0201
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0171
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 0.8333
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3452928
                    Iteration time: 1.16s
                      Time elapsed: 00:05:04
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 281/1000 [0m                      

                       Computation: 11306 steps/s (collection: 1.041s, learning 0.046s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 1.2655
                       Mean reward: 4.59
               Mean episode length: 678.59
Episode_Reward/track_lin_vel_xy_exp: 0.1742
Episode_Reward/track_ang_vel_z_exp: 0.2854
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0182
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.0250
     Episode_Reward/action_rate_l2: -0.0222
      Episode_Reward/feet_air_time: -0.0244
 Episode_Reward/undesired_contacts: -0.0075
Episode_Reward/flat_orientation_l2: -0.0143
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 1.09s
                      Time elapsed: 00:05:05
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 282/1000 [0m                      

                       Computation: 11263 steps/s (collection: 1.034s, learning 0.057s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 1.2450
                       Mean reward: 5.00
               Mean episode length: 727.34
Episode_Reward/track_lin_vel_xy_exp: 0.2270
Episode_Reward/track_ang_vel_z_exp: 0.3385
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0214
     Episode_Reward/dof_torques_l2: -0.1105
         Episode_Reward/dof_acc_l2: -0.0286
     Episode_Reward/action_rate_l2: -0.0267
      Episode_Reward/feet_air_time: -0.0309
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3477504
                    Iteration time: 1.09s
                      Time elapsed: 00:05:06
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 283/1000 [0m                      

                       Computation: 11212 steps/s (collection: 1.047s, learning 0.049s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 1.2037
                       Mean reward: 5.37
               Mean episode length: 720.58
Episode_Reward/track_lin_vel_xy_exp: 0.3192
Episode_Reward/track_ang_vel_z_exp: 0.3143
       Episode_Reward/lin_vel_z_l2: -0.0160
      Episode_Reward/ang_vel_xy_l2: -0.0206
     Episode_Reward/dof_torques_l2: -0.0966
         Episode_Reward/dof_acc_l2: -0.0321
     Episode_Reward/action_rate_l2: -0.0251
      Episode_Reward/feet_air_time: -0.0318
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0144
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 1.10s
                      Time elapsed: 00:05:07
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 284/1000 [0m                      

                       Computation: 11872 steps/s (collection: 0.978s, learning 0.057s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 1.1829
                       Mean reward: 5.49
               Mean episode length: 698.10
Episode_Reward/track_lin_vel_xy_exp: 0.2321
Episode_Reward/track_ang_vel_z_exp: 0.2980
       Episode_Reward/lin_vel_z_l2: -0.0163
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.0924
         Episode_Reward/dof_acc_l2: -0.0316
     Episode_Reward/action_rate_l2: -0.0226
      Episode_Reward/feet_air_time: -0.0253
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0133
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3502080
                    Iteration time: 1.03s
                      Time elapsed: 00:05:08
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 285/1000 [0m                      

                       Computation: 12287 steps/s (collection: 0.955s, learning 0.045s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 1.1608
                       Mean reward: 5.77
               Mean episode length: 711.82
Episode_Reward/track_lin_vel_xy_exp: 0.2450
Episode_Reward/track_ang_vel_z_exp: 0.3215
       Episode_Reward/lin_vel_z_l2: -0.0203
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.1142
         Episode_Reward/dof_acc_l2: -0.0361
     Episode_Reward/action_rate_l2: -0.0242
      Episode_Reward/feet_air_time: -0.0262
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 1.00s
                      Time elapsed: 00:05:09
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 286/1000 [0m                      

                       Computation: 11967 steps/s (collection: 0.980s, learning 0.047s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 1.1501
                       Mean reward: 5.63
               Mean episode length: 732.13
Episode_Reward/track_lin_vel_xy_exp: 0.1435
Episode_Reward/track_ang_vel_z_exp: 0.2854
       Episode_Reward/lin_vel_z_l2: -0.0149
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.1035
         Episode_Reward/dof_acc_l2: -0.0286
     Episode_Reward/action_rate_l2: -0.0219
      Episode_Reward/feet_air_time: -0.0263
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: -0.0204
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3526656
                    Iteration time: 1.03s
                      Time elapsed: 00:05:10
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 287/1000 [0m                      

                       Computation: 12306 steps/s (collection: 0.949s, learning 0.050s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 1.1094
                       Mean reward: 5.75
               Mean episode length: 739.18
Episode_Reward/track_lin_vel_xy_exp: 0.1761
Episode_Reward/track_ang_vel_z_exp: 0.3317
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.1075
         Episode_Reward/dof_acc_l2: -0.0278
     Episode_Reward/action_rate_l2: -0.0240
      Episode_Reward/feet_air_time: -0.0249
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0115
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.00s
                      Time elapsed: 00:05:11
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 288/1000 [0m                      

                       Computation: 11846 steps/s (collection: 0.991s, learning 0.047s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 1.0796
                       Mean reward: 5.70
               Mean episode length: 710.68
Episode_Reward/track_lin_vel_xy_exp: 0.2436
Episode_Reward/track_ang_vel_z_exp: 0.2658
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.0847
         Episode_Reward/dof_acc_l2: -0.0224
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0178
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: -0.0146
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3551232
                    Iteration time: 1.04s
                      Time elapsed: 00:05:12
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 289/1000 [0m                      

                       Computation: 12581 steps/s (collection: 0.932s, learning 0.045s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 1.0659
                       Mean reward: 5.55
               Mean episode length: 711.89
Episode_Reward/track_lin_vel_xy_exp: 0.1662
Episode_Reward/track_ang_vel_z_exp: 0.2863
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0792
         Episode_Reward/dof_acc_l2: -0.0183
     Episode_Reward/action_rate_l2: -0.0211
      Episode_Reward/feet_air_time: -0.0201
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: -0.0230
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 0.98s
                      Time elapsed: 00:05:13
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 290/1000 [0m                      

                       Computation: 11910 steps/s (collection: 0.987s, learning 0.045s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 1.0309
                       Mean reward: 5.73
               Mean episode length: 709.46
Episode_Reward/track_lin_vel_xy_exp: 0.2578
Episode_Reward/track_ang_vel_z_exp: 0.2317
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0139
     Episode_Reward/dof_torques_l2: -0.0690
         Episode_Reward/dof_acc_l2: -0.0215
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0113
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 3575808
                    Iteration time: 1.03s
                      Time elapsed: 00:05:14
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 291/1000 [0m                      

                       Computation: 12120 steps/s (collection: 0.966s, learning 0.047s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 1.0172
                       Mean reward: 5.72
               Mean episode length: 698.32
Episode_Reward/track_lin_vel_xy_exp: 0.1201
Episode_Reward/track_ang_vel_z_exp: 0.2213
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0136
     Episode_Reward/dof_torques_l2: -0.0627
         Episode_Reward/dof_acc_l2: -0.0156
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0158
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 1.01s
                      Time elapsed: 00:05:15
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 292/1000 [0m                      

                       Computation: 12020 steps/s (collection: 0.970s, learning 0.053s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 0.9820
                       Mean reward: 6.06
               Mean episode length: 717.64
Episode_Reward/track_lin_vel_xy_exp: 0.4092
Episode_Reward/track_ang_vel_z_exp: 0.2999
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.1177
         Episode_Reward/dof_acc_l2: -0.0327
     Episode_Reward/action_rate_l2: -0.0227
      Episode_Reward/feet_air_time: -0.0262
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3600384
                    Iteration time: 1.02s
                      Time elapsed: 00:05:16
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 293/1000 [0m                      

                       Computation: 11516 steps/s (collection: 1.019s, learning 0.048s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 0.9352
                       Mean reward: 6.14
               Mean episode length: 700.09
Episode_Reward/track_lin_vel_xy_exp: 0.2313
Episode_Reward/track_ang_vel_z_exp: 0.2209
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0160
     Episode_Reward/dof_torques_l2: -0.0814
         Episode_Reward/dof_acc_l2: -0.0185
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0148
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0169
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 1.07s
                      Time elapsed: 00:05:17
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 294/1000 [0m                      

                       Computation: 12452 steps/s (collection: 0.941s, learning 0.045s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 0.9127
                       Mean reward: 6.39
               Mean episode length: 710.30
Episode_Reward/track_lin_vel_xy_exp: 0.1355
Episode_Reward/track_ang_vel_z_exp: 0.3010
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.0843
         Episode_Reward/dof_acc_l2: -0.0170
     Episode_Reward/action_rate_l2: -0.0213
      Episode_Reward/feet_air_time: -0.0195
 Episode_Reward/undesired_contacts: -0.0204
Episode_Reward/flat_orientation_l2: -0.0195
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3624960
                    Iteration time: 0.99s
                      Time elapsed: 00:05:18
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 295/1000 [0m                      

                       Computation: 12317 steps/s (collection: 0.953s, learning 0.045s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 0.9085
                       Mean reward: 6.19
               Mean episode length: 706.47
Episode_Reward/track_lin_vel_xy_exp: 0.2710
Episode_Reward/track_ang_vel_z_exp: 0.2539
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.0729
         Episode_Reward/dof_acc_l2: -0.0181
     Episode_Reward/action_rate_l2: -0.0214
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0132
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.00s
                      Time elapsed: 00:05:19
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 296/1000 [0m                      

                       Computation: 12405 steps/s (collection: 0.944s, learning 0.047s)
             Mean action noise std: 0.27
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 0.9106
                       Mean reward: 6.04
               Mean episode length: 693.23
Episode_Reward/track_lin_vel_xy_exp: 0.0610
Episode_Reward/track_ang_vel_z_exp: 0.1343
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0109
     Episode_Reward/dof_torques_l2: -0.0417
         Episode_Reward/dof_acc_l2: -0.0175
     Episode_Reward/action_rate_l2: -0.0101
      Episode_Reward/feet_air_time: -0.0140
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 1.1667
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3649536
                    Iteration time: 0.99s
                      Time elapsed: 00:05:20
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 297/1000 [0m                      

                       Computation: 12261 steps/s (collection: 0.952s, learning 0.050s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 0.8804
                       Mean reward: 6.37
               Mean episode length: 710.53
Episode_Reward/track_lin_vel_xy_exp: 0.3436
Episode_Reward/track_ang_vel_z_exp: 0.3328
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1058
         Episode_Reward/dof_acc_l2: -0.0265
     Episode_Reward/action_rate_l2: -0.0244
      Episode_Reward/feet_air_time: -0.0263
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: -0.0185
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 1.00s
                      Time elapsed: 00:05:21
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 298/1000 [0m                      

                       Computation: 11823 steps/s (collection: 0.983s, learning 0.056s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 0.8450
                       Mean reward: 6.09
               Mean episode length: 712.51
Episode_Reward/track_lin_vel_xy_exp: 0.0975
Episode_Reward/track_ang_vel_z_exp: 0.1705
       Episode_Reward/lin_vel_z_l2: -0.0091
      Episode_Reward/ang_vel_xy_l2: -0.0099
     Episode_Reward/dof_torques_l2: -0.0483
         Episode_Reward/dof_acc_l2: -0.0111
     Episode_Reward/action_rate_l2: -0.0119
      Episode_Reward/feet_air_time: -0.0110
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: -0.0109
  Episode_Termination/base_contact: 0.8333
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 3674112
                    Iteration time: 1.04s
                      Time elapsed: 00:05:22
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 299/1000 [0m                      

                       Computation: 12035 steps/s (collection: 0.972s, learning 0.049s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 0.8138
                       Mean reward: 5.55
               Mean episode length: 702.26
Episode_Reward/track_lin_vel_xy_exp: 0.2097
Episode_Reward/track_ang_vel_z_exp: 0.2807
       Episode_Reward/lin_vel_z_l2: -0.0107
      Episode_Reward/ang_vel_xy_l2: -0.0176
     Episode_Reward/dof_torques_l2: -0.0832
         Episode_Reward/dof_acc_l2: -0.0224
     Episode_Reward/action_rate_l2: -0.0219
      Episode_Reward/feet_air_time: -0.0243
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 1.02s
                      Time elapsed: 00:05:23
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 300/1000 [0m                      

                       Computation: 12377 steps/s (collection: 0.946s, learning 0.047s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 0.7982
                       Mean reward: 5.89
               Mean episode length: 735.27
Episode_Reward/track_lin_vel_xy_exp: 0.3318
Episode_Reward/track_ang_vel_z_exp: 0.3116
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.1097
         Episode_Reward/dof_acc_l2: -0.0348
     Episode_Reward/action_rate_l2: -0.0258
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0240
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3698688
                    Iteration time: 0.99s
                      Time elapsed: 00:05:24
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 301/1000 [0m                      

                       Computation: 12121 steps/s (collection: 0.968s, learning 0.046s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 0.7791
                       Mean reward: 5.95
               Mean episode length: 730.83
Episode_Reward/track_lin_vel_xy_exp: 0.2154
Episode_Reward/track_ang_vel_z_exp: 0.3795
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1111
         Episode_Reward/dof_acc_l2: -0.0268
     Episode_Reward/action_rate_l2: -0.0256
      Episode_Reward/feet_air_time: -0.0264
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 1.01s
                      Time elapsed: 00:05:25
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 302/1000 [0m                      

                       Computation: 12251 steps/s (collection: 0.958s, learning 0.045s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 0.7433
                       Mean reward: 6.08
               Mean episode length: 747.04
Episode_Reward/track_lin_vel_xy_exp: 0.1935
Episode_Reward/track_ang_vel_z_exp: 0.1446
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0141
     Episode_Reward/dof_torques_l2: -0.0639
         Episode_Reward/dof_acc_l2: -0.0176
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0151
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0210
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 3723264
                    Iteration time: 1.00s
                      Time elapsed: 00:05:26
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 303/1000 [0m                      

                       Computation: 12519 steps/s (collection: 0.936s, learning 0.046s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 0.7111
                       Mean reward: 5.55
               Mean episode length: 709.89
Episode_Reward/track_lin_vel_xy_exp: 0.2169
Episode_Reward/track_ang_vel_z_exp: 0.2555
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0757
         Episode_Reward/dof_acc_l2: -0.0261
     Episode_Reward/action_rate_l2: -0.0179
      Episode_Reward/feet_air_time: -0.0221
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0249
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.98s
                      Time elapsed: 00:05:27
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 304/1000 [0m                      

                       Computation: 12262 steps/s (collection: 0.957s, learning 0.045s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 0.7120
                       Mean reward: 6.52
               Mean episode length: 763.59
Episode_Reward/track_lin_vel_xy_exp: 0.2648
Episode_Reward/track_ang_vel_z_exp: 0.2702
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0165
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.0235
     Episode_Reward/action_rate_l2: -0.0207
      Episode_Reward/feet_air_time: -0.0216
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0137
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3747840
                    Iteration time: 1.00s
                      Time elapsed: 00:05:28
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 305/1000 [0m                      

                       Computation: 11250 steps/s (collection: 1.042s, learning 0.050s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 0.6886
                       Mean reward: 6.47
               Mean episode length: 738.10
Episode_Reward/track_lin_vel_xy_exp: 0.1374
Episode_Reward/track_ang_vel_z_exp: 0.2725
       Episode_Reward/lin_vel_z_l2: -0.0122
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.0841
         Episode_Reward/dof_acc_l2: -0.0234
     Episode_Reward/action_rate_l2: -0.0211
      Episode_Reward/feet_air_time: -0.0282
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0159
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 1.09s
                      Time elapsed: 00:05:29
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 306/1000 [0m                      

                       Computation: 11005 steps/s (collection: 1.062s, learning 0.054s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 0.6670
                       Mean reward: 6.20
               Mean episode length: 714.45
Episode_Reward/track_lin_vel_xy_exp: 0.1674
Episode_Reward/track_ang_vel_z_exp: 0.1738
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0643
         Episode_Reward/dof_acc_l2: -0.0186
     Episode_Reward/action_rate_l2: -0.0139
      Episode_Reward/feet_air_time: -0.0187
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0134
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3772416
                    Iteration time: 1.12s
                      Time elapsed: 00:05:30
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 307/1000 [0m                      

                       Computation: 11262 steps/s (collection: 1.047s, learning 0.044s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 0.6358
                       Mean reward: 6.07
               Mean episode length: 698.39
Episode_Reward/track_lin_vel_xy_exp: 0.2003
Episode_Reward/track_ang_vel_z_exp: 0.2589
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0169
     Episode_Reward/dof_torques_l2: -0.0894
         Episode_Reward/dof_acc_l2: -0.0250
     Episode_Reward/action_rate_l2: -0.0204
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: -0.0156
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 1.09s
                      Time elapsed: 00:05:31
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 308/1000 [0m                      

                       Computation: 10873 steps/s (collection: 1.081s, learning 0.049s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 0.6067
                       Mean reward: 6.13
               Mean episode length: 709.10
Episode_Reward/track_lin_vel_xy_exp: 0.1678
Episode_Reward/track_ang_vel_z_exp: 0.2815
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.0232
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0167
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3796992
                    Iteration time: 1.13s
                      Time elapsed: 00:05:33
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 309/1000 [0m                      

                       Computation: 11240 steps/s (collection: 1.049s, learning 0.044s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0066
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 0.5819
                       Mean reward: 5.97
               Mean episode length: 695.74
Episode_Reward/track_lin_vel_xy_exp: 0.2109
Episode_Reward/track_ang_vel_z_exp: 0.3272
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.0907
         Episode_Reward/dof_acc_l2: -0.0246
     Episode_Reward/action_rate_l2: -0.0230
      Episode_Reward/feet_air_time: -0.0249
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0135
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 1.09s
                      Time elapsed: 00:05:34
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 310/1000 [0m                      

                       Computation: 10981 steps/s (collection: 1.065s, learning 0.054s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 0.5633
                       Mean reward: 5.43
               Mean episode length: 697.68
Episode_Reward/track_lin_vel_xy_exp: 0.1585
Episode_Reward/track_ang_vel_z_exp: 0.3493
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.1018
         Episode_Reward/dof_acc_l2: -0.0304
     Episode_Reward/action_rate_l2: -0.0228
      Episode_Reward/feet_air_time: -0.0242
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0232
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3821568
                    Iteration time: 1.12s
                      Time elapsed: 00:05:35
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 311/1000 [0m                      

                       Computation: 11257 steps/s (collection: 1.033s, learning 0.058s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 0.5695
                       Mean reward: 5.77
               Mean episode length: 733.07
Episode_Reward/track_lin_vel_xy_exp: 0.2096
Episode_Reward/track_ang_vel_z_exp: 0.2678
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0162
     Episode_Reward/dof_torques_l2: -0.0938
         Episode_Reward/dof_acc_l2: -0.0271
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0218
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.09s
                      Time elapsed: 00:05:36
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 312/1000 [0m                      

                       Computation: 11527 steps/s (collection: 1.019s, learning 0.047s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 0.5576
                       Mean reward: 5.33
               Mean episode length: 718.00
Episode_Reward/track_lin_vel_xy_exp: 0.2154
Episode_Reward/track_ang_vel_z_exp: 0.2744
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.0246
     Episode_Reward/action_rate_l2: -0.0201
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0310
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3846144
                    Iteration time: 1.07s
                      Time elapsed: 00:05:37
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 313/1000 [0m                      

                       Computation: 10990 steps/s (collection: 1.070s, learning 0.048s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 0.5384
                       Mean reward: 5.30
               Mean episode length: 728.51
Episode_Reward/track_lin_vel_xy_exp: 0.1432
Episode_Reward/track_ang_vel_z_exp: 0.2292
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0769
         Episode_Reward/dof_acc_l2: -0.0196
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: -0.0094
Episode_Reward/flat_orientation_l2: -0.0244
  Episode_Termination/base_contact: 0.9583
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 1.12s
                      Time elapsed: 00:05:38
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 314/1000 [0m                      

                       Computation: 11601 steps/s (collection: 1.014s, learning 0.045s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 0.5273
                       Mean reward: 5.42
               Mean episode length: 749.41
Episode_Reward/track_lin_vel_xy_exp: 0.2140
Episode_Reward/track_ang_vel_z_exp: 0.3715
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0206
     Episode_Reward/dof_torques_l2: -0.0937
         Episode_Reward/dof_acc_l2: -0.0210
     Episode_Reward/action_rate_l2: -0.0235
      Episode_Reward/feet_air_time: -0.0231
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0211
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3870720
                    Iteration time: 1.06s
                      Time elapsed: 00:05:39
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 315/1000 [0m                      

                       Computation: 11563 steps/s (collection: 1.013s, learning 0.050s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 0.5199
                       Mean reward: 5.68
               Mean episode length: 780.45
Episode_Reward/track_lin_vel_xy_exp: 0.1689
Episode_Reward/track_ang_vel_z_exp: 0.3239
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.0980
         Episode_Reward/dof_acc_l2: -0.0233
     Episode_Reward/action_rate_l2: -0.0242
      Episode_Reward/feet_air_time: -0.0260
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: -0.0178
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 1.06s
                      Time elapsed: 00:05:40
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 316/1000 [0m                      

                       Computation: 10854 steps/s (collection: 1.083s, learning 0.049s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 0.5033
                       Mean reward: 5.71
               Mean episode length: 764.78
Episode_Reward/track_lin_vel_xy_exp: 0.3192
Episode_Reward/track_ang_vel_z_exp: 0.3201
       Episode_Reward/lin_vel_z_l2: -0.0138
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.1032
         Episode_Reward/dof_acc_l2: -0.0288
     Episode_Reward/action_rate_l2: -0.0225
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3895296
                    Iteration time: 1.13s
                      Time elapsed: 00:05:41
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 317/1000 [0m                      

                       Computation: 11307 steps/s (collection: 1.031s, learning 0.056s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 0.4711
                       Mean reward: 5.53
               Mean episode length: 771.20
Episode_Reward/track_lin_vel_xy_exp: 0.1507
Episode_Reward/track_ang_vel_z_exp: 0.2572
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.0775
         Episode_Reward/dof_acc_l2: -0.0200
     Episode_Reward/action_rate_l2: -0.0205
      Episode_Reward/feet_air_time: -0.0224
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0215
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 1.09s
                      Time elapsed: 00:05:42
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 318/1000 [0m                      

                       Computation: 10969 steps/s (collection: 1.072s, learning 0.048s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 0.4425
                       Mean reward: 5.84
               Mean episode length: 773.73
Episode_Reward/track_lin_vel_xy_exp: 0.2781
Episode_Reward/track_ang_vel_z_exp: 0.2758
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0161
     Episode_Reward/dof_torques_l2: -0.0792
         Episode_Reward/dof_acc_l2: -0.0217
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0205
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0127
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3919872
                    Iteration time: 1.12s
                      Time elapsed: 00:05:44
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 319/1000 [0m                      

                       Computation: 11164 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 0.4314
                       Mean reward: 5.77
               Mean episode length: 760.84
Episode_Reward/track_lin_vel_xy_exp: 0.1672
Episode_Reward/track_ang_vel_z_exp: 0.2686
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.0233
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0225
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0175
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.10s
                      Time elapsed: 00:05:45
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 320/1000 [0m                      

                       Computation: 12043 steps/s (collection: 0.976s, learning 0.045s)
             Mean action noise std: 0.26
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 0.4325
                       Mean reward: 6.21
               Mean episode length: 742.23
Episode_Reward/track_lin_vel_xy_exp: 0.2059
Episode_Reward/track_ang_vel_z_exp: 0.1830
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0623
         Episode_Reward/dof_acc_l2: -0.0171
     Episode_Reward/action_rate_l2: -0.0141
      Episode_Reward/feet_air_time: -0.0172
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.8750
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3944448
                    Iteration time: 1.02s
                      Time elapsed: 00:05:46
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 321/1000 [0m                      

                       Computation: 11996 steps/s (collection: 0.979s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 0.4254
                       Mean reward: 5.92
               Mean episode length: 731.25
Episode_Reward/track_lin_vel_xy_exp: 0.1312
Episode_Reward/track_ang_vel_z_exp: 0.2208
       Episode_Reward/lin_vel_z_l2: -0.0122
      Episode_Reward/ang_vel_xy_l2: -0.0150
     Episode_Reward/dof_torques_l2: -0.0682
         Episode_Reward/dof_acc_l2: -0.0211
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0160
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0196
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 1.02s
                      Time elapsed: 00:05:47
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 322/1000 [0m                      

                       Computation: 12443 steps/s (collection: 0.942s, learning 0.045s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 0.3941
                       Mean reward: 5.65
               Mean episode length: 732.22
Episode_Reward/track_lin_vel_xy_exp: 0.1702
Episode_Reward/track_ang_vel_z_exp: 0.2675
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.0173
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0193
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: -0.0246
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3969024
                    Iteration time: 0.99s
                      Time elapsed: 00:05:48
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 323/1000 [0m                      

                       Computation: 12173 steps/s (collection: 0.958s, learning 0.052s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 0.3914
                       Mean reward: 6.35
               Mean episode length: 752.59
Episode_Reward/track_lin_vel_xy_exp: 0.3204
Episode_Reward/track_ang_vel_z_exp: 0.3294
       Episode_Reward/lin_vel_z_l2: -0.0122
      Episode_Reward/ang_vel_xy_l2: -0.0182
     Episode_Reward/dof_torques_l2: -0.0956
         Episode_Reward/dof_acc_l2: -0.0235
     Episode_Reward/action_rate_l2: -0.0212
      Episode_Reward/feet_air_time: -0.0226
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0176
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 1.01s
                      Time elapsed: 00:05:49
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 324/1000 [0m                      

                       Computation: 12312 steps/s (collection: 0.953s, learning 0.045s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 0.3529
                       Mean reward: 6.36
               Mean episode length: 768.18
Episode_Reward/track_lin_vel_xy_exp: 0.2391
Episode_Reward/track_ang_vel_z_exp: 0.2640
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.0231
     Episode_Reward/action_rate_l2: -0.0213
      Episode_Reward/feet_air_time: -0.0255
 Episode_Reward/undesired_contacts: -0.0039
Episode_Reward/flat_orientation_l2: -0.0197
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3993600
                    Iteration time: 1.00s
                      Time elapsed: 00:05:50
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 325/1000 [0m                      

                       Computation: 12247 steps/s (collection: 0.957s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 0.3236
                       Mean reward: 5.60
               Mean episode length: 744.53
Episode_Reward/track_lin_vel_xy_exp: 0.1029
Episode_Reward/track_ang_vel_z_exp: 0.2988
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0956
         Episode_Reward/dof_acc_l2: -0.0249
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0215
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 1.00s
                      Time elapsed: 00:05:51
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 326/1000 [0m                      

                       Computation: 12413 steps/s (collection: 0.946s, learning 0.044s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 0.2927
                       Mean reward: 5.71
               Mean episode length: 751.33
Episode_Reward/track_lin_vel_xy_exp: 0.2631
Episode_Reward/track_ang_vel_z_exp: 0.3011
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.0922
         Episode_Reward/dof_acc_l2: -0.0270
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0216
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0255
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4018176
                    Iteration time: 0.99s
                      Time elapsed: 00:05:52
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 327/1000 [0m                      

                       Computation: 12172 steps/s (collection: 0.943s, learning 0.066s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 0.2773
                       Mean reward: 6.00
               Mean episode length: 760.17
Episode_Reward/track_lin_vel_xy_exp: 0.2716
Episode_Reward/track_ang_vel_z_exp: 0.3510
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.1069
         Episode_Reward/dof_acc_l2: -0.0257
     Episode_Reward/action_rate_l2: -0.0236
      Episode_Reward/feet_air_time: -0.0279
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0181
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.01s
                      Time elapsed: 00:05:53
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 328/1000 [0m                      

                       Computation: 12039 steps/s (collection: 0.975s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 0.2486
                       Mean reward: 5.86
               Mean episode length: 752.60
Episode_Reward/track_lin_vel_xy_exp: 0.1074
Episode_Reward/track_ang_vel_z_exp: 0.2621
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.0278
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0235
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0219
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4042752
                    Iteration time: 1.02s
                      Time elapsed: 00:05:54
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 329/1000 [0m                      

                       Computation: 12410 steps/s (collection: 0.944s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 0.2430
                       Mean reward: 5.38
               Mean episode length: 726.86
Episode_Reward/track_lin_vel_xy_exp: 0.1525
Episode_Reward/track_ang_vel_z_exp: 0.2859
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.0973
         Episode_Reward/dof_acc_l2: -0.0334
     Episode_Reward/action_rate_l2: -0.0207
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0224
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 0.99s
                      Time elapsed: 00:05:55
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 330/1000 [0m                      

                       Computation: 12058 steps/s (collection: 0.970s, learning 0.049s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 0.2327
                       Mean reward: 5.32
               Mean episode length: 733.09
Episode_Reward/track_lin_vel_xy_exp: 0.3079
Episode_Reward/track_ang_vel_z_exp: 0.2817
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.0236
     Episode_Reward/action_rate_l2: -0.0209
      Episode_Reward/feet_air_time: -0.0206
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0213
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4067328
                    Iteration time: 1.02s
                      Time elapsed: 00:05:56
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 331/1000 [0m                      

                       Computation: 12327 steps/s (collection: 0.951s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 0.1998
                       Mean reward: 5.69
               Mean episode length: 743.02
Episode_Reward/track_lin_vel_xy_exp: 0.2517
Episode_Reward/track_ang_vel_z_exp: 0.3366
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1069
         Episode_Reward/dof_acc_l2: -0.0317
     Episode_Reward/action_rate_l2: -0.0246
      Episode_Reward/feet_air_time: -0.0241
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0226
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 1.00s
                      Time elapsed: 00:05:57
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 332/1000 [0m                      

                       Computation: 12615 steps/s (collection: 0.929s, learning 0.045s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 0.1671
                       Mean reward: 5.79
               Mean episode length: 749.23
Episode_Reward/track_lin_vel_xy_exp: 0.1661
Episode_Reward/track_ang_vel_z_exp: 0.3916
       Episode_Reward/lin_vel_z_l2: -0.0150
      Episode_Reward/ang_vel_xy_l2: -0.0231
     Episode_Reward/dof_torques_l2: -0.1188
         Episode_Reward/dof_acc_l2: -0.0349
     Episode_Reward/action_rate_l2: -0.0258
      Episode_Reward/feet_air_time: -0.0366
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0256
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4091904
                    Iteration time: 0.97s
                      Time elapsed: 00:05:58
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 333/1000 [0m                      

                       Computation: 12215 steps/s (collection: 0.961s, learning 0.045s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 0.1516
                       Mean reward: 6.48
               Mean episode length: 768.35
Episode_Reward/track_lin_vel_xy_exp: 0.2474
Episode_Reward/track_ang_vel_z_exp: 0.2859
       Episode_Reward/lin_vel_z_l2: -0.0154
      Episode_Reward/ang_vel_xy_l2: -0.0166
     Episode_Reward/dof_torques_l2: -0.0930
         Episode_Reward/dof_acc_l2: -0.0281
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0199
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0251
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 1.01s
                      Time elapsed: 00:05:59
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 334/1000 [0m                      

                       Computation: 11876 steps/s (collection: 0.982s, learning 0.053s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 0.1450
                       Mean reward: 6.13
               Mean episode length: 761.51
Episode_Reward/track_lin_vel_xy_exp: 0.0756
Episode_Reward/track_ang_vel_z_exp: 0.2837
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.0238
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0205
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0198
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4116480
                    Iteration time: 1.03s
                      Time elapsed: 00:06:00
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 335/1000 [0m                      

                       Computation: 12320 steps/s (collection: 0.951s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 0.1276
                       Mean reward: 6.05
               Mean episode length: 777.05
Episode_Reward/track_lin_vel_xy_exp: 0.2286
Episode_Reward/track_ang_vel_z_exp: 0.3059
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.0991
         Episode_Reward/dof_acc_l2: -0.0260
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0218
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.00s
                      Time elapsed: 00:06:01
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 336/1000 [0m                      

                       Computation: 12565 steps/s (collection: 0.931s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 0.1164
                       Mean reward: 5.85
               Mean episode length: 772.35
Episode_Reward/track_lin_vel_xy_exp: 0.3548
Episode_Reward/track_ang_vel_z_exp: 0.4112
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.1365
         Episode_Reward/dof_acc_l2: -0.0324
     Episode_Reward/action_rate_l2: -0.0261
      Episode_Reward/feet_air_time: -0.0258
 Episode_Reward/undesired_contacts: -0.1132
Episode_Reward/flat_orientation_l2: -0.0317
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4141056
                    Iteration time: 0.98s
                      Time elapsed: 00:06:02
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 337/1000 [0m                      

                       Computation: 12727 steps/s (collection: 0.919s, learning 0.047s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 0.0945
                       Mean reward: 6.01
               Mean episode length: 793.88
Episode_Reward/track_lin_vel_xy_exp: 0.2729
Episode_Reward/track_ang_vel_z_exp: 0.3319
       Episode_Reward/lin_vel_z_l2: -0.0155
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.1075
         Episode_Reward/dof_acc_l2: -0.0323
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0256
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0144
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 0.97s
                      Time elapsed: 00:06:03
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 338/1000 [0m                      

                       Computation: 12096 steps/s (collection: 0.971s, learning 0.045s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 0.0812
                       Mean reward: 5.86
               Mean episode length: 786.72
Episode_Reward/track_lin_vel_xy_exp: 0.2103
Episode_Reward/track_ang_vel_z_exp: 0.2346
       Episode_Reward/lin_vel_z_l2: -0.0136
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.0849
         Episode_Reward/dof_acc_l2: -0.0298
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0244
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0142
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4165632
                    Iteration time: 1.02s
                      Time elapsed: 00:06:04
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 339/1000 [0m                      

                       Computation: 12504 steps/s (collection: 0.936s, learning 0.047s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 0.0674
                       Mean reward: 6.16
               Mean episode length: 793.19
Episode_Reward/track_lin_vel_xy_exp: 0.3040
Episode_Reward/track_ang_vel_z_exp: 0.3354
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.1195
         Episode_Reward/dof_acc_l2: -0.0353
     Episode_Reward/action_rate_l2: -0.0225
      Episode_Reward/feet_air_time: -0.0233
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0244
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 0.98s
                      Time elapsed: 00:06:05
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 340/1000 [0m                      

                       Computation: 12270 steps/s (collection: 0.948s, learning 0.054s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 0.0487
                       Mean reward: 6.19
               Mean episode length: 798.85
Episode_Reward/track_lin_vel_xy_exp: 0.3624
Episode_Reward/track_ang_vel_z_exp: 0.2900
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.0245
     Episode_Reward/action_rate_l2: -0.0207
      Episode_Reward/feet_air_time: -0.0269
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0168
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4190208
                    Iteration time: 1.00s
                      Time elapsed: 00:06:06
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 341/1000 [0m                      

                       Computation: 12193 steps/s (collection: 0.962s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 0.0306
                       Mean reward: 6.01
               Mean episode length: 788.76
Episode_Reward/track_lin_vel_xy_exp: 0.1305
Episode_Reward/track_ang_vel_z_exp: 0.3268
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.0965
         Episode_Reward/dof_acc_l2: -0.0286
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0282
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: -0.0279
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 1.01s
                      Time elapsed: 00:06:07
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 342/1000 [0m                      

                       Computation: 11808 steps/s (collection: 0.992s, learning 0.048s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 0.0142
                       Mean reward: 5.78
               Mean episode length: 806.32
Episode_Reward/track_lin_vel_xy_exp: 0.1984
Episode_Reward/track_ang_vel_z_exp: 0.3659
       Episode_Reward/lin_vel_z_l2: -0.0152
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.1311
         Episode_Reward/dof_acc_l2: -0.0388
     Episode_Reward/action_rate_l2: -0.0246
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0175
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 4214784
                    Iteration time: 1.04s
                      Time elapsed: 00:06:08
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 343/1000 [0m                      

                       Computation: 12373 steps/s (collection: 0.926s, learning 0.067s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 0.0153
                       Mean reward: 5.76
               Mean episode length: 774.90
Episode_Reward/track_lin_vel_xy_exp: 0.1513
Episode_Reward/track_ang_vel_z_exp: 0.2545
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.0215
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0200
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0255
  Episode_Termination/base_contact: 0.7917
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.99s
                      Time elapsed: 00:06:09
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 344/1000 [0m                      

                       Computation: 12031 steps/s (collection: 0.977s, learning 0.045s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 0.0199
                       Mean reward: 5.55
               Mean episode length: 771.58
Episode_Reward/track_lin_vel_xy_exp: 0.1128
Episode_Reward/track_ang_vel_z_exp: 0.2868
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0148
     Episode_Reward/dof_torques_l2: -0.0776
         Episode_Reward/dof_acc_l2: -0.0210
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0172
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4239360
                    Iteration time: 1.02s
                      Time elapsed: 00:06:10
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 345/1000 [0m                      

                       Computation: 12457 steps/s (collection: 0.942s, learning 0.045s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 0.0254
                       Mean reward: 6.01
               Mean episode length: 774.38
Episode_Reward/track_lin_vel_xy_exp: 0.2605
Episode_Reward/track_ang_vel_z_exp: 0.3103
       Episode_Reward/lin_vel_z_l2: -0.0136
      Episode_Reward/ang_vel_xy_l2: -0.0164
     Episode_Reward/dof_torques_l2: -0.0923
         Episode_Reward/dof_acc_l2: -0.0266
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0218
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 0.99s
                      Time elapsed: 00:06:11
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 346/1000 [0m                      

                       Computation: 12186 steps/s (collection: 0.960s, learning 0.048s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 0.0041
                       Mean reward: 5.80
               Mean episode length: 755.90
Episode_Reward/track_lin_vel_xy_exp: 0.2426
Episode_Reward/track_ang_vel_z_exp: 0.3921
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.0985
         Episode_Reward/dof_acc_l2: -0.0236
     Episode_Reward/action_rate_l2: -0.0213
      Episode_Reward/feet_air_time: -0.0257
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 4263936
                    Iteration time: 1.01s
                      Time elapsed: 00:06:12
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 347/1000 [0m                      

                       Computation: 12291 steps/s (collection: 0.951s, learning 0.049s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -0.0178
                       Mean reward: 5.79
               Mean episode length: 750.23
Episode_Reward/track_lin_vel_xy_exp: 0.3087
Episode_Reward/track_ang_vel_z_exp: 0.3598
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.0216
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0188
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 1.00s
                      Time elapsed: 00:06:13
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 348/1000 [0m                      

                       Computation: 12591 steps/s (collection: 0.930s, learning 0.046s)
             Mean action noise std: 0.25
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -0.0370
                       Mean reward: 6.16
               Mean episode length: 756.99
Episode_Reward/track_lin_vel_xy_exp: 0.2862
Episode_Reward/track_ang_vel_z_exp: 0.2957
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.0932
         Episode_Reward/dof_acc_l2: -0.0269
     Episode_Reward/action_rate_l2: -0.0205
      Episode_Reward/feet_air_time: -0.0215
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0311
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 4288512
                    Iteration time: 0.98s
                      Time elapsed: 00:06:14
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 349/1000 [0m                      

                       Computation: 12447 steps/s (collection: 0.940s, learning 0.047s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -0.0622
                       Mean reward: 6.08
               Mean episode length: 757.57
Episode_Reward/track_lin_vel_xy_exp: 0.1173
Episode_Reward/track_ang_vel_z_exp: 0.3113
       Episode_Reward/lin_vel_z_l2: -0.0155
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.1042
         Episode_Reward/dof_acc_l2: -0.0371
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0257
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0186
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 0.99s
                      Time elapsed: 00:06:15
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 350/1000 [0m                      

                       Computation: 12100 steps/s (collection: 0.967s, learning 0.048s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -0.0926
                       Mean reward: 6.84
               Mean episode length: 790.96
Episode_Reward/track_lin_vel_xy_exp: 0.1993
Episode_Reward/track_ang_vel_z_exp: 0.3401
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.0932
         Episode_Reward/dof_acc_l2: -0.0241
     Episode_Reward/action_rate_l2: -0.0202
      Episode_Reward/feet_air_time: -0.0265
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0146
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4313088
                    Iteration time: 1.02s
                      Time elapsed: 00:06:16
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 351/1000 [0m                      

                       Computation: 12320 steps/s (collection: 0.951s, learning 0.046s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -0.1150
                       Mean reward: 6.93
               Mean episode length: 795.69
Episode_Reward/track_lin_vel_xy_exp: 0.2062
Episode_Reward/track_ang_vel_z_exp: 0.2955
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0161
     Episode_Reward/dof_torques_l2: -0.0849
         Episode_Reward/dof_acc_l2: -0.0228
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0250
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: -0.0179
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.00s
                      Time elapsed: 00:06:17
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 352/1000 [0m                      

                       Computation: 12511 steps/s (collection: 0.937s, learning 0.045s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0045
                 Mean entropy loss: -0.1349
                       Mean reward: 7.36
               Mean episode length: 830.01
Episode_Reward/track_lin_vel_xy_exp: 0.3118
Episode_Reward/track_ang_vel_z_exp: 0.3156
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.1101
         Episode_Reward/dof_acc_l2: -0.0278
     Episode_Reward/action_rate_l2: -0.0214
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0170
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4337664
                    Iteration time: 0.98s
                      Time elapsed: 00:06:18
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 353/1000 [0m                      

                       Computation: 12561 steps/s (collection: 0.928s, learning 0.051s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -0.1508
                       Mean reward: 7.47
               Mean episode length: 856.70
Episode_Reward/track_lin_vel_xy_exp: 0.1782
Episode_Reward/track_ang_vel_z_exp: 0.3756
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.1029
         Episode_Reward/dof_acc_l2: -0.0280
     Episode_Reward/action_rate_l2: -0.0224
      Episode_Reward/feet_air_time: -0.0279
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 0.98s
                      Time elapsed: 00:06:19
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 354/1000 [0m                      

                       Computation: 11527 steps/s (collection: 0.994s, learning 0.072s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -0.1358
                       Mean reward: 7.76
               Mean episode length: 885.79
Episode_Reward/track_lin_vel_xy_exp: 0.3574
Episode_Reward/track_ang_vel_z_exp: 0.4132
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1162
         Episode_Reward/dof_acc_l2: -0.0340
     Episode_Reward/action_rate_l2: -0.0249
      Episode_Reward/feet_air_time: -0.0318
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4362240
                    Iteration time: 1.07s
                      Time elapsed: 00:06:20
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 355/1000 [0m                      

                       Computation: 11749 steps/s (collection: 1.000s, learning 0.046s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -0.1413
                       Mean reward: 7.79
               Mean episode length: 880.17
Episode_Reward/track_lin_vel_xy_exp: 0.2058
Episode_Reward/track_ang_vel_z_exp: 0.2601
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.0242
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0274
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: -0.0260
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 1.05s
                      Time elapsed: 00:06:21
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 356/1000 [0m                      

                       Computation: 11962 steps/s (collection: 0.982s, learning 0.045s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -0.1754
                       Mean reward: 7.89
               Mean episode length: 878.55
Episode_Reward/track_lin_vel_xy_exp: 0.2581
Episode_Reward/track_ang_vel_z_exp: 0.3514
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.0206
     Episode_Reward/action_rate_l2: -0.0202
      Episode_Reward/feet_air_time: -0.0225
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0185
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4386816
                    Iteration time: 1.03s
                      Time elapsed: 00:06:22
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 357/1000 [0m                      

                       Computation: 12074 steps/s (collection: 0.966s, learning 0.051s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -0.2037
                       Mean reward: 7.59
               Mean episode length: 887.72
Episode_Reward/track_lin_vel_xy_exp: 0.3283
Episode_Reward/track_ang_vel_z_exp: 0.3455
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0200
     Episode_Reward/dof_torques_l2: -0.1011
         Episode_Reward/dof_acc_l2: -0.0247
     Episode_Reward/action_rate_l2: -0.0235
      Episode_Reward/feet_air_time: -0.0255
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0164
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 1.02s
                      Time elapsed: 00:06:23
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 358/1000 [0m                      

                       Computation: 11670 steps/s (collection: 1.006s, learning 0.047s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -0.2077
                       Mean reward: 7.01
               Mean episode length: 860.85
Episode_Reward/track_lin_vel_xy_exp: 0.1273
Episode_Reward/track_ang_vel_z_exp: 0.2941
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0986
         Episode_Reward/dof_acc_l2: -0.0276
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0225
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: -0.0231
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4411392
                    Iteration time: 1.05s
                      Time elapsed: 00:06:24
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 359/1000 [0m                      

                       Computation: 12337 steps/s (collection: 0.952s, learning 0.044s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -0.2184
                       Mean reward: 6.41
               Mean episode length: 820.86
Episode_Reward/track_lin_vel_xy_exp: 0.2125
Episode_Reward/track_ang_vel_z_exp: 0.3076
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.0241
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0236
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.00s
                      Time elapsed: 00:06:25
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 360/1000 [0m                      

                       Computation: 12471 steps/s (collection: 0.941s, learning 0.045s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -0.2171
                       Mean reward: 5.94
               Mean episode length: 818.99
Episode_Reward/track_lin_vel_xy_exp: 0.1550
Episode_Reward/track_ang_vel_z_exp: 0.3497
       Episode_Reward/lin_vel_z_l2: -0.0183
      Episode_Reward/ang_vel_xy_l2: -0.0245
     Episode_Reward/dof_torques_l2: -0.1126
         Episode_Reward/dof_acc_l2: -0.0459
     Episode_Reward/action_rate_l2: -0.0217
      Episode_Reward/feet_air_time: -0.0287
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: -0.0374
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4435968
                    Iteration time: 0.99s
                      Time elapsed: 00:06:26
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 361/1000 [0m                      

                       Computation: 11850 steps/s (collection: 0.991s, learning 0.046s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -0.1967
                       Mean reward: 5.81
               Mean episode length: 794.74
Episode_Reward/track_lin_vel_xy_exp: 0.2067
Episode_Reward/track_ang_vel_z_exp: 0.2474
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.0846
         Episode_Reward/dof_acc_l2: -0.0298
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0154
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0305
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 1.04s
                      Time elapsed: 00:06:27
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 362/1000 [0m                      

                       Computation: 11989 steps/s (collection: 0.979s, learning 0.046s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -0.1907
                       Mean reward: 5.15
               Mean episode length: 750.84
Episode_Reward/track_lin_vel_xy_exp: 0.2114
Episode_Reward/track_ang_vel_z_exp: 0.2159
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0153
     Episode_Reward/dof_torques_l2: -0.0741
         Episode_Reward/dof_acc_l2: -0.0228
     Episode_Reward/action_rate_l2: -0.0154
      Episode_Reward/feet_air_time: -0.0182
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0256
  Episode_Termination/base_contact: 1.0000
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4460544
                    Iteration time: 1.02s
                      Time elapsed: 00:06:28
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 363/1000 [0m                      

                       Computation: 11965 steps/s (collection: 0.972s, learning 0.055s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -0.1905
                       Mean reward: 5.80
               Mean episode length: 755.81
Episode_Reward/track_lin_vel_xy_exp: 0.2681
Episode_Reward/track_ang_vel_z_exp: 0.3218
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.0905
         Episode_Reward/dof_acc_l2: -0.0244
     Episode_Reward/action_rate_l2: -0.0206
      Episode_Reward/feet_air_time: -0.0250
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0254
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 1.03s
                      Time elapsed: 00:06:29
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 364/1000 [0m                      

                       Computation: 12380 steps/s (collection: 0.945s, learning 0.048s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0062
                 Mean entropy loss: -0.2185
                       Mean reward: 6.54
               Mean episode length: 788.93
Episode_Reward/track_lin_vel_xy_exp: 0.3295
Episode_Reward/track_ang_vel_z_exp: 0.3449
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.1146
         Episode_Reward/dof_acc_l2: -0.0360
     Episode_Reward/action_rate_l2: -0.0234
      Episode_Reward/feet_air_time: -0.0299
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0240
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4485120
                    Iteration time: 0.99s
                      Time elapsed: 00:06:30
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 365/1000 [0m                      

                       Computation: 12337 steps/s (collection: 0.949s, learning 0.047s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -0.2101
                       Mean reward: 6.69
               Mean episode length: 798.64
Episode_Reward/track_lin_vel_xy_exp: 0.2434
Episode_Reward/track_ang_vel_z_exp: 0.2499
       Episode_Reward/lin_vel_z_l2: -0.0101
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0993
         Episode_Reward/dof_acc_l2: -0.0261
     Episode_Reward/action_rate_l2: -0.0225
      Episode_Reward/feet_air_time: -0.0288
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0225
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 1.00s
                      Time elapsed: 00:06:31
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 366/1000 [0m                      

                       Computation: 12790 steps/s (collection: 0.915s, learning 0.046s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -0.2080
                       Mean reward: 6.93
               Mean episode length: 817.21
Episode_Reward/track_lin_vel_xy_exp: 0.2984
Episode_Reward/track_ang_vel_z_exp: 0.2832
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.0213
     Episode_Reward/action_rate_l2: -0.0228
      Episode_Reward/feet_air_time: -0.0260
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0190
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4509696
                    Iteration time: 0.96s
                      Time elapsed: 00:06:32
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 367/1000 [0m                      

                       Computation: 12496 steps/s (collection: 0.936s, learning 0.047s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -0.2234
                       Mean reward: 7.10
               Mean episode length: 837.06
Episode_Reward/track_lin_vel_xy_exp: 0.2216
Episode_Reward/track_ang_vel_z_exp: 0.3290
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0979
         Episode_Reward/dof_acc_l2: -0.0273
     Episode_Reward/action_rate_l2: -0.0197
      Episode_Reward/feet_air_time: -0.0267
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0178
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.98s
                      Time elapsed: 00:06:33
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 368/1000 [0m                      

                       Computation: 12018 steps/s (collection: 0.975s, learning 0.048s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -0.2496
                       Mean reward: 7.12
               Mean episode length: 832.73
Episode_Reward/track_lin_vel_xy_exp: 0.2193
Episode_Reward/track_ang_vel_z_exp: 0.2208
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0151
     Episode_Reward/dof_torques_l2: -0.0697
         Episode_Reward/dof_acc_l2: -0.0194
     Episode_Reward/action_rate_l2: -0.0154
      Episode_Reward/feet_air_time: -0.0196
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0243
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4534272
                    Iteration time: 1.02s
                      Time elapsed: 00:06:34
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 369/1000 [0m                      

                       Computation: 12046 steps/s (collection: 0.973s, learning 0.047s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -0.2587
                       Mean reward: 7.44
               Mean episode length: 843.06
Episode_Reward/track_lin_vel_xy_exp: 0.3123
Episode_Reward/track_ang_vel_z_exp: 0.3472
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0166
     Episode_Reward/dof_torques_l2: -0.1005
         Episode_Reward/dof_acc_l2: -0.0227
     Episode_Reward/action_rate_l2: -0.0210
      Episode_Reward/feet_air_time: -0.0274
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 1.02s
                      Time elapsed: 00:06:35
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 370/1000 [0m                      

                       Computation: 11580 steps/s (collection: 1.015s, learning 0.046s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0045
                 Mean entropy loss: -0.2675
                       Mean reward: 7.27
               Mean episode length: 860.60
Episode_Reward/track_lin_vel_xy_exp: 0.3046
Episode_Reward/track_ang_vel_z_exp: 0.4095
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1148
         Episode_Reward/dof_acc_l2: -0.0390
     Episode_Reward/action_rate_l2: -0.0238
      Episode_Reward/feet_air_time: -0.0290
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0171
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4558848
                    Iteration time: 1.06s
                      Time elapsed: 00:06:36
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 371/1000 [0m                      

                       Computation: 11373 steps/s (collection: 1.033s, learning 0.047s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -0.3009
                       Mean reward: 7.33
               Mean episode length: 851.73
Episode_Reward/track_lin_vel_xy_exp: 0.4031
Episode_Reward/track_ang_vel_z_exp: 0.3948
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.0194
     Episode_Reward/action_rate_l2: -0.0225
      Episode_Reward/feet_air_time: -0.0227
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 1.08s
                      Time elapsed: 00:06:37
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 372/1000 [0m                      

                       Computation: 11546 steps/s (collection: 1.019s, learning 0.046s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -0.3189
                       Mean reward: 7.55
               Mean episode length: 842.94
Episode_Reward/track_lin_vel_xy_exp: 0.3181
Episode_Reward/track_ang_vel_z_exp: 0.2114
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0125
     Episode_Reward/dof_torques_l2: -0.0670
         Episode_Reward/dof_acc_l2: -0.0157
     Episode_Reward/action_rate_l2: -0.0155
      Episode_Reward/feet_air_time: -0.0167
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0126
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4583424
                    Iteration time: 1.06s
                      Time elapsed: 00:06:38
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 373/1000 [0m                      

                       Computation: 10915 steps/s (collection: 1.072s, learning 0.054s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -0.3448
                       Mean reward: 7.48
               Mean episode length: 849.84
Episode_Reward/track_lin_vel_xy_exp: 0.1553
Episode_Reward/track_ang_vel_z_exp: 0.2605
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0145
     Episode_Reward/dof_torques_l2: -0.0662
         Episode_Reward/dof_acc_l2: -0.0146
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0196
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 1.13s
                      Time elapsed: 00:06:39
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 374/1000 [0m                      

                       Computation: 11370 steps/s (collection: 1.035s, learning 0.045s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -0.3741
                       Mean reward: 7.33
               Mean episode length: 831.72
Episode_Reward/track_lin_vel_xy_exp: 0.2217
Episode_Reward/track_ang_vel_z_exp: 0.3873
       Episode_Reward/lin_vel_z_l2: -0.0188
      Episode_Reward/ang_vel_xy_l2: -0.0266
     Episode_Reward/dof_torques_l2: -0.1356
         Episode_Reward/dof_acc_l2: -0.0488
     Episode_Reward/action_rate_l2: -0.0241
      Episode_Reward/feet_air_time: -0.0362
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0244
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4608000
                    Iteration time: 1.08s
                      Time elapsed: 00:06:40
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 375/1000 [0m                      

                       Computation: 11123 steps/s (collection: 1.056s, learning 0.049s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -0.4233
                       Mean reward: 7.13
               Mean episode length: 835.02
Episode_Reward/track_lin_vel_xy_exp: 0.0979
Episode_Reward/track_ang_vel_z_exp: 0.1915
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0131
     Episode_Reward/dof_torques_l2: -0.0673
         Episode_Reward/dof_acc_l2: -0.0178
     Episode_Reward/action_rate_l2: -0.0139
      Episode_Reward/feet_air_time: -0.0157
 Episode_Reward/undesired_contacts: -0.0186
Episode_Reward/flat_orientation_l2: -0.0377
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.10s
                      Time elapsed: 00:06:41
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 376/1000 [0m                      

                       Computation: 10797 steps/s (collection: 1.092s, learning 0.047s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -0.4705
                       Mean reward: 7.29
               Mean episode length: 826.26
Episode_Reward/track_lin_vel_xy_exp: 0.3762
Episode_Reward/track_ang_vel_z_exp: 0.3101
       Episode_Reward/lin_vel_z_l2: -0.0163
      Episode_Reward/ang_vel_xy_l2: -0.0193
     Episode_Reward/dof_torques_l2: -0.0969
         Episode_Reward/dof_acc_l2: -0.0374
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0194
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4632576
                    Iteration time: 1.14s
                      Time elapsed: 00:06:43
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 377/1000 [0m                      

                       Computation: 11111 steps/s (collection: 1.062s, learning 0.044s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -0.4921
                       Mean reward: 7.23
               Mean episode length: 838.97
Episode_Reward/track_lin_vel_xy_exp: 0.1807
Episode_Reward/track_ang_vel_z_exp: 0.3983
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0165
     Episode_Reward/dof_torques_l2: -0.1106
         Episode_Reward/dof_acc_l2: -0.0216
     Episode_Reward/action_rate_l2: -0.0205
      Episode_Reward/feet_air_time: -0.0205
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0219
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 1.11s
                      Time elapsed: 00:06:44
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 378/1000 [0m                      

                       Computation: 10822 steps/s (collection: 1.083s, learning 0.053s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0036
                 Mean entropy loss: -0.4904
                       Mean reward: 7.47
               Mean episode length: 846.40
Episode_Reward/track_lin_vel_xy_exp: 0.4351
Episode_Reward/track_ang_vel_z_exp: 0.4555
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.1314
         Episode_Reward/dof_acc_l2: -0.0336
     Episode_Reward/action_rate_l2: -0.0244
      Episode_Reward/feet_air_time: -0.0322
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0266
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4657152
                    Iteration time: 1.14s
                      Time elapsed: 00:06:45
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 379/1000 [0m                      

                       Computation: 11202 steps/s (collection: 1.048s, learning 0.049s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -0.4995
                       Mean reward: 7.32
               Mean episode length: 843.04
Episode_Reward/track_lin_vel_xy_exp: 0.1288
Episode_Reward/track_ang_vel_z_exp: 0.3877
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.0193
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0212
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0219
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 1.10s
                      Time elapsed: 00:06:46
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 380/1000 [0m                      

                       Computation: 10315 steps/s (collection: 1.093s, learning 0.098s)
             Mean action noise std: 0.24
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -0.5207
                       Mean reward: 7.27
               Mean episode length: 840.44
Episode_Reward/track_lin_vel_xy_exp: 0.3204
Episode_Reward/track_ang_vel_z_exp: 0.3240
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.0904
         Episode_Reward/dof_acc_l2: -0.0227
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0221
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0216
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4681728
                    Iteration time: 1.19s
                      Time elapsed: 00:06:47
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 381/1000 [0m                      

                       Computation: 11364 steps/s (collection: 1.030s, learning 0.052s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -0.5383
                       Mean reward: 7.14
               Mean episode length: 837.00
Episode_Reward/track_lin_vel_xy_exp: 0.2257
Episode_Reward/track_ang_vel_z_exp: 0.3327
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.1111
         Episode_Reward/dof_acc_l2: -0.0280
     Episode_Reward/action_rate_l2: -0.0217
      Episode_Reward/feet_air_time: -0.0226
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0223
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 1.08s
                      Time elapsed: 00:06:48
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 382/1000 [0m                      

                       Computation: 11375 steps/s (collection: 1.034s, learning 0.046s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -0.5703
                       Mean reward: 6.94
               Mean episode length: 834.08
Episode_Reward/track_lin_vel_xy_exp: 0.2261
Episode_Reward/track_ang_vel_z_exp: 0.3403
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.1135
         Episode_Reward/dof_acc_l2: -0.0297
     Episode_Reward/action_rate_l2: -0.0229
      Episode_Reward/feet_air_time: -0.0292
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4706304
                    Iteration time: 1.08s
                      Time elapsed: 00:06:49
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 383/1000 [0m                      

                       Computation: 11407 steps/s (collection: 1.022s, learning 0.055s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -0.5853
                       Mean reward: 7.18
               Mean episode length: 847.84
Episode_Reward/track_lin_vel_xy_exp: 0.4585
Episode_Reward/track_ang_vel_z_exp: 0.3538
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.0951
         Episode_Reward/dof_acc_l2: -0.0254
     Episode_Reward/action_rate_l2: -0.0210
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.08s
                      Time elapsed: 00:06:50
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 384/1000 [0m                      

                       Computation: 11328 steps/s (collection: 1.035s, learning 0.050s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -0.5997
                       Mean reward: 7.11
               Mean episode length: 852.65
Episode_Reward/track_lin_vel_xy_exp: 0.2231
Episode_Reward/track_ang_vel_z_exp: 0.2647
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0865
         Episode_Reward/dof_acc_l2: -0.0242
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0220
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0258
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4730880
                    Iteration time: 1.08s
                      Time elapsed: 00:06:51
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 385/1000 [0m                      

                       Computation: 11637 steps/s (collection: 1.002s, learning 0.054s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -0.6117
                       Mean reward: 7.13
               Mean episode length: 863.52
Episode_Reward/track_lin_vel_xy_exp: 0.2997
Episode_Reward/track_ang_vel_z_exp: 0.3142
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0165
     Episode_Reward/dof_torques_l2: -0.1003
         Episode_Reward/dof_acc_l2: -0.0278
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0244
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 1.06s
                      Time elapsed: 00:06:52
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 386/1000 [0m                      

                       Computation: 11427 steps/s (collection: 1.027s, learning 0.048s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -0.6658
                       Mean reward: 7.37
               Mean episode length: 867.10
Episode_Reward/track_lin_vel_xy_exp: 0.3337
Episode_Reward/track_ang_vel_z_exp: 0.3958
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1067
         Episode_Reward/dof_acc_l2: -0.0267
     Episode_Reward/action_rate_l2: -0.0209
      Episode_Reward/feet_air_time: -0.0280
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0227
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 4755456
                    Iteration time: 1.08s
                      Time elapsed: 00:06:54
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 387/1000 [0m                      

                       Computation: 11594 steps/s (collection: 1.003s, learning 0.056s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -0.7025
                       Mean reward: 7.24
               Mean episode length: 882.18
Episode_Reward/track_lin_vel_xy_exp: 0.2492
Episode_Reward/track_ang_vel_z_exp: 0.3719
       Episode_Reward/lin_vel_z_l2: -0.0154
      Episode_Reward/ang_vel_xy_l2: -0.0222
     Episode_Reward/dof_torques_l2: -0.1146
         Episode_Reward/dof_acc_l2: -0.0352
     Episode_Reward/action_rate_l2: -0.0236
      Episode_Reward/feet_air_time: -0.0302
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0290
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 1.06s
                      Time elapsed: 00:06:55
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 388/1000 [0m                      

                       Computation: 10887 steps/s (collection: 1.083s, learning 0.046s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -0.7315
                       Mean reward: 7.36
               Mean episode length: 876.93
Episode_Reward/track_lin_vel_xy_exp: 0.2696
Episode_Reward/track_ang_vel_z_exp: 0.3016
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0922
         Episode_Reward/dof_acc_l2: -0.0240
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0255
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: -0.0267
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4780032
                    Iteration time: 1.13s
                      Time elapsed: 00:06:56
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 389/1000 [0m                      

                       Computation: 11720 steps/s (collection: 0.996s, learning 0.053s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -0.7574
                       Mean reward: 7.30
               Mean episode length: 878.65
Episode_Reward/track_lin_vel_xy_exp: 0.2680
Episode_Reward/track_ang_vel_z_exp: 0.3561
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0200
     Episode_Reward/dof_torques_l2: -0.1038
         Episode_Reward/dof_acc_l2: -0.0225
     Episode_Reward/action_rate_l2: -0.0220
      Episode_Reward/feet_air_time: -0.0215
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0223
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 1.05s
                      Time elapsed: 00:06:57
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 390/1000 [0m                      

                       Computation: 10624 steps/s (collection: 1.103s, learning 0.054s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -0.7602
                       Mean reward: 7.72
               Mean episode length: 889.46
Episode_Reward/track_lin_vel_xy_exp: 0.2654
Episode_Reward/track_ang_vel_z_exp: 0.3143
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.0184
     Episode_Reward/action_rate_l2: -0.0197
      Episode_Reward/feet_air_time: -0.0199
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0230
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4804608
                    Iteration time: 1.16s
                      Time elapsed: 00:06:58
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 391/1000 [0m                      

                       Computation: 11153 steps/s (collection: 1.049s, learning 0.053s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -0.7811
                       Mean reward: 7.49
               Mean episode length: 877.42
Episode_Reward/track_lin_vel_xy_exp: 0.3533
Episode_Reward/track_ang_vel_z_exp: 0.3272
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.1167
         Episode_Reward/dof_acc_l2: -0.0299
     Episode_Reward/action_rate_l2: -0.0223
      Episode_Reward/feet_air_time: -0.0340
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0206
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.10s
                      Time elapsed: 00:06:59
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 392/1000 [0m                      

                       Computation: 10991 steps/s (collection: 1.037s, learning 0.081s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -0.7893
                       Mean reward: 7.49
               Mean episode length: 890.16
Episode_Reward/track_lin_vel_xy_exp: 0.4599
Episode_Reward/track_ang_vel_z_exp: 0.3744
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0264
     Episode_Reward/dof_torques_l2: -0.1302
         Episode_Reward/dof_acc_l2: -0.0497
     Episode_Reward/action_rate_l2: -0.0242
      Episode_Reward/feet_air_time: -0.0325
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0313
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4829184
                    Iteration time: 1.12s
                      Time elapsed: 00:07:00
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 393/1000 [0m                      

                       Computation: 10896 steps/s (collection: 1.075s, learning 0.053s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -0.8149
                       Mean reward: 7.44
               Mean episode length: 889.47
Episode_Reward/track_lin_vel_xy_exp: 0.2336
Episode_Reward/track_ang_vel_z_exp: 0.2869
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.0969
         Episode_Reward/dof_acc_l2: -0.0189
     Episode_Reward/action_rate_l2: -0.0209
      Episode_Reward/feet_air_time: -0.0191
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0204
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 1.13s
                      Time elapsed: 00:07:01
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 394/1000 [0m                      

                       Computation: 10832 steps/s (collection: 1.078s, learning 0.056s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -0.8162
                       Mean reward: 7.26
               Mean episode length: 888.97
Episode_Reward/track_lin_vel_xy_exp: 0.2244
Episode_Reward/track_ang_vel_z_exp: 0.3177
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.0934
         Episode_Reward/dof_acc_l2: -0.0209
     Episode_Reward/action_rate_l2: -0.0194
      Episode_Reward/feet_air_time: -0.0257
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0232
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 4853760
                    Iteration time: 1.13s
                      Time elapsed: 00:07:02
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 395/1000 [0m                      

                       Computation: 11112 steps/s (collection: 1.048s, learning 0.058s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -0.8244
                       Mean reward: 6.79
               Mean episode length: 900.44
Episode_Reward/track_lin_vel_xy_exp: 0.1461
Episode_Reward/track_ang_vel_z_exp: 0.2865
       Episode_Reward/lin_vel_z_l2: -0.0105
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.0797
         Episode_Reward/dof_acc_l2: -0.0183
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0217
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0310
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 1.11s
                      Time elapsed: 00:07:04
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 396/1000 [0m                      

                       Computation: 11517 steps/s (collection: 1.018s, learning 0.049s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -0.8776
                       Mean reward: 7.24
               Mean episode length: 939.97
Episode_Reward/track_lin_vel_xy_exp: 0.3248
Episode_Reward/track_ang_vel_z_exp: 0.3516
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.1066
         Episode_Reward/dof_acc_l2: -0.0278
     Episode_Reward/action_rate_l2: -0.0228
      Episode_Reward/feet_air_time: -0.0251
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0194
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4878336
                    Iteration time: 1.07s
                      Time elapsed: 00:07:05
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 397/1000 [0m                      

                       Computation: 11106 steps/s (collection: 1.058s, learning 0.048s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -0.9237
                       Mean reward: 7.01
               Mean episode length: 945.29
Episode_Reward/track_lin_vel_xy_exp: 0.2903
Episode_Reward/track_ang_vel_z_exp: 0.3238
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.0944
         Episode_Reward/dof_acc_l2: -0.0258
     Episode_Reward/action_rate_l2: -0.0226
      Episode_Reward/feet_air_time: -0.0262
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 1.11s
                      Time elapsed: 00:07:06
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 398/1000 [0m                      

                       Computation: 10715 steps/s (collection: 1.093s, learning 0.054s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -0.9534
                       Mean reward: 7.54
               Mean episode length: 958.47
Episode_Reward/track_lin_vel_xy_exp: 0.4037
Episode_Reward/track_ang_vel_z_exp: 0.3330
       Episode_Reward/lin_vel_z_l2: -0.0103
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.1050
         Episode_Reward/dof_acc_l2: -0.0227
     Episode_Reward/action_rate_l2: -0.0227
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4902912
                    Iteration time: 1.15s
                      Time elapsed: 00:07:07
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 399/1000 [0m                      

                       Computation: 11027 steps/s (collection: 1.070s, learning 0.044s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -0.9738
                       Mean reward: 7.44
               Mean episode length: 966.54
Episode_Reward/track_lin_vel_xy_exp: 0.1926
Episode_Reward/track_ang_vel_z_exp: 0.3313
       Episode_Reward/lin_vel_z_l2: -0.0093
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0972
         Episode_Reward/dof_acc_l2: -0.0155
     Episode_Reward/action_rate_l2: -0.0220
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0231
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.11s
                      Time elapsed: 00:07:08
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 400/1000 [0m                      

                       Computation: 11139 steps/s (collection: 1.056s, learning 0.048s)
             Mean action noise std: 0.23
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -1.0011
                       Mean reward: 7.36
               Mean episode length: 951.69
Episode_Reward/track_lin_vel_xy_exp: 0.2410
Episode_Reward/track_ang_vel_z_exp: 0.3189
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.0924
         Episode_Reward/dof_acc_l2: -0.0187
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0182
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4927488
                    Iteration time: 1.10s
                      Time elapsed: 00:07:09
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 401/1000 [0m                      

                       Computation: 11193 steps/s (collection: 1.047s, learning 0.051s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -1.0451
                       Mean reward: 7.92
               Mean episode length: 958.44
Episode_Reward/track_lin_vel_xy_exp: 0.2506
Episode_Reward/track_ang_vel_z_exp: 0.2989
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0165
     Episode_Reward/dof_torques_l2: -0.0798
         Episode_Reward/dof_acc_l2: -0.0158
     Episode_Reward/action_rate_l2: -0.0201
      Episode_Reward/feet_air_time: -0.0205
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0142
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 1.10s
                      Time elapsed: 00:07:10
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 402/1000 [0m                      

                       Computation: 11386 steps/s (collection: 1.035s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0068
                 Mean entropy loss: -1.1061
                       Mean reward: 7.87
               Mean episode length: 930.15
Episode_Reward/track_lin_vel_xy_exp: 0.2102
Episode_Reward/track_ang_vel_z_exp: 0.2607
       Episode_Reward/lin_vel_z_l2: -0.0097
      Episode_Reward/ang_vel_xy_l2: -0.0161
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.0187
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0239
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4952064
                    Iteration time: 1.08s
                      Time elapsed: 00:07:11
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 403/1000 [0m                      

                       Computation: 11048 steps/s (collection: 1.045s, learning 0.067s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -1.1519
                       Mean reward: 7.80
               Mean episode length: 923.60
Episode_Reward/track_lin_vel_xy_exp: 0.3360
Episode_Reward/track_ang_vel_z_exp: 0.2893
       Episode_Reward/lin_vel_z_l2: -0.0102
      Episode_Reward/ang_vel_xy_l2: -0.0176
     Episode_Reward/dof_torques_l2: -0.0905
         Episode_Reward/dof_acc_l2: -0.0198
     Episode_Reward/action_rate_l2: -0.0204
      Episode_Reward/feet_air_time: -0.0182
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 1.11s
                      Time elapsed: 00:07:12
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 404/1000 [0m                      

                       Computation: 11372 steps/s (collection: 1.035s, learning 0.046s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -1.1888
                       Mean reward: 7.64
               Mean episode length: 923.60
Episode_Reward/track_lin_vel_xy_exp: 0.2006
Episode_Reward/track_ang_vel_z_exp: 0.4398
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.1091
         Episode_Reward/dof_acc_l2: -0.0237
     Episode_Reward/action_rate_l2: -0.0218
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0185
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4976640
                    Iteration time: 1.08s
                      Time elapsed: 00:07:13
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 405/1000 [0m                      

                       Computation: 11250 steps/s (collection: 1.043s, learning 0.049s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -1.2267
                       Mean reward: 7.74
               Mean episode length: 921.57
Episode_Reward/track_lin_vel_xy_exp: 0.2336
Episode_Reward/track_ang_vel_z_exp: 0.2974
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.1004
         Episode_Reward/dof_acc_l2: -0.0185
     Episode_Reward/action_rate_l2: -0.0215
      Episode_Reward/feet_air_time: -0.0251
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0174
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 1.09s
                      Time elapsed: 00:07:15
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 406/1000 [0m                      

                       Computation: 11747 steps/s (collection: 0.998s, learning 0.048s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -1.2607
                       Mean reward: 8.08
               Mean episode length: 940.91
Episode_Reward/track_lin_vel_xy_exp: 0.1983
Episode_Reward/track_ang_vel_z_exp: 0.3345
       Episode_Reward/lin_vel_z_l2: -0.0103
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0985
         Episode_Reward/dof_acc_l2: -0.0184
     Episode_Reward/action_rate_l2: -0.0218
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0169
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5001216
                    Iteration time: 1.05s
                      Time elapsed: 00:07:16
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 407/1000 [0m                      

                       Computation: 11139 steps/s (collection: 1.054s, learning 0.049s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0054
                 Mean entropy loss: -1.2599
                       Mean reward: 7.58
               Mean episode length: 929.97
Episode_Reward/track_lin_vel_xy_exp: 0.2351
Episode_Reward/track_ang_vel_z_exp: 0.2750
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.1009
         Episode_Reward/dof_acc_l2: -0.0244
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0224
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0199
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.10s
                      Time elapsed: 00:07:17
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 408/1000 [0m                      

                       Computation: 10958 steps/s (collection: 1.071s, learning 0.051s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -1.3092
                       Mean reward: 7.54
               Mean episode length: 948.40
Episode_Reward/track_lin_vel_xy_exp: 0.1653
Episode_Reward/track_ang_vel_z_exp: 0.2150
       Episode_Reward/lin_vel_z_l2: -0.0094
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0965
         Episode_Reward/dof_acc_l2: -0.0206
     Episode_Reward/action_rate_l2: -0.0216
      Episode_Reward/feet_air_time: -0.0302
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0174
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5025792
                    Iteration time: 1.12s
                      Time elapsed: 00:07:18
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 409/1000 [0m                      

                       Computation: 11467 steps/s (collection: 1.019s, learning 0.052s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -1.3331
                       Mean reward: 7.75
               Mean episode length: 967.47
Episode_Reward/track_lin_vel_xy_exp: 0.2628
Episode_Reward/track_ang_vel_z_exp: 0.3651
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0167
     Episode_Reward/dof_torques_l2: -0.1009
         Episode_Reward/dof_acc_l2: -0.0260
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0261
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0154
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 1.07s
                      Time elapsed: 00:07:19
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 410/1000 [0m                      

                       Computation: 11152 steps/s (collection: 1.047s, learning 0.055s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -1.3538
                       Mean reward: 7.32
               Mean episode length: 950.82
Episode_Reward/track_lin_vel_xy_exp: 0.2340
Episode_Reward/track_ang_vel_z_exp: 0.2791
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.1002
         Episode_Reward/dof_acc_l2: -0.0202
     Episode_Reward/action_rate_l2: -0.0206
      Episode_Reward/feet_air_time: -0.0253
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5050368
                    Iteration time: 1.10s
                      Time elapsed: 00:07:20
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 411/1000 [0m                      

                       Computation: 11167 steps/s (collection: 1.055s, learning 0.046s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -1.3640
                       Mean reward: 7.05
               Mean episode length: 934.68
Episode_Reward/track_lin_vel_xy_exp: 0.2467
Episode_Reward/track_ang_vel_z_exp: 0.2776
       Episode_Reward/lin_vel_z_l2: -0.0097
      Episode_Reward/ang_vel_xy_l2: -0.0167
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.0210
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0195
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 1.10s
                      Time elapsed: 00:07:21
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 412/1000 [0m                      

                       Computation: 10879 steps/s (collection: 1.083s, learning 0.046s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -1.3721
                       Mean reward: 6.85
               Mean episode length: 922.96
Episode_Reward/track_lin_vel_xy_exp: 0.2181
Episode_Reward/track_ang_vel_z_exp: 0.2684
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.0189
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0195
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0215
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5074944
                    Iteration time: 1.13s
                      Time elapsed: 00:07:22
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 413/1000 [0m                      

                       Computation: 11101 steps/s (collection: 1.062s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -1.3830
                       Mean reward: 7.11
               Mean episode length: 922.96
Episode_Reward/track_lin_vel_xy_exp: 0.2163
Episode_Reward/track_ang_vel_z_exp: 0.4205
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1085
         Episode_Reward/dof_acc_l2: -0.0255
     Episode_Reward/action_rate_l2: -0.0214
      Episode_Reward/feet_air_time: -0.0228
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0225
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 1.11s
                      Time elapsed: 00:07:23
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 414/1000 [0m                      

                       Computation: 10785 steps/s (collection: 1.078s, learning 0.061s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -1.3820
                       Mean reward: 6.19
               Mean episode length: 902.85
Episode_Reward/track_lin_vel_xy_exp: 0.1820
Episode_Reward/track_ang_vel_z_exp: 0.2792
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.0951
         Episode_Reward/dof_acc_l2: -0.0308
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0273
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0295
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5099520
                    Iteration time: 1.14s
                      Time elapsed: 00:07:24
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 415/1000 [0m                      

                       Computation: 10810 steps/s (collection: 1.081s, learning 0.056s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -1.3887
                       Mean reward: 6.04
               Mean episode length: 904.48
Episode_Reward/track_lin_vel_xy_exp: 0.0822
Episode_Reward/track_ang_vel_z_exp: 0.2066
       Episode_Reward/lin_vel_z_l2: -0.0085
      Episode_Reward/ang_vel_xy_l2: -0.0143
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.0133
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0142
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: -0.0239
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.14s
                      Time elapsed: 00:07:26
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 416/1000 [0m                      

                       Computation: 11454 steps/s (collection: 1.029s, learning 0.044s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -1.3888
                       Mean reward: 5.95
               Mean episode length: 903.88
Episode_Reward/track_lin_vel_xy_exp: 0.1041
Episode_Reward/track_ang_vel_z_exp: 0.3369
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.1069
         Episode_Reward/dof_acc_l2: -0.0221
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0193
 Episode_Reward/undesired_contacts: -0.0120
Episode_Reward/flat_orientation_l2: -0.0335
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5124096
                    Iteration time: 1.07s
                      Time elapsed: 00:07:27
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 417/1000 [0m                      

                       Computation: 11098 steps/s (collection: 1.055s, learning 0.052s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -1.4021
                       Mean reward: 5.98
               Mean episode length: 903.88
Episode_Reward/track_lin_vel_xy_exp: 0.2536
Episode_Reward/track_ang_vel_z_exp: 0.3856
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.1104
         Episode_Reward/dof_acc_l2: -0.0255
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0196
 Episode_Reward/undesired_contacts: -0.0126
Episode_Reward/flat_orientation_l2: -0.0313
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 1.11s
                      Time elapsed: 00:07:28
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 418/1000 [0m                      

                       Computation: 11293 steps/s (collection: 1.044s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -1.4100
                       Mean reward: 5.96
               Mean episode length: 880.16
Episode_Reward/track_lin_vel_xy_exp: 0.3444
Episode_Reward/track_ang_vel_z_exp: 0.3544
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1050
         Episode_Reward/dof_acc_l2: -0.0275
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0225
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5148672
                    Iteration time: 1.09s
                      Time elapsed: 00:07:29
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 419/1000 [0m                      

                       Computation: 10919 steps/s (collection: 1.076s, learning 0.049s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -1.4117
                       Mean reward: 5.79
               Mean episode length: 878.07
Episode_Reward/track_lin_vel_xy_exp: 0.3392
Episode_Reward/track_ang_vel_z_exp: 0.2189
       Episode_Reward/lin_vel_z_l2: -0.0094
      Episode_Reward/ang_vel_xy_l2: -0.0159
     Episode_Reward/dof_torques_l2: -0.0946
         Episode_Reward/dof_acc_l2: -0.0226
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0208
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 1.13s
                      Time elapsed: 00:07:30
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 420/1000 [0m                      

                       Computation: 10841 steps/s (collection: 1.084s, learning 0.050s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -1.4142
                       Mean reward: 5.76
               Mean episode length: 868.74
Episode_Reward/track_lin_vel_xy_exp: 0.0775
Episode_Reward/track_ang_vel_z_exp: 0.3168
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.1073
         Episode_Reward/dof_acc_l2: -0.0222
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0195
 Episode_Reward/undesired_contacts: -0.0216
Episode_Reward/flat_orientation_l2: -0.0326
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5173248
                    Iteration time: 1.13s
                      Time elapsed: 00:07:31
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 421/1000 [0m                      

                       Computation: 11602 steps/s (collection: 1.004s, learning 0.055s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -1.4312
                       Mean reward: 5.79
               Mean episode length: 885.46
Episode_Reward/track_lin_vel_xy_exp: 0.1241
Episode_Reward/track_ang_vel_z_exp: 0.2870
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0165
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.0192
     Episode_Reward/action_rate_l2: -0.0208
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0126
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 1.06s
                      Time elapsed: 00:07:32
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 422/1000 [0m                      

                       Computation: 11278 steps/s (collection: 1.035s, learning 0.055s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0053
                 Mean entropy loss: -1.4293
                       Mean reward: 6.04
               Mean episode length: 884.46
Episode_Reward/track_lin_vel_xy_exp: 0.2822
Episode_Reward/track_ang_vel_z_exp: 0.3742
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.1047
         Episode_Reward/dof_acc_l2: -0.0267
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0194
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0262
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5197824
                    Iteration time: 1.09s
                      Time elapsed: 00:07:33
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 423/1000 [0m                      

                       Computation: 11175 steps/s (collection: 1.055s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -1.4177
                       Mean reward: 6.24
               Mean episode length: 895.13
Episode_Reward/track_lin_vel_xy_exp: 0.2208
Episode_Reward/track_ang_vel_z_exp: 0.3045
       Episode_Reward/lin_vel_z_l2: -0.0086
      Episode_Reward/ang_vel_xy_l2: -0.0166
     Episode_Reward/dof_torques_l2: -0.0928
         Episode_Reward/dof_acc_l2: -0.0128
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0179
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0224
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.10s
                      Time elapsed: 00:07:34
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 424/1000 [0m                      

                       Computation: 11287 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -1.3955
                       Mean reward: 5.38
               Mean episode length: 906.36
Episode_Reward/track_lin_vel_xy_exp: 0.0655
Episode_Reward/track_ang_vel_z_exp: 0.2514
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.1095
         Episode_Reward/dof_acc_l2: -0.0203
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0197
 Episode_Reward/undesired_contacts: -0.0260
Episode_Reward/flat_orientation_l2: -0.0296
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5222400
                    Iteration time: 1.09s
                      Time elapsed: 00:07:35
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 425/1000 [0m                      

                       Computation: 11057 steps/s (collection: 1.066s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -1.3786
                       Mean reward: 5.66
               Mean episode length: 908.36
Episode_Reward/track_lin_vel_xy_exp: 0.2186
Episode_Reward/track_ang_vel_z_exp: 0.2169
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0154
     Episode_Reward/dof_torques_l2: -0.0799
         Episode_Reward/dof_acc_l2: -0.0174
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0185
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 1.11s
                      Time elapsed: 00:07:37
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 426/1000 [0m                      

                       Computation: 11520 steps/s (collection: 1.021s, learning 0.046s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -1.3832
                       Mean reward: 5.41
               Mean episode length: 902.84
Episode_Reward/track_lin_vel_xy_exp: 0.1792
Episode_Reward/track_ang_vel_z_exp: 0.2649
       Episode_Reward/lin_vel_z_l2: -0.0091
      Episode_Reward/ang_vel_xy_l2: -0.0159
     Episode_Reward/dof_torques_l2: -0.1025
         Episode_Reward/dof_acc_l2: -0.0210
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0253
 Episode_Reward/undesired_contacts: -0.0069
Episode_Reward/flat_orientation_l2: -0.0213
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5246976
                    Iteration time: 1.07s
                      Time elapsed: 00:07:38
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 427/1000 [0m                      

                       Computation: 10926 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -1.3876
                       Mean reward: 5.33
               Mean episode length: 920.68
Episode_Reward/track_lin_vel_xy_exp: 0.2052
Episode_Reward/track_ang_vel_z_exp: 0.2632
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0153
     Episode_Reward/dof_torques_l2: -0.1077
         Episode_Reward/dof_acc_l2: -0.0197
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0126
 Episode_Reward/undesired_contacts: -0.0257
Episode_Reward/flat_orientation_l2: -0.0324
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 1.12s
                      Time elapsed: 00:07:39
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 428/1000 [0m                      

                       Computation: 11371 steps/s (collection: 1.034s, learning 0.046s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -1.3756
                       Mean reward: 5.06
               Mean episode length: 921.05
Episode_Reward/track_lin_vel_xy_exp: 0.3499
Episode_Reward/track_ang_vel_z_exp: 0.3510
       Episode_Reward/lin_vel_z_l2: -0.0162
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.1219
         Episode_Reward/dof_acc_l2: -0.0370
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0287
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5271552
                    Iteration time: 1.08s
                      Time elapsed: 00:07:40
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 429/1000 [0m                      

                       Computation: 11204 steps/s (collection: 1.052s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -1.3702
                       Mean reward: 5.20
               Mean episode length: 919.07
Episode_Reward/track_lin_vel_xy_exp: 0.2383
Episode_Reward/track_ang_vel_z_exp: 0.3190
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.0949
         Episode_Reward/dof_acc_l2: -0.0235
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0230
 Episode_Reward/undesired_contacts: -0.0206
Episode_Reward/flat_orientation_l2: -0.0324
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 1.10s
                      Time elapsed: 00:07:41
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 430/1000 [0m                      

                       Computation: 11536 steps/s (collection: 1.018s, learning 0.047s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -1.3801
                       Mean reward: 4.50
               Mean episode length: 913.51
Episode_Reward/track_lin_vel_xy_exp: 0.1753
Episode_Reward/track_ang_vel_z_exp: 0.2423
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.0231
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0196
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0202
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5296128
                    Iteration time: 1.07s
                      Time elapsed: 00:07:42
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 431/1000 [0m                      

                       Computation: 11603 steps/s (collection: 0.994s, learning 0.065s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -1.3795
                       Mean reward: 5.64
               Mean episode length: 911.99
Episode_Reward/track_lin_vel_xy_exp: 0.2202
Episode_Reward/track_ang_vel_z_exp: 0.2979
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.0995
         Episode_Reward/dof_acc_l2: -0.0197
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0181
 Episode_Reward/undesired_contacts: -0.0088
Episode_Reward/flat_orientation_l2: -0.0228
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.06s
                      Time elapsed: 00:07:43
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 432/1000 [0m                      

                       Computation: 11650 steps/s (collection: 1.010s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -1.3698
                       Mean reward: 5.66
               Mean episode length: 896.74
Episode_Reward/track_lin_vel_xy_exp: 0.2026
Episode_Reward/track_ang_vel_z_exp: 0.3527
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.1165
         Episode_Reward/dof_acc_l2: -0.0296
     Episode_Reward/action_rate_l2: -0.0193
      Episode_Reward/feet_air_time: -0.0253
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: -0.0242
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5320704
                    Iteration time: 1.05s
                      Time elapsed: 00:07:44
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 433/1000 [0m                      

                       Computation: 10971 steps/s (collection: 1.074s, learning 0.046s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -1.3543
                       Mean reward: 5.52
               Mean episode length: 895.01
Episode_Reward/track_lin_vel_xy_exp: 0.1933
Episode_Reward/track_ang_vel_z_exp: 0.3118
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1058
         Episode_Reward/dof_acc_l2: -0.0268
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0231
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: -0.0226
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 1.12s
                      Time elapsed: 00:07:45
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 434/1000 [0m                      

                       Computation: 11296 steps/s (collection: 1.041s, learning 0.047s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -1.3408
                       Mean reward: 5.61
               Mean episode length: 904.80
Episode_Reward/track_lin_vel_xy_exp: 0.2523
Episode_Reward/track_ang_vel_z_exp: 0.3655
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.1119
         Episode_Reward/dof_acc_l2: -0.0290
     Episode_Reward/action_rate_l2: -0.0194
      Episode_Reward/feet_air_time: -0.0255
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0262
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5345280
                    Iteration time: 1.09s
                      Time elapsed: 00:07:46
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 435/1000 [0m                      

                       Computation: 10752 steps/s (collection: 1.072s, learning 0.071s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0073
                 Mean entropy loss: -1.3353
                       Mean reward: 5.46
               Mean episode length: 876.64
Episode_Reward/track_lin_vel_xy_exp: 0.1749
Episode_Reward/track_ang_vel_z_exp: 0.2426
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.1140
         Episode_Reward/dof_acc_l2: -0.0341
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0225
 Episode_Reward/undesired_contacts: -0.0063
Episode_Reward/flat_orientation_l2: -0.0282
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 1.14s
                      Time elapsed: 00:07:47
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 436/1000 [0m                      

                       Computation: 11088 steps/s (collection: 1.063s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -1.3418
                       Mean reward: 5.81
               Mean episode length: 878.52
Episode_Reward/track_lin_vel_xy_exp: 0.3225
Episode_Reward/track_ang_vel_z_exp: 0.2677
       Episode_Reward/lin_vel_z_l2: -0.0093
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.0924
         Episode_Reward/dof_acc_l2: -0.0188
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0178
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0227
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5369856
                    Iteration time: 1.11s
                      Time elapsed: 00:07:49
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 437/1000 [0m                      

                       Computation: 10720 steps/s (collection: 1.087s, learning 0.060s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -1.3564
                       Mean reward: 5.81
               Mean episode length: 872.95
Episode_Reward/track_lin_vel_xy_exp: 0.2990
Episode_Reward/track_ang_vel_z_exp: 0.3111
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.1032
         Episode_Reward/dof_acc_l2: -0.0287
     Episode_Reward/action_rate_l2: -0.0204
      Episode_Reward/feet_air_time: -0.0237
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0190
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 1.15s
                      Time elapsed: 00:07:50
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 438/1000 [0m                      

                       Computation: 11210 steps/s (collection: 1.053s, learning 0.044s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -1.3667
                       Mean reward: 5.47
               Mean episode length: 881.86
Episode_Reward/track_lin_vel_xy_exp: 0.2677
Episode_Reward/track_ang_vel_z_exp: 0.3058
       Episode_Reward/lin_vel_z_l2: -0.0138
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.1313
         Episode_Reward/dof_acc_l2: -0.0389
     Episode_Reward/action_rate_l2: -0.0207
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0478
Episode_Reward/flat_orientation_l2: -0.0381
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5394432
                    Iteration time: 1.10s
                      Time elapsed: 00:07:51
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 439/1000 [0m                      

                       Computation: 11453 steps/s (collection: 1.028s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -1.3819
                       Mean reward: 5.75
               Mean episode length: 887.01
Episode_Reward/track_lin_vel_xy_exp: 0.2038
Episode_Reward/track_ang_vel_z_exp: 0.2960
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0155
     Episode_Reward/dof_torques_l2: -0.1000
         Episode_Reward/dof_acc_l2: -0.0183
     Episode_Reward/action_rate_l2: -0.0186
      Episode_Reward/feet_air_time: -0.0172
 Episode_Reward/undesired_contacts: -0.0265
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.07s
                      Time elapsed: 00:07:52
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 440/1000 [0m                      

                       Computation: 11436 steps/s (collection: 1.029s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -1.4070
                       Mean reward: 5.96
               Mean episode length: 872.28
Episode_Reward/track_lin_vel_xy_exp: 0.3354
Episode_Reward/track_ang_vel_z_exp: 0.3364
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.1167
         Episode_Reward/dof_acc_l2: -0.0367
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0268
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0188
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5419008
                    Iteration time: 1.07s
                      Time elapsed: 00:07:53
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 441/1000 [0m                      

                       Computation: 10937 steps/s (collection: 1.079s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -1.4152
                       Mean reward: 6.35
               Mean episode length: 902.66
Episode_Reward/track_lin_vel_xy_exp: 0.1381
Episode_Reward/track_ang_vel_z_exp: 0.3414
       Episode_Reward/lin_vel_z_l2: -0.0105
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.1102
         Episode_Reward/dof_acc_l2: -0.0231
     Episode_Reward/action_rate_l2: -0.0201
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0208
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 1.12s
                      Time elapsed: 00:07:54
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 442/1000 [0m                      

                       Computation: 11108 steps/s (collection: 1.057s, learning 0.049s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -1.4109
                       Mean reward: 6.12
               Mean episode length: 910.18
Episode_Reward/track_lin_vel_xy_exp: 0.2142
Episode_Reward/track_ang_vel_z_exp: 0.3232
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.1050
         Episode_Reward/dof_acc_l2: -0.0287
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0240
 Episode_Reward/undesired_contacts: -0.0238
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5443584
                    Iteration time: 1.11s
                      Time elapsed: 00:07:55
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 443/1000 [0m                      

                       Computation: 11184 steps/s (collection: 1.044s, learning 0.055s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -1.4540
                       Mean reward: 6.26
               Mean episode length: 905.25
Episode_Reward/track_lin_vel_xy_exp: 0.2903
Episode_Reward/track_ang_vel_z_exp: 0.3705
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.1128
         Episode_Reward/dof_acc_l2: -0.0352
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0374
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 1.10s
                      Time elapsed: 00:07:56
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 444/1000 [0m                      

                       Computation: 11311 steps/s (collection: 1.043s, learning 0.043s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -1.4918
                       Mean reward: 5.89
               Mean episode length: 897.81
Episode_Reward/track_lin_vel_xy_exp: 0.1279
Episode_Reward/track_ang_vel_z_exp: 0.3604
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.1053
         Episode_Reward/dof_acc_l2: -0.0214
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: -0.0552
Episode_Reward/flat_orientation_l2: -0.0288
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5468160
                    Iteration time: 1.09s
                      Time elapsed: 00:07:57
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 445/1000 [0m                      

                       Computation: 11360 steps/s (collection: 1.033s, learning 0.048s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -1.5104
                       Mean reward: 6.21
               Mean episode length: 889.77
Episode_Reward/track_lin_vel_xy_exp: 0.2652
Episode_Reward/track_ang_vel_z_exp: 0.2673
       Episode_Reward/lin_vel_z_l2: -0.0085
      Episode_Reward/ang_vel_xy_l2: -0.0152
     Episode_Reward/dof_torques_l2: -0.0909
         Episode_Reward/dof_acc_l2: -0.0141
     Episode_Reward/action_rate_l2: -0.0173
      Episode_Reward/feet_air_time: -0.0156
 Episode_Reward/undesired_contacts: -0.0088
Episode_Reward/flat_orientation_l2: -0.0154
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 1.08s
                      Time elapsed: 00:07:58
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 446/1000 [0m                      

                       Computation: 11135 steps/s (collection: 1.058s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -1.5270
                       Mean reward: 6.24
               Mean episode length: 899.22
Episode_Reward/track_lin_vel_xy_exp: 0.2665
Episode_Reward/track_ang_vel_z_exp: 0.2622
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.1081
         Episode_Reward/dof_acc_l2: -0.0378
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0217
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0196
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5492736
                    Iteration time: 1.10s
                      Time elapsed: 00:08:00
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 447/1000 [0m                      

                       Computation: 11404 steps/s (collection: 1.032s, learning 0.045s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -1.5267
                       Mean reward: 6.51
               Mean episode length: 900.44
Episode_Reward/track_lin_vel_xy_exp: 0.3461
Episode_Reward/track_ang_vel_z_exp: 0.3092
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.0244
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0209
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0145
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.08s
                      Time elapsed: 00:08:01
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 448/1000 [0m                      

                       Computation: 10796 steps/s (collection: 1.088s, learning 0.050s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -1.5302
                       Mean reward: 6.59
               Mean episode length: 905.81
Episode_Reward/track_lin_vel_xy_exp: 0.2819
Episode_Reward/track_ang_vel_z_exp: 0.3111
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.1026
         Episode_Reward/dof_acc_l2: -0.0265
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0227
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0198
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5517312
                    Iteration time: 1.14s
                      Time elapsed: 00:08:02
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 449/1000 [0m                      

                       Computation: 11029 steps/s (collection: 1.058s, learning 0.056s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -1.5636
                       Mean reward: 6.31
               Mean episode length: 889.57
Episode_Reward/track_lin_vel_xy_exp: 0.1918
Episode_Reward/track_ang_vel_z_exp: 0.3925
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.1078
         Episode_Reward/dof_acc_l2: -0.0265
     Episode_Reward/action_rate_l2: -0.0193
      Episode_Reward/feet_air_time: -0.0294
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 1.11s
                      Time elapsed: 00:08:03
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 450/1000 [0m                      

                       Computation: 10631 steps/s (collection: 1.093s, learning 0.063s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -1.5928
                       Mean reward: 6.47
               Mean episode length: 898.28
Episode_Reward/track_lin_vel_xy_exp: 0.1386
Episode_Reward/track_ang_vel_z_exp: 0.3107
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.0181
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0174
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0180
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5541888
                    Iteration time: 1.16s
                      Time elapsed: 00:08:04
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 451/1000 [0m                      

                       Computation: 10963 steps/s (collection: 1.076s, learning 0.044s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -1.6008
                       Mean reward: 6.84
               Mean episode length: 907.36
Episode_Reward/track_lin_vel_xy_exp: 0.2601
Episode_Reward/track_ang_vel_z_exp: 0.3668
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0216
     Episode_Reward/dof_torques_l2: -0.1151
         Episode_Reward/dof_acc_l2: -0.0302
     Episode_Reward/action_rate_l2: -0.0207
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 1.12s
                      Time elapsed: 00:08:05
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 452/1000 [0m                      

                       Computation: 11519 steps/s (collection: 1.015s, learning 0.052s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -1.6183
                       Mean reward: 7.01
               Mean episode length: 903.05
Episode_Reward/track_lin_vel_xy_exp: 0.2922
Episode_Reward/track_ang_vel_z_exp: 0.2906
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0167
     Episode_Reward/dof_torques_l2: -0.0966
         Episode_Reward/dof_acc_l2: -0.0248
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0233
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5566464
                    Iteration time: 1.07s
                      Time elapsed: 00:08:06
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 453/1000 [0m                      

                       Computation: 11533 steps/s (collection: 1.021s, learning 0.044s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -1.6257
                       Mean reward: 6.96
               Mean episode length: 901.97
Episode_Reward/track_lin_vel_xy_exp: 0.3757
Episode_Reward/track_ang_vel_z_exp: 0.2268
       Episode_Reward/lin_vel_z_l2: -0.0097
      Episode_Reward/ang_vel_xy_l2: -0.0161
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.0247
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0133
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 1.07s
                      Time elapsed: 00:08:07
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 454/1000 [0m                      

                       Computation: 11369 steps/s (collection: 1.032s, learning 0.049s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0051
                 Mean entropy loss: -1.6232
                       Mean reward: 6.93
               Mean episode length: 883.74
Episode_Reward/track_lin_vel_xy_exp: 0.3075
Episode_Reward/track_ang_vel_z_exp: 0.2692
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0153
     Episode_Reward/dof_torques_l2: -0.0949
         Episode_Reward/dof_acc_l2: -0.0273
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0143
 Episode_Reward/undesired_contacts: -0.0355
Episode_Reward/flat_orientation_l2: -0.0199
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5591040
                    Iteration time: 1.08s
                      Time elapsed: 00:08:08
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 455/1000 [0m                      

                       Computation: 11217 steps/s (collection: 1.051s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -1.6217
                       Mean reward: 6.47
               Mean episode length: 869.56
Episode_Reward/track_lin_vel_xy_exp: 0.2107
Episode_Reward/track_ang_vel_z_exp: 0.1849
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0142
     Episode_Reward/dof_torques_l2: -0.0908
         Episode_Reward/dof_acc_l2: -0.0197
     Episode_Reward/action_rate_l2: -0.0155
      Episode_Reward/feet_air_time: -0.0141
 Episode_Reward/undesired_contacts: -0.0349
Episode_Reward/flat_orientation_l2: -0.0214
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.10s
                      Time elapsed: 00:08:09
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 456/1000 [0m                      

                       Computation: 11236 steps/s (collection: 1.047s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -1.6436
                       Mean reward: 6.38
               Mean episode length: 874.26
Episode_Reward/track_lin_vel_xy_exp: 0.2391
Episode_Reward/track_ang_vel_z_exp: 0.4299
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.0981
         Episode_Reward/dof_acc_l2: -0.0218
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0199
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0082
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5615616
                    Iteration time: 1.09s
                      Time elapsed: 00:08:11
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 457/1000 [0m                      

                       Computation: 11406 steps/s (collection: 1.034s, learning 0.044s)
             Mean action noise std: 0.22
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -1.6408
                       Mean reward: 6.89
               Mean episode length: 874.26
Episode_Reward/track_lin_vel_xy_exp: 0.3800
Episode_Reward/track_ang_vel_z_exp: 0.3621
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0166
     Episode_Reward/dof_torques_l2: -0.1133
         Episode_Reward/dof_acc_l2: -0.0245
     Episode_Reward/action_rate_l2: -0.0202
      Episode_Reward/feet_air_time: -0.0125
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0120
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 1.08s
                      Time elapsed: 00:08:12
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 458/1000 [0m                      

                       Computation: 10855 steps/s (collection: 1.084s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -1.6365
                       Mean reward: 7.16
               Mean episode length: 875.60
Episode_Reward/track_lin_vel_xy_exp: 0.1917
Episode_Reward/track_ang_vel_z_exp: 0.2091
       Episode_Reward/lin_vel_z_l2: -0.0092
      Episode_Reward/ang_vel_xy_l2: -0.0133
     Episode_Reward/dof_torques_l2: -0.0700
         Episode_Reward/dof_acc_l2: -0.0168
     Episode_Reward/action_rate_l2: -0.0136
      Episode_Reward/feet_air_time: -0.0158
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5640192
                    Iteration time: 1.13s
                      Time elapsed: 00:08:13
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 459/1000 [0m                      

                       Computation: 11036 steps/s (collection: 1.053s, learning 0.060s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -1.6458
                       Mean reward: 6.98
               Mean episode length: 875.64
Episode_Reward/track_lin_vel_xy_exp: 0.1123
Episode_Reward/track_ang_vel_z_exp: 0.1927
       Episode_Reward/lin_vel_z_l2: -0.0082
      Episode_Reward/ang_vel_xy_l2: -0.0125
     Episode_Reward/dof_torques_l2: -0.0679
         Episode_Reward/dof_acc_l2: -0.0149
     Episode_Reward/action_rate_l2: -0.0148
      Episode_Reward/feet_air_time: -0.0150
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0124
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 1.11s
                      Time elapsed: 00:08:14
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 460/1000 [0m                      

                       Computation: 10841 steps/s (collection: 1.085s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -1.6392
                       Mean reward: 6.63
               Mean episode length: 854.73
Episode_Reward/track_lin_vel_xy_exp: 0.0902
Episode_Reward/track_ang_vel_z_exp: 0.2479
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0162
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.0243
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0172
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0211
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5664768
                    Iteration time: 1.13s
                      Time elapsed: 00:08:15
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 461/1000 [0m                      

                       Computation: 11101 steps/s (collection: 1.040s, learning 0.067s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -1.6541
                       Mean reward: 6.59
               Mean episode length: 852.01
Episode_Reward/track_lin_vel_xy_exp: 0.1416
Episode_Reward/track_ang_vel_z_exp: 0.3195
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0150
     Episode_Reward/dof_torques_l2: -0.0960
         Episode_Reward/dof_acc_l2: -0.0127
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0102
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0128
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 1.11s
                      Time elapsed: 00:08:16
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 462/1000 [0m                      

                       Computation: 10944 steps/s (collection: 1.060s, learning 0.062s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -1.6463
                       Mean reward: 6.25
               Mean episode length: 834.02
Episode_Reward/track_lin_vel_xy_exp: 0.1328
Episode_Reward/track_ang_vel_z_exp: 0.2550
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.0962
         Episode_Reward/dof_acc_l2: -0.0257
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0187
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0238
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5689344
                    Iteration time: 1.12s
                      Time elapsed: 00:08:17
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 463/1000 [0m                      

                       Computation: 11170 steps/s (collection: 1.055s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -1.6412
                       Mean reward: 6.18
               Mean episode length: 834.02
Episode_Reward/track_lin_vel_xy_exp: 0.1015
Episode_Reward/track_ang_vel_z_exp: 0.2527
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.0163
     Episode_Reward/action_rate_l2: -0.0193
      Episode_Reward/feet_air_time: -0.0186
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.10s
                      Time elapsed: 00:08:18
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 464/1000 [0m                      

                       Computation: 11086 steps/s (collection: 1.062s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0073
                 Mean entropy loss: -1.6509
                       Mean reward: 6.32
               Mean episode length: 852.09
Episode_Reward/track_lin_vel_xy_exp: 0.3508
Episode_Reward/track_ang_vel_z_exp: 0.3929
       Episode_Reward/lin_vel_z_l2: -0.0154
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.1205
         Episode_Reward/dof_acc_l2: -0.0395
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0291
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0268
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5713920
                    Iteration time: 1.11s
                      Time elapsed: 00:08:19
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 465/1000 [0m                      

                       Computation: 11050 steps/s (collection: 1.061s, learning 0.051s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -1.6539
                       Mean reward: 6.60
               Mean episode length: 871.12
Episode_Reward/track_lin_vel_xy_exp: 0.3619
Episode_Reward/track_ang_vel_z_exp: 0.3600
       Episode_Reward/lin_vel_z_l2: -0.0170
      Episode_Reward/ang_vel_xy_l2: -0.0270
     Episode_Reward/dof_torques_l2: -0.1278
         Episode_Reward/dof_acc_l2: -0.0471
     Episode_Reward/action_rate_l2: -0.0217
      Episode_Reward/feet_air_time: -0.0248
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0197
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 1.11s
                      Time elapsed: 00:08:21
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 466/1000 [0m                      

                       Computation: 10577 steps/s (collection: 1.109s, learning 0.053s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -1.6563
                       Mean reward: 6.30
               Mean episode length: 871.12
Episode_Reward/track_lin_vel_xy_exp: 0.2625
Episode_Reward/track_ang_vel_z_exp: 0.2738
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.1180
         Episode_Reward/dof_acc_l2: -0.0326
     Episode_Reward/action_rate_l2: -0.0209
      Episode_Reward/feet_air_time: -0.0259
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0129
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5738496
                    Iteration time: 1.16s
                      Time elapsed: 00:08:22
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 467/1000 [0m                      

                       Computation: 11286 steps/s (collection: 1.040s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -1.6774
                       Mean reward: 6.39
               Mean episode length: 879.68
Episode_Reward/track_lin_vel_xy_exp: 0.2152
Episode_Reward/track_ang_vel_z_exp: 0.2093
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0152
     Episode_Reward/dof_torques_l2: -0.0744
         Episode_Reward/dof_acc_l2: -0.0167
     Episode_Reward/action_rate_l2: -0.0144
      Episode_Reward/feet_air_time: -0.0153
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 1.09s
                      Time elapsed: 00:08:23
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 468/1000 [0m                      

                       Computation: 11016 steps/s (collection: 1.065s, learning 0.051s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -1.7139
                       Mean reward: 6.83
               Mean episode length: 893.12
Episode_Reward/track_lin_vel_xy_exp: 0.3147
Episode_Reward/track_ang_vel_z_exp: 0.3013
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0155
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.0201
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0221
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0126
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5763072
                    Iteration time: 1.12s
                      Time elapsed: 00:08:24
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 469/1000 [0m                      

                       Computation: 11145 steps/s (collection: 1.057s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -1.7352
                       Mean reward: 7.19
               Mean episode length: 907.49
Episode_Reward/track_lin_vel_xy_exp: 0.2950
Episode_Reward/track_ang_vel_z_exp: 0.2898
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.1045
         Episode_Reward/dof_acc_l2: -0.0340
     Episode_Reward/action_rate_l2: -0.0197
      Episode_Reward/feet_air_time: -0.0240
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0187
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 1.10s
                      Time elapsed: 00:08:25
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 470/1000 [0m                      

                       Computation: 10645 steps/s (collection: 1.101s, learning 0.053s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -1.7553
                       Mean reward: 6.88
               Mean episode length: 903.89
Episode_Reward/track_lin_vel_xy_exp: 0.2009
Episode_Reward/track_ang_vel_z_exp: 0.1588
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0133
     Episode_Reward/dof_torques_l2: -0.0665
         Episode_Reward/dof_acc_l2: -0.0256
     Episode_Reward/action_rate_l2: -0.0125
      Episode_Reward/feet_air_time: -0.0164
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0252
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5787648
                    Iteration time: 1.15s
                      Time elapsed: 00:08:26
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 471/1000 [0m                      

                       Computation: 11432 steps/s (collection: 1.020s, learning 0.055s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -1.7736
                       Mean reward: 6.72
               Mean episode length: 903.40
Episode_Reward/track_lin_vel_xy_exp: 0.0982
Episode_Reward/track_ang_vel_z_exp: 0.2835
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0153
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.0160
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0136
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0144
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.07s
                      Time elapsed: 00:08:27
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 472/1000 [0m                      

                       Computation: 10890 steps/s (collection: 1.077s, learning 0.051s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -1.7934
                       Mean reward: 6.96
               Mean episode length: 912.45
Episode_Reward/track_lin_vel_xy_exp: 0.4255
Episode_Reward/track_ang_vel_z_exp: 0.2466
       Episode_Reward/lin_vel_z_l2: -0.0102
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0973
         Episode_Reward/dof_acc_l2: -0.0231
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0119
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5812224
                    Iteration time: 1.13s
                      Time elapsed: 00:08:28
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 473/1000 [0m                      

                       Computation: 10539 steps/s (collection: 1.116s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -1.8317
                       Mean reward: 6.84
               Mean episode length: 917.53
Episode_Reward/track_lin_vel_xy_exp: 0.2409
Episode_Reward/track_ang_vel_z_exp: 0.2672
       Episode_Reward/lin_vel_z_l2: -0.0081
      Episode_Reward/ang_vel_xy_l2: -0.0142
     Episode_Reward/dof_torques_l2: -0.0911
         Episode_Reward/dof_acc_l2: -0.0126
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0122
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 1.17s
                      Time elapsed: 00:08:30
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 474/1000 [0m                      

                       Computation: 10966 steps/s (collection: 1.071s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0040
                 Mean entropy loss: -1.8546
                       Mean reward: 6.87
               Mean episode length: 926.94
Episode_Reward/track_lin_vel_xy_exp: 0.3561
Episode_Reward/track_ang_vel_z_exp: 0.3990
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.1176
         Episode_Reward/dof_acc_l2: -0.0265
     Episode_Reward/action_rate_l2: -0.0201
      Episode_Reward/feet_air_time: -0.0217
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0125
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5836800
                    Iteration time: 1.12s
                      Time elapsed: 00:08:31
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 475/1000 [0m                      

                       Computation: 10770 steps/s (collection: 1.096s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -1.8518
                       Mean reward: 6.78
               Mean episode length: 921.30
Episode_Reward/track_lin_vel_xy_exp: 0.1815
Episode_Reward/track_ang_vel_z_exp: 0.2878
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.0188
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0216
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0137
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 1.14s
                      Time elapsed: 00:08:32
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 476/1000 [0m                      

                       Computation: 10979 steps/s (collection: 1.071s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -1.8565
                       Mean reward: 7.94
               Mean episode length: 952.57
Episode_Reward/track_lin_vel_xy_exp: 0.4916
Episode_Reward/track_ang_vel_z_exp: 0.3615
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.1043
         Episode_Reward/dof_acc_l2: -0.0267
     Episode_Reward/action_rate_l2: -0.0203
      Episode_Reward/feet_air_time: -0.0261
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0113
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5861376
                    Iteration time: 1.12s
                      Time elapsed: 00:08:33
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 477/1000 [0m                      

                       Computation: 11346 steps/s (collection: 1.030s, learning 0.053s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -1.8801
                       Mean reward: 8.18
               Mean episode length: 952.57
Episode_Reward/track_lin_vel_xy_exp: 0.3078
Episode_Reward/track_ang_vel_z_exp: 0.2898
       Episode_Reward/lin_vel_z_l2: -0.0102
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.1153
         Episode_Reward/dof_acc_l2: -0.0246
     Episode_Reward/action_rate_l2: -0.0201
      Episode_Reward/feet_air_time: -0.0170
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0102
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 1.08s
                      Time elapsed: 00:08:34
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 478/1000 [0m                      

                       Computation: 11006 steps/s (collection: 1.061s, learning 0.055s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -1.8947
                       Mean reward: 7.86
               Mean episode length: 953.79
Episode_Reward/track_lin_vel_xy_exp: 0.1291
Episode_Reward/track_ang_vel_z_exp: 0.2986
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0136
     Episode_Reward/dof_torques_l2: -0.0799
         Episode_Reward/dof_acc_l2: -0.0186
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0204
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0110
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5885952
                    Iteration time: 1.12s
                      Time elapsed: 00:08:35
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 479/1000 [0m                      

                       Computation: 11157 steps/s (collection: 1.052s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -1.9029
                       Mean reward: 7.68
               Mean episode length: 935.66
Episode_Reward/track_lin_vel_xy_exp: 0.2787
Episode_Reward/track_ang_vel_z_exp: 0.3320
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0162
     Episode_Reward/dof_torques_l2: -0.0879
         Episode_Reward/dof_acc_l2: -0.0162
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0182
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0082
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.10s
                      Time elapsed: 00:08:36
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 480/1000 [0m                      

                       Computation: 11139 steps/s (collection: 1.058s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -1.8871
                       Mean reward: 8.39
               Mean episode length: 948.90
Episode_Reward/track_lin_vel_xy_exp: 0.4267
Episode_Reward/track_ang_vel_z_exp: 0.3154
       Episode_Reward/lin_vel_z_l2: -0.0122
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.1082
         Episode_Reward/dof_acc_l2: -0.0261
     Episode_Reward/action_rate_l2: -0.0203
      Episode_Reward/feet_air_time: -0.0212
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0137
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5910528
                    Iteration time: 1.10s
                      Time elapsed: 00:08:37
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 481/1000 [0m                      

                       Computation: 11063 steps/s (collection: 1.062s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -1.9194
                       Mean reward: 8.29
               Mean episode length: 924.73
Episode_Reward/track_lin_vel_xy_exp: 0.3309
Episode_Reward/track_ang_vel_z_exp: 0.3284
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0142
     Episode_Reward/dof_torques_l2: -0.0839
         Episode_Reward/dof_acc_l2: -0.0146
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0114
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: -0.0139
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 1.11s
                      Time elapsed: 00:08:38
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 482/1000 [0m                      

                       Computation: 11158 steps/s (collection: 1.051s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -1.9193
                       Mean reward: 8.10
               Mean episode length: 936.90
Episode_Reward/track_lin_vel_xy_exp: 0.0979
Episode_Reward/track_ang_vel_z_exp: 0.3077
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0155
     Episode_Reward/dof_torques_l2: -0.0962
         Episode_Reward/dof_acc_l2: -0.0145
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0132
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5935104
                    Iteration time: 1.10s
                      Time elapsed: 00:08:40
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 483/1000 [0m                      

                       Computation: 11453 steps/s (collection: 1.026s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -1.9409
                       Mean reward: 8.23
               Mean episode length: 933.57
Episode_Reward/track_lin_vel_xy_exp: 0.3641
Episode_Reward/track_ang_vel_z_exp: 0.3491
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.1028
         Episode_Reward/dof_acc_l2: -0.0232
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0219
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 1.07s
                      Time elapsed: 00:08:41
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 484/1000 [0m                      

                       Computation: 10538 steps/s (collection: 1.122s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0073
                 Mean entropy loss: -1.9610
                       Mean reward: 7.33
               Mean episode length: 924.77
Episode_Reward/track_lin_vel_xy_exp: 0.2614
Episode_Reward/track_ang_vel_z_exp: 0.2142
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0155
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.0176
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0190
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0168
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5959680
                    Iteration time: 1.17s
                      Time elapsed: 00:08:42
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 485/1000 [0m                      

                       Computation: 11507 steps/s (collection: 1.023s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -1.9701
                       Mean reward: 8.16
               Mean episode length: 935.72
Episode_Reward/track_lin_vel_xy_exp: 0.3417
Episode_Reward/track_ang_vel_z_exp: 0.4005
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.1100
         Episode_Reward/dof_acc_l2: -0.0342
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0295
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0131
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 1.07s
                      Time elapsed: 00:08:43
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 486/1000 [0m                      

                       Computation: 11273 steps/s (collection: 1.041s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -1.9684
                       Mean reward: 8.15
               Mean episode length: 937.98
Episode_Reward/track_lin_vel_xy_exp: 0.2033
Episode_Reward/track_ang_vel_z_exp: 0.3051
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0151
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.0149
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0158
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: -0.0171
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5984256
                    Iteration time: 1.09s
                      Time elapsed: 00:08:44
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 487/1000 [0m                      

                       Computation: 11324 steps/s (collection: 1.032s, learning 0.053s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -1.9534
                       Mean reward: 7.56
               Mean episode length: 925.68
Episode_Reward/track_lin_vel_xy_exp: 0.4089
Episode_Reward/track_ang_vel_z_exp: 0.3352
       Episode_Reward/lin_vel_z_l2: -0.0167
      Episode_Reward/ang_vel_xy_l2: -0.0247
     Episode_Reward/dof_torques_l2: -0.1301
         Episode_Reward/dof_acc_l2: -0.0390
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0237
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0198
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.09s
                      Time elapsed: 00:08:45
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 488/1000 [0m                      

                       Computation: 10722 steps/s (collection: 1.088s, learning 0.058s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -1.9551
                       Mean reward: 7.74
               Mean episode length: 928.36
Episode_Reward/track_lin_vel_xy_exp: 0.2795
Episode_Reward/track_ang_vel_z_exp: 0.3102
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.1046
         Episode_Reward/dof_acc_l2: -0.0264
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0188
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6008832
                    Iteration time: 1.15s
                      Time elapsed: 00:08:46
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 489/1000 [0m                      

                       Computation: 11116 steps/s (collection: 1.057s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -1.9535
                       Mean reward: 7.59
               Mean episode length: 941.07
Episode_Reward/track_lin_vel_xy_exp: 0.4771
Episode_Reward/track_ang_vel_z_exp: 0.3944
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.1030
         Episode_Reward/dof_acc_l2: -0.0232
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0251
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0120
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 1.11s
                      Time elapsed: 00:08:47
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 490/1000 [0m                      

                       Computation: 11072 steps/s (collection: 1.023s, learning 0.087s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -1.9498
                       Mean reward: 7.68
               Mean episode length: 925.65
Episode_Reward/track_lin_vel_xy_exp: 0.2454
Episode_Reward/track_ang_vel_z_exp: 0.3336
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.0962
         Episode_Reward/dof_acc_l2: -0.0226
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0199
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0236
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6033408
                    Iteration time: 1.11s
                      Time elapsed: 00:08:48
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 491/1000 [0m                      

                       Computation: 11055 steps/s (collection: 1.065s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -1.9438
                       Mean reward: 7.19
               Mean episode length: 908.30
Episode_Reward/track_lin_vel_xy_exp: 0.3822
Episode_Reward/track_ang_vel_z_exp: 0.3356
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0152
     Episode_Reward/dof_torques_l2: -0.1008
         Episode_Reward/dof_acc_l2: -0.0163
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.0183
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 1.11s
                      Time elapsed: 00:08:50
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 492/1000 [0m                      

                       Computation: 11677 steps/s (collection: 1.007s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -1.9560
                       Mean reward: 7.13
               Mean episode length: 905.33
Episode_Reward/track_lin_vel_xy_exp: 0.1402
Episode_Reward/track_ang_vel_z_exp: 0.3879
       Episode_Reward/lin_vel_z_l2: -0.0092
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0832
         Episode_Reward/dof_acc_l2: -0.0138
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0165
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0190
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6057984
                    Iteration time: 1.05s
                      Time elapsed: 00:08:51
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 493/1000 [0m                      

                       Computation: 11398 steps/s (collection: 1.033s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -1.9624
                       Mean reward: 7.31
               Mean episode length: 916.57
Episode_Reward/track_lin_vel_xy_exp: 0.3380
Episode_Reward/track_ang_vel_z_exp: 0.3468
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0182
     Episode_Reward/dof_torques_l2: -0.1091
         Episode_Reward/dof_acc_l2: -0.0259
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0264
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 1.08s
                      Time elapsed: 00:08:52
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 494/1000 [0m                      

                       Computation: 10655 steps/s (collection: 1.103s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0110
                 Mean entropy loss: -1.9913
                       Mean reward: 7.20
               Mean episode length: 897.64
Episode_Reward/track_lin_vel_xy_exp: 0.2410
Episode_Reward/track_ang_vel_z_exp: 0.3635
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.0226
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0255
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6082560
                    Iteration time: 1.15s
                      Time elapsed: 00:08:53
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 495/1000 [0m                      

                       Computation: 11120 steps/s (collection: 1.060s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.0177
                       Mean reward: 7.32
               Mean episode length: 885.24
Episode_Reward/track_lin_vel_xy_exp: 0.2668
Episode_Reward/track_ang_vel_z_exp: 0.2655
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.0203
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0188
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0213
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.10s
                      Time elapsed: 00:08:54
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 496/1000 [0m                      

                       Computation: 11361 steps/s (collection: 1.037s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -2.0328
                       Mean reward: 7.19
               Mean episode length: 868.81
Episode_Reward/track_lin_vel_xy_exp: 0.4086
Episode_Reward/track_ang_vel_z_exp: 0.2945
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0144
     Episode_Reward/dof_torques_l2: -0.0999
         Episode_Reward/dof_acc_l2: -0.0219
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: -0.0131
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6107136
                    Iteration time: 1.08s
                      Time elapsed: 00:08:55
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 497/1000 [0m                      

                       Computation: 11381 steps/s (collection: 1.030s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.0303
                       Mean reward: 7.08
               Mean episode length: 854.38
Episode_Reward/track_lin_vel_xy_exp: 0.1922
Episode_Reward/track_ang_vel_z_exp: 0.2534
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0920
         Episode_Reward/dof_acc_l2: -0.0196
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0178
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0178
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 1.08s
                      Time elapsed: 00:08:56
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 498/1000 [0m                      

                       Computation: 11153 steps/s (collection: 1.054s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.0331
                       Mean reward: 7.40
               Mean episode length: 858.04
Episode_Reward/track_lin_vel_xy_exp: 0.4536
Episode_Reward/track_ang_vel_z_exp: 0.3285
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.0974
         Episode_Reward/dof_acc_l2: -0.0319
     Episode_Reward/action_rate_l2: -0.0179
      Episode_Reward/feet_air_time: -0.0231
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0141
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6131712
                    Iteration time: 1.10s
                      Time elapsed: 00:08:57
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 499/1000 [0m                      

                       Computation: 11089 steps/s (collection: 1.051s, learning 0.057s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -2.0379
                       Mean reward: 7.66
               Mean episode length: 860.05
Episode_Reward/track_lin_vel_xy_exp: 0.3104
Episode_Reward/track_ang_vel_z_exp: 0.3227
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.0185
     Episode_Reward/action_rate_l2: -0.0155
      Episode_Reward/feet_air_time: -0.0136
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 1.11s
                      Time elapsed: 00:08:58
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 500/1000 [0m                      

                       Computation: 11427 steps/s (collection: 1.031s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.0431
                       Mean reward: 8.19
               Mean episode length: 863.24
Episode_Reward/track_lin_vel_xy_exp: 0.2703
Episode_Reward/track_ang_vel_z_exp: 0.3018
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0144
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.0157
     Episode_Reward/action_rate_l2: -0.0164
      Episode_Reward/feet_air_time: -0.0170
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0229
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6156288
                    Iteration time: 1.08s
                      Time elapsed: 00:08:59
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 501/1000 [0m                      

                       Computation: 10906 steps/s (collection: 1.081s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.0387
                       Mean reward: 7.63
               Mean episode length: 841.53
Episode_Reward/track_lin_vel_xy_exp: 0.0807
Episode_Reward/track_ang_vel_z_exp: 0.2173
       Episode_Reward/lin_vel_z_l2: -0.0094
      Episode_Reward/ang_vel_xy_l2: -0.0152
     Episode_Reward/dof_torques_l2: -0.0783
         Episode_Reward/dof_acc_l2: -0.0183
     Episode_Reward/action_rate_l2: -0.0143
      Episode_Reward/feet_air_time: -0.0159
 Episode_Reward/undesired_contacts: -0.0392
Episode_Reward/flat_orientation_l2: -0.0241
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 1.13s
                      Time elapsed: 00:09:00
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 502/1000 [0m                      

                       Computation: 11260 steps/s (collection: 1.036s, learning 0.056s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.0530
                       Mean reward: 6.95
               Mean episode length: 810.08
Episode_Reward/track_lin_vel_xy_exp: 0.1800
Episode_Reward/track_ang_vel_z_exp: 0.2419
       Episode_Reward/lin_vel_z_l2: -0.0105
      Episode_Reward/ang_vel_xy_l2: -0.0136
     Episode_Reward/dof_torques_l2: -0.0761
         Episode_Reward/dof_acc_l2: -0.0184
     Episode_Reward/action_rate_l2: -0.0130
      Episode_Reward/feet_air_time: -0.0174
 Episode_Reward/undesired_contacts: -0.0331
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6180864
                    Iteration time: 1.09s
                      Time elapsed: 00:09:02
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 503/1000 [0m                      

                       Computation: 11096 steps/s (collection: 1.061s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -2.0730
                       Mean reward: 6.50
               Mean episode length: 827.62
Episode_Reward/track_lin_vel_xy_exp: 0.0997
Episode_Reward/track_ang_vel_z_exp: 0.1524
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0158
     Episode_Reward/dof_torques_l2: -0.0816
         Episode_Reward/dof_acc_l2: -0.0159
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0155
 Episode_Reward/undesired_contacts: -0.0229
Episode_Reward/flat_orientation_l2: -0.0167
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.11s
                      Time elapsed: 00:09:03
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 504/1000 [0m                      

                       Computation: 10529 steps/s (collection: 1.107s, learning 0.060s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -2.0826
                       Mean reward: 6.74
               Mean episode length: 837.95
Episode_Reward/track_lin_vel_xy_exp: 0.2339
Episode_Reward/track_ang_vel_z_exp: 0.2618
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0874
         Episode_Reward/dof_acc_l2: -0.0234
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0187
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0204
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6205440
                    Iteration time: 1.17s
                      Time elapsed: 00:09:04
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 505/1000 [0m                      

                       Computation: 10824 steps/s (collection: 1.090s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.0716
                       Mean reward: 6.37
               Mean episode length: 838.60
Episode_Reward/track_lin_vel_xy_exp: 0.1862
Episode_Reward/track_ang_vel_z_exp: 0.3088
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.0958
         Episode_Reward/dof_acc_l2: -0.0284
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0172
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 1.14s
                      Time elapsed: 00:09:05
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 506/1000 [0m                      

                       Computation: 11594 steps/s (collection: 1.014s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.0535
                       Mean reward: 6.03
               Mean episode length: 855.48
Episode_Reward/track_lin_vel_xy_exp: 0.3284
Episode_Reward/track_ang_vel_z_exp: 0.2816
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.1056
         Episode_Reward/dof_acc_l2: -0.0217
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0161
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: -0.0180
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6230016
                    Iteration time: 1.06s
                      Time elapsed: 00:09:06
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 507/1000 [0m                      

                       Computation: 11751 steps/s (collection: 1.000s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.0307
                       Mean reward: 5.91
               Mean episode length: 859.92
Episode_Reward/track_lin_vel_xy_exp: 0.2636
Episode_Reward/track_ang_vel_z_exp: 0.3201
       Episode_Reward/lin_vel_z_l2: -0.0173
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1299
         Episode_Reward/dof_acc_l2: -0.0479
     Episode_Reward/action_rate_l2: -0.0204
      Episode_Reward/feet_air_time: -0.0269
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0254
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 1.05s
                      Time elapsed: 00:09:07
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 508/1000 [0m                      

                       Computation: 11430 steps/s (collection: 1.026s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -2.0366
                       Mean reward: 5.62
               Mean episode length: 869.68
Episode_Reward/track_lin_vel_xy_exp: 0.2743
Episode_Reward/track_ang_vel_z_exp: 0.3744
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0221
     Episode_Reward/dof_torques_l2: -0.1344
         Episode_Reward/dof_acc_l2: -0.0498
     Episode_Reward/action_rate_l2: -0.0214
      Episode_Reward/feet_air_time: -0.0365
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0199
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6254592
                    Iteration time: 1.08s
                      Time elapsed: 00:09:08
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 509/1000 [0m                      

                       Computation: 11189 steps/s (collection: 1.053s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.0472
                       Mean reward: 5.47
               Mean episode length: 858.40
Episode_Reward/track_lin_vel_xy_exp: 0.2186
Episode_Reward/track_ang_vel_z_exp: 0.3389
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0978
         Episode_Reward/dof_acc_l2: -0.0281
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0226
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0211
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 1.10s
                      Time elapsed: 00:09:09
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 510/1000 [0m                      

                       Computation: 10706 steps/s (collection: 1.089s, learning 0.058s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.0737
                       Mean reward: 5.83
               Mean episode length: 871.05
Episode_Reward/track_lin_vel_xy_exp: 0.3307
Episode_Reward/track_ang_vel_z_exp: 0.3202
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.0950
         Episode_Reward/dof_acc_l2: -0.0217
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0180
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6279168
                    Iteration time: 1.15s
                      Time elapsed: 00:09:10
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 511/1000 [0m                      

                       Computation: 10984 steps/s (collection: 1.066s, learning 0.052s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -2.0583
                       Mean reward: 5.49
               Mean episode length: 902.50
Episode_Reward/track_lin_vel_xy_exp: 0.1057
Episode_Reward/track_ang_vel_z_exp: 0.3248
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0149
     Episode_Reward/dof_torques_l2: -0.1167
         Episode_Reward/dof_acc_l2: -0.0193
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.1964
Episode_Reward/flat_orientation_l2: -0.0207
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.12s
                      Time elapsed: 00:09:12
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 512/1000 [0m                      

                       Computation: 11227 steps/s (collection: 1.042s, learning 0.053s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -2.0421
                       Mean reward: 5.45
               Mean episode length: 909.33
Episode_Reward/track_lin_vel_xy_exp: 0.4109
Episode_Reward/track_ang_vel_z_exp: 0.2735
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0176
     Episode_Reward/dof_torques_l2: -0.1112
         Episode_Reward/dof_acc_l2: -0.0247
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0215
 Episode_Reward/undesired_contacts: -0.1132
Episode_Reward/flat_orientation_l2: -0.0185
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6303744
                    Iteration time: 1.09s
                      Time elapsed: 00:09:13
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 513/1000 [0m                      

                       Computation: 10908 steps/s (collection: 1.080s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -2.0506
                       Mean reward: 6.63
               Mean episode length: 941.53
Episode_Reward/track_lin_vel_xy_exp: 0.4166
Episode_Reward/track_ang_vel_z_exp: 0.3714
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.1023
         Episode_Reward/dof_acc_l2: -0.0255
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0199
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0136
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 1.13s
                      Time elapsed: 00:09:14
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 514/1000 [0m                      

                       Computation: 10841 steps/s (collection: 1.084s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -2.0489
                       Mean reward: 7.15
               Mean episode length: 947.67
Episode_Reward/track_lin_vel_xy_exp: 0.2624
Episode_Reward/track_ang_vel_z_exp: 0.3158
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0159
     Episode_Reward/dof_torques_l2: -0.1183
         Episode_Reward/dof_acc_l2: -0.0220
     Episode_Reward/action_rate_l2: -0.0186
      Episode_Reward/feet_air_time: -0.0148
 Episode_Reward/undesired_contacts: -0.0529
Episode_Reward/flat_orientation_l2: -0.0199
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6328320
                    Iteration time: 1.13s
                      Time elapsed: 00:09:15
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 515/1000 [0m                      

                       Computation: 11785 steps/s (collection: 0.997s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.0567
                       Mean reward: 7.35
               Mean episode length: 973.73
Episode_Reward/track_lin_vel_xy_exp: 0.2982
Episode_Reward/track_ang_vel_z_exp: 0.2257
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0172
     Episode_Reward/dof_torques_l2: -0.0965
         Episode_Reward/dof_acc_l2: -0.0243
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0199
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0215
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 1.04s
                      Time elapsed: 00:09:16
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 516/1000 [0m                      

                       Computation: 11624 steps/s (collection: 0.999s, learning 0.058s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -2.0714
                       Mean reward: 7.21
               Mean episode length: 964.44
Episode_Reward/track_lin_vel_xy_exp: 0.2688
Episode_Reward/track_ang_vel_z_exp: 0.3073
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.1000
         Episode_Reward/dof_acc_l2: -0.0192
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0184
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0134
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6352896
                    Iteration time: 1.06s
                      Time elapsed: 00:09:17
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 517/1000 [0m                      

                       Computation: 11826 steps/s (collection: 0.996s, learning 0.043s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -2.0936
                       Mean reward: 8.19
               Mean episode length: 959.80
Episode_Reward/track_lin_vel_xy_exp: 0.3391
Episode_Reward/track_ang_vel_z_exp: 0.2935
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.1057
         Episode_Reward/dof_acc_l2: -0.0346
     Episode_Reward/action_rate_l2: -0.0197
      Episode_Reward/feet_air_time: -0.0259
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 1.04s
                      Time elapsed: 00:09:18
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 518/1000 [0m                      

                       Computation: 11208 steps/s (collection: 1.049s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.1012
                       Mean reward: 8.17
               Mean episode length: 955.03
Episode_Reward/track_lin_vel_xy_exp: 0.4582
Episode_Reward/track_ang_vel_z_exp: 0.3124
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.1245
         Episode_Reward/dof_acc_l2: -0.0335
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0266
 Episode_Reward/undesired_contacts: -0.0240
Episode_Reward/flat_orientation_l2: -0.0137
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6377472
                    Iteration time: 1.10s
                      Time elapsed: 00:09:19
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 519/1000 [0m                      

                       Computation: 11510 steps/s (collection: 1.023s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -2.1263
                       Mean reward: 7.61
               Mean episode length: 945.20
Episode_Reward/track_lin_vel_xy_exp: 0.3336
Episode_Reward/track_ang_vel_z_exp: 0.2776
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1264
         Episode_Reward/dof_acc_l2: -0.0358
     Episode_Reward/action_rate_l2: -0.0201
      Episode_Reward/feet_air_time: -0.0250
 Episode_Reward/undesired_contacts: -0.0424
Episode_Reward/flat_orientation_l2: -0.0137
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.07s
                      Time elapsed: 00:09:20
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 520/1000 [0m                      

                       Computation: 11754 steps/s (collection: 0.995s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.1465
                       Mean reward: 7.53
               Mean episode length: 945.20
Episode_Reward/track_lin_vel_xy_exp: 0.3566
Episode_Reward/track_ang_vel_z_exp: 0.2476
       Episode_Reward/lin_vel_z_l2: -0.0085
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.0949
         Episode_Reward/dof_acc_l2: -0.0181
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0129
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0097
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6402048
                    Iteration time: 1.05s
                      Time elapsed: 00:09:21
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 521/1000 [0m                      

                       Computation: 11091 steps/s (collection: 1.057s, learning 0.051s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -2.1503
                       Mean reward: 7.50
               Mean episode length: 939.49
Episode_Reward/track_lin_vel_xy_exp: 0.2842
Episode_Reward/track_ang_vel_z_exp: 0.3623
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.1114
         Episode_Reward/dof_acc_l2: -0.0304
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 1.11s
                      Time elapsed: 00:09:22
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 522/1000 [0m                      

                       Computation: 10637 steps/s (collection: 1.110s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.1315
                       Mean reward: 7.43
               Mean episode length: 920.65
Episode_Reward/track_lin_vel_xy_exp: 0.3362
Episode_Reward/track_ang_vel_z_exp: 0.3005
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0155
     Episode_Reward/dof_torques_l2: -0.0922
         Episode_Reward/dof_acc_l2: -0.0254
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0202
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6426624
                    Iteration time: 1.16s
                      Time elapsed: 00:09:23
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 523/1000 [0m                      

                       Computation: 10906 steps/s (collection: 1.079s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0044
                 Mean entropy loss: -2.1004
                       Mean reward: 7.18
               Mean episode length: 897.50
Episode_Reward/track_lin_vel_xy_exp: 0.2601
Episode_Reward/track_ang_vel_z_exp: 0.2252
       Episode_Reward/lin_vel_z_l2: -0.0068
      Episode_Reward/ang_vel_xy_l2: -0.0126
     Episode_Reward/dof_torques_l2: -0.0712
         Episode_Reward/dof_acc_l2: -0.0083
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0077
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0143
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 1.13s
                      Time elapsed: 00:09:25
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 524/1000 [0m                      

                       Computation: 11050 steps/s (collection: 1.065s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -2.0971
                       Mean reward: 7.36
               Mean episode length: 897.84
Episode_Reward/track_lin_vel_xy_exp: 0.2792
Episode_Reward/track_ang_vel_z_exp: 0.3230
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0169
     Episode_Reward/dof_torques_l2: -0.0956
         Episode_Reward/dof_acc_l2: -0.0289
     Episode_Reward/action_rate_l2: -0.0170
      Episode_Reward/feet_air_time: -0.0220
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0214
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6451200
                    Iteration time: 1.11s
                      Time elapsed: 00:09:26
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 525/1000 [0m                      

                       Computation: 11211 steps/s (collection: 1.051s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -2.0800
                       Mean reward: 7.18
               Mean episode length: 874.98
Episode_Reward/track_lin_vel_xy_exp: 0.2624
Episode_Reward/track_ang_vel_z_exp: 0.2825
       Episode_Reward/lin_vel_z_l2: -0.0079
      Episode_Reward/ang_vel_xy_l2: -0.0139
     Episode_Reward/dof_torques_l2: -0.0749
         Episode_Reward/dof_acc_l2: -0.0147
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0157
 Episode_Reward/undesired_contacts: -0.0100
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 1.10s
                      Time elapsed: 00:09:27
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 526/1000 [0m                      

                       Computation: 11410 steps/s (collection: 1.030s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -2.0684
                       Mean reward: 7.58
               Mean episode length: 871.92
Episode_Reward/track_lin_vel_xy_exp: 0.2380
Episode_Reward/track_ang_vel_z_exp: 0.2216
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.0766
         Episode_Reward/dof_acc_l2: -0.0214
     Episode_Reward/action_rate_l2: -0.0149
      Episode_Reward/feet_air_time: -0.0148
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0251
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6475776
                    Iteration time: 1.08s
                      Time elapsed: 00:09:28
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 527/1000 [0m                      

                       Computation: 11088 steps/s (collection: 1.062s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -2.0823
                       Mean reward: 7.52
               Mean episode length: 862.72
Episode_Reward/track_lin_vel_xy_exp: 0.3739
Episode_Reward/track_ang_vel_z_exp: 0.3287
       Episode_Reward/lin_vel_z_l2: -0.0137
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.1123
         Episode_Reward/dof_acc_l2: -0.0375
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0269
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0140
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.11s
                      Time elapsed: 00:09:29
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 528/1000 [0m                      

                       Computation: 11234 steps/s (collection: 1.049s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -2.0861
                       Mean reward: 8.03
               Mean episode length: 875.21
Episode_Reward/track_lin_vel_xy_exp: 0.2842
Episode_Reward/track_ang_vel_z_exp: 0.2487
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.0843
         Episode_Reward/dof_acc_l2: -0.0217
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0196
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0212
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6500352
                    Iteration time: 1.09s
                      Time elapsed: 00:09:30
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 529/1000 [0m                      

                       Computation: 10976 steps/s (collection: 1.070s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0054
                 Mean entropy loss: -2.0949
                       Mean reward: 7.92
               Mean episode length: 881.15
Episode_Reward/track_lin_vel_xy_exp: 0.3581
Episode_Reward/track_ang_vel_z_exp: 0.3022
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.1076
         Episode_Reward/dof_acc_l2: -0.0327
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0225
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0206
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 1.12s
                      Time elapsed: 00:09:31
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 530/1000 [0m                      

                       Computation: 11882 steps/s (collection: 0.986s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -2.1023
                       Mean reward: 7.69
               Mean episode length: 888.88
Episode_Reward/track_lin_vel_xy_exp: 0.2602
Episode_Reward/track_ang_vel_z_exp: 0.3400
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.1058
         Episode_Reward/dof_acc_l2: -0.0429
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0301
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6524928
                    Iteration time: 1.03s
                      Time elapsed: 00:09:32
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 531/1000 [0m                      

                       Computation: 11431 steps/s (collection: 1.027s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -2.1037
                       Mean reward: 7.97
               Mean episode length: 904.89
Episode_Reward/track_lin_vel_xy_exp: 0.3690
Episode_Reward/track_ang_vel_z_exp: 0.3604
       Episode_Reward/lin_vel_z_l2: -0.0175
      Episode_Reward/ang_vel_xy_l2: -0.0317
     Episode_Reward/dof_torques_l2: -0.1193
         Episode_Reward/dof_acc_l2: -0.0507
     Episode_Reward/action_rate_l2: -0.0207
      Episode_Reward/feet_air_time: -0.0277
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0212
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 1.07s
                      Time elapsed: 00:09:33
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 532/1000 [0m                      

                       Computation: 10932 steps/s (collection: 1.075s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.1161
                       Mean reward: 8.12
               Mean episode length: 920.55
Episode_Reward/track_lin_vel_xy_exp: 0.3382
Episode_Reward/track_ang_vel_z_exp: 0.2408
       Episode_Reward/lin_vel_z_l2: -0.0086
      Episode_Reward/ang_vel_xy_l2: -0.0159
     Episode_Reward/dof_torques_l2: -0.0903
         Episode_Reward/dof_acc_l2: -0.0169
     Episode_Reward/action_rate_l2: -0.0173
      Episode_Reward/feet_air_time: -0.0112
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: -0.0119
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6549504
                    Iteration time: 1.12s
                      Time elapsed: 00:09:34
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 533/1000 [0m                      

                       Computation: 11448 steps/s (collection: 1.026s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -2.1027
                       Mean reward: 7.31
               Mean episode length: 900.37
Episode_Reward/track_lin_vel_xy_exp: 0.2736
Episode_Reward/track_ang_vel_z_exp: 0.2663
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0225
     Episode_Reward/dof_torques_l2: -0.0962
         Episode_Reward/dof_acc_l2: -0.0297
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.0145
Episode_Reward/flat_orientation_l2: -0.0342
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 1.07s
                      Time elapsed: 00:09:36
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 534/1000 [0m                      

                       Computation: 11061 steps/s (collection: 1.066s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.1018
                       Mean reward: 7.08
               Mean episode length: 874.67
Episode_Reward/track_lin_vel_xy_exp: 0.3175
Episode_Reward/track_ang_vel_z_exp: 0.3048
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0206
     Episode_Reward/dof_torques_l2: -0.0953
         Episode_Reward/dof_acc_l2: -0.0353
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0237
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0221
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6574080
                    Iteration time: 1.11s
                      Time elapsed: 00:09:37
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 535/1000 [0m                      

                       Computation: 11598 steps/s (collection: 1.013s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.0998
                       Mean reward: 7.04
               Mean episode length: 884.81
Episode_Reward/track_lin_vel_xy_exp: 0.1944
Episode_Reward/track_ang_vel_z_exp: 0.2540
       Episode_Reward/lin_vel_z_l2: -0.0137
      Episode_Reward/ang_vel_xy_l2: -0.0266
     Episode_Reward/dof_torques_l2: -0.0950
         Episode_Reward/dof_acc_l2: -0.0285
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0225
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0381
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.06s
                      Time elapsed: 00:09:38
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 536/1000 [0m                      

                       Computation: 10998 steps/s (collection: 1.069s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -2.0744
                       Mean reward: 6.93
               Mean episode length: 863.10
Episode_Reward/track_lin_vel_xy_exp: 0.2585
Episode_Reward/track_ang_vel_z_exp: 0.3116
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.0237
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0215
 Episode_Reward/undesired_contacts: -0.0074
Episode_Reward/flat_orientation_l2: -0.0188
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6598656
                    Iteration time: 1.12s
                      Time elapsed: 00:09:39
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 537/1000 [0m                      

                       Computation: 11045 steps/s (collection: 1.052s, learning 0.060s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -2.0734
                       Mean reward: 6.90
               Mean episode length: 850.46
Episode_Reward/track_lin_vel_xy_exp: 0.2366
Episode_Reward/track_ang_vel_z_exp: 0.2649
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1062
         Episode_Reward/dof_acc_l2: -0.0356
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0225
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: -0.0194
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 1.11s
                      Time elapsed: 00:09:40
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 538/1000 [0m                      

                       Computation: 10317 steps/s (collection: 1.134s, learning 0.057s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -2.0838
                       Mean reward: 6.83
               Mean episode length: 838.78
Episode_Reward/track_lin_vel_xy_exp: 0.2811
Episode_Reward/track_ang_vel_z_exp: 0.3092
       Episode_Reward/lin_vel_z_l2: -0.0097
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.0920
         Episode_Reward/dof_acc_l2: -0.0235
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0200
 Episode_Reward/undesired_contacts: -0.0176
Episode_Reward/flat_orientation_l2: -0.0188
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6623232
                    Iteration time: 1.19s
                      Time elapsed: 00:09:41
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 539/1000 [0m                      

                       Computation: 10953 steps/s (collection: 1.070s, learning 0.052s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -2.0870
                       Mean reward: 6.60
               Mean episode length: 834.03
Episode_Reward/track_lin_vel_xy_exp: 0.3006
Episode_Reward/track_ang_vel_z_exp: 0.2273
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.0268
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0299
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 1.12s
                      Time elapsed: 00:09:42
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 540/1000 [0m                      

                       Computation: 10918 steps/s (collection: 1.081s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.0788
                       Mean reward: 6.95
               Mean episode length: 818.35
Episode_Reward/track_lin_vel_xy_exp: 0.3236
Episode_Reward/track_ang_vel_z_exp: 0.3012
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.0295
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0270
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0206
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6647808
                    Iteration time: 1.13s
                      Time elapsed: 00:09:43
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 541/1000 [0m                      

                       Computation: 11022 steps/s (collection: 1.067s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0053
                 Mean entropy loss: -2.0642
                       Mean reward: 7.11
               Mean episode length: 816.64
Episode_Reward/track_lin_vel_xy_exp: 0.3745
Episode_Reward/track_ang_vel_z_exp: 0.3456
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0959
         Episode_Reward/dof_acc_l2: -0.0328
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0294
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0194
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 1.11s
                      Time elapsed: 00:09:44
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 542/1000 [0m                      

                       Computation: 11548 steps/s (collection: 1.019s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0068
                 Mean entropy loss: -2.0607
                       Mean reward: 7.18
               Mean episode length: 827.13
Episode_Reward/track_lin_vel_xy_exp: 0.2733
Episode_Reward/track_ang_vel_z_exp: 0.2939
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.1008
         Episode_Reward/dof_acc_l2: -0.0336
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0248
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0290
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6672384
                    Iteration time: 1.06s
                      Time elapsed: 00:09:46
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 543/1000 [0m                      

                       Computation: 10743 steps/s (collection: 1.080s, learning 0.064s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.0615
                       Mean reward: 7.95
               Mean episode length: 846.33
Episode_Reward/track_lin_vel_xy_exp: 0.4764
Episode_Reward/track_ang_vel_z_exp: 0.3684
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1111
         Episode_Reward/dof_acc_l2: -0.0356
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0280
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0206
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.14s
                      Time elapsed: 00:09:47
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 544/1000 [0m                      

                       Computation: 11534 steps/s (collection: 1.016s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -2.0632
                       Mean reward: 7.79
               Mean episode length: 833.71
Episode_Reward/track_lin_vel_xy_exp: 0.3015
Episode_Reward/track_ang_vel_z_exp: 0.2278
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.1002
         Episode_Reward/dof_acc_l2: -0.0319
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0229
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0202
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6696960
                    Iteration time: 1.07s
                      Time elapsed: 00:09:48
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 545/1000 [0m                      

                       Computation: 11161 steps/s (collection: 1.053s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.0557
                       Mean reward: 7.79
               Mean episode length: 846.46
Episode_Reward/track_lin_vel_xy_exp: 0.1398
Episode_Reward/track_ang_vel_z_exp: 0.1942
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.0732
         Episode_Reward/dof_acc_l2: -0.0226
     Episode_Reward/action_rate_l2: -0.0134
      Episode_Reward/feet_air_time: -0.0175
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0186
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 1.10s
                      Time elapsed: 00:09:49
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 546/1000 [0m                      

                       Computation: 11196 steps/s (collection: 1.049s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.0459
                       Mean reward: 7.62
               Mean episode length: 844.92
Episode_Reward/track_lin_vel_xy_exp: 0.2917
Episode_Reward/track_ang_vel_z_exp: 0.3873
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.0931
         Episode_Reward/dof_acc_l2: -0.0253
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0206
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0242
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6721536
                    Iteration time: 1.10s
                      Time elapsed: 00:09:50
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 547/1000 [0m                      

                       Computation: 11270 steps/s (collection: 1.041s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.0471
                       Mean reward: 7.99
               Mean episode length: 870.42
Episode_Reward/track_lin_vel_xy_exp: 0.3196
Episode_Reward/track_ang_vel_z_exp: 0.3130
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.1034
         Episode_Reward/dof_acc_l2: -0.0341
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0279
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0240
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 1.09s
                      Time elapsed: 00:09:51
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 548/1000 [0m                      

                       Computation: 10579 steps/s (collection: 1.101s, learning 0.061s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.0637
                       Mean reward: 7.58
               Mean episode length: 884.22
Episode_Reward/track_lin_vel_xy_exp: 0.1579
Episode_Reward/track_ang_vel_z_exp: 0.4213
       Episode_Reward/lin_vel_z_l2: -0.0159
      Episode_Reward/ang_vel_xy_l2: -0.0279
     Episode_Reward/dof_torques_l2: -0.1114
         Episode_Reward/dof_acc_l2: -0.0392
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0286
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0206
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6746112
                    Iteration time: 1.16s
                      Time elapsed: 00:09:52
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 549/1000 [0m                      

                       Computation: 10875 steps/s (collection: 1.085s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -2.0564
                       Mean reward: 7.46
               Mean episode length: 884.92
Episode_Reward/track_lin_vel_xy_exp: 0.2746
Episode_Reward/track_ang_vel_z_exp: 0.3074
       Episode_Reward/lin_vel_z_l2: -0.0101
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.0215
     Episode_Reward/action_rate_l2: -0.0149
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 1.13s
                      Time elapsed: 00:09:53
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 550/1000 [0m                      

                       Computation: 11064 steps/s (collection: 1.064s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -2.0509
                       Mean reward: 7.16
               Mean episode length: 871.85
Episode_Reward/track_lin_vel_xy_exp: 0.1706
Episode_Reward/track_ang_vel_z_exp: 0.2098
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0131
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.0192
     Episode_Reward/action_rate_l2: -0.0137
      Episode_Reward/feet_air_time: -0.0179
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: -0.0262
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6770688
                    Iteration time: 1.11s
                      Time elapsed: 00:09:54
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 551/1000 [0m                      

                       Computation: 10939 steps/s (collection: 1.079s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.0464
                       Mean reward: 6.88
               Mean episode length: 859.75
Episode_Reward/track_lin_vel_xy_exp: 0.2738
Episode_Reward/track_ang_vel_z_exp: 0.2505
       Episode_Reward/lin_vel_z_l2: -0.0099
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0799
         Episode_Reward/dof_acc_l2: -0.0239
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0218
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0221
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.12s
                      Time elapsed: 00:09:56
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 552/1000 [0m                      

                       Computation: 11127 steps/s (collection: 1.052s, learning 0.053s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -2.0455
                       Mean reward: 7.22
               Mean episode length: 849.47
Episode_Reward/track_lin_vel_xy_exp: 0.2521
Episode_Reward/track_ang_vel_z_exp: 0.2765
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.0787
         Episode_Reward/dof_acc_l2: -0.0242
     Episode_Reward/action_rate_l2: -0.0148
      Episode_Reward/feet_air_time: -0.0226
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0265
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6795264
                    Iteration time: 1.10s
                      Time elapsed: 00:09:57
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 553/1000 [0m                      

                       Computation: 11169 steps/s (collection: 1.055s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -2.0561
                       Mean reward: 7.48
               Mean episode length: 842.53
Episode_Reward/track_lin_vel_xy_exp: 0.3492
Episode_Reward/track_ang_vel_z_exp: 0.2865
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.0891
         Episode_Reward/dof_acc_l2: -0.0246
     Episode_Reward/action_rate_l2: -0.0148
      Episode_Reward/feet_air_time: -0.0197
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0336
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 1.10s
                      Time elapsed: 00:09:58
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 554/1000 [0m                      

                       Computation: 10837 steps/s (collection: 1.088s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.0426
                       Mean reward: 7.16
               Mean episode length: 828.32
Episode_Reward/track_lin_vel_xy_exp: 0.2434
Episode_Reward/track_ang_vel_z_exp: 0.2624
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.0770
         Episode_Reward/dof_acc_l2: -0.0285
     Episode_Reward/action_rate_l2: -0.0149
      Episode_Reward/feet_air_time: -0.0246
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0197
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6819840
                    Iteration time: 1.13s
                      Time elapsed: 00:09:59
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 555/1000 [0m                      

                       Computation: 11145 steps/s (collection: 1.056s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -2.0362
                       Mean reward: 7.52
               Mean episode length: 818.22
Episode_Reward/track_lin_vel_xy_exp: 0.3688
Episode_Reward/track_ang_vel_z_exp: 0.2496
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.0950
         Episode_Reward/dof_acc_l2: -0.0300
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0218
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0202
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 1.10s
                      Time elapsed: 00:10:00
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 556/1000 [0m                      

                       Computation: 11206 steps/s (collection: 1.052s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0073
                 Mean entropy loss: -2.0336
                       Mean reward: 7.94
               Mean episode length: 839.74
Episode_Reward/track_lin_vel_xy_exp: 0.4543
Episode_Reward/track_ang_vel_z_exp: 0.3741
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.1117
         Episode_Reward/dof_acc_l2: -0.0295
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0244
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0195
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6844416
                    Iteration time: 1.10s
                      Time elapsed: 00:10:01
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 557/1000 [0m                      

                       Computation: 11092 steps/s (collection: 1.059s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -2.0275
                       Mean reward: 8.26
               Mean episode length: 847.39
Episode_Reward/track_lin_vel_xy_exp: 0.2626
Episode_Reward/track_ang_vel_z_exp: 0.2688
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.0214
     Episode_Reward/action_rate_l2: -0.0164
      Episode_Reward/feet_air_time: -0.0175
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0209
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 1.11s
                      Time elapsed: 00:10:02
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 558/1000 [0m                      

                       Computation: 10723 steps/s (collection: 1.098s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.0163
                       Mean reward: 8.37
               Mean episode length: 866.13
Episode_Reward/track_lin_vel_xy_exp: 0.3705
Episode_Reward/track_ang_vel_z_exp: 0.2277
       Episode_Reward/lin_vel_z_l2: -0.0101
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.1015
         Episode_Reward/dof_acc_l2: -0.0275
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0262
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6868992
                    Iteration time: 1.15s
                      Time elapsed: 00:10:03
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 559/1000 [0m                      

                       Computation: 11161 steps/s (collection: 1.056s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -2.0008
                       Mean reward: 8.76
               Mean episode length: 912.21
Episode_Reward/track_lin_vel_xy_exp: 0.4240
Episode_Reward/track_ang_vel_z_exp: 0.3457
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.1075
         Episode_Reward/dof_acc_l2: -0.0347
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0145
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.10s
                      Time elapsed: 00:10:04
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 560/1000 [0m                      

                       Computation: 11016 steps/s (collection: 1.066s, learning 0.049s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -1.9885
                       Mean reward: 8.87
               Mean episode length: 924.82
Episode_Reward/track_lin_vel_xy_exp: 0.4359
Episode_Reward/track_ang_vel_z_exp: 0.2472
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0214
     Episode_Reward/dof_torques_l2: -0.1051
         Episode_Reward/dof_acc_l2: -0.0294
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0245
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6893568
                    Iteration time: 1.12s
                      Time elapsed: 00:10:06
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 561/1000 [0m                      

                       Computation: 10913 steps/s (collection: 1.080s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -1.9832
                       Mean reward: 8.45
               Mean episode length: 922.07
Episode_Reward/track_lin_vel_xy_exp: 0.1770
Episode_Reward/track_ang_vel_z_exp: 0.3448
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.0972
         Episode_Reward/dof_acc_l2: -0.0328
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0228
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0243
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 1.13s
                      Time elapsed: 00:10:07
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 562/1000 [0m                      

                       Computation: 11235 steps/s (collection: 1.043s, learning 0.051s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0059
                 Mean entropy loss: -1.9892
                       Mean reward: 8.53
               Mean episode length: 913.26
Episode_Reward/track_lin_vel_xy_exp: 0.3647
Episode_Reward/track_ang_vel_z_exp: 0.3204
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.1095
         Episode_Reward/dof_acc_l2: -0.0377
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0275
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0284
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6918144
                    Iteration time: 1.09s
                      Time elapsed: 00:10:08
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 563/1000 [0m                      

                       Computation: 11119 steps/s (collection: 1.061s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.0019
                       Mean reward: 8.18
               Mean episode length: 919.70
Episode_Reward/track_lin_vel_xy_exp: 0.3086
Episode_Reward/track_ang_vel_z_exp: 0.3200
       Episode_Reward/lin_vel_z_l2: -0.0103
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.1026
         Episode_Reward/dof_acc_l2: -0.0245
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0164
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 1.11s
                      Time elapsed: 00:10:09
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 564/1000 [0m                      

                       Computation: 11177 steps/s (collection: 1.054s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -2.0266
                       Mean reward: 7.71
               Mean episode length: 910.95
Episode_Reward/track_lin_vel_xy_exp: 0.3865
Episode_Reward/track_ang_vel_z_exp: 0.2082
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.0210
     Episode_Reward/action_rate_l2: -0.0157
      Episode_Reward/feet_air_time: -0.0163
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0231
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6942720
                    Iteration time: 1.10s
                      Time elapsed: 00:10:10
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 565/1000 [0m                      

                       Computation: 11370 steps/s (collection: 1.035s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -2.0385
                       Mean reward: 7.77
               Mean episode length: 905.98
Episode_Reward/track_lin_vel_xy_exp: 0.1326
Episode_Reward/track_ang_vel_z_exp: 0.2995
       Episode_Reward/lin_vel_z_l2: -0.0086
      Episode_Reward/ang_vel_xy_l2: -0.0162
     Episode_Reward/dof_torques_l2: -0.0826
         Episode_Reward/dof_acc_l2: -0.0172
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0172
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0287
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 1.08s
                      Time elapsed: 00:10:11
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 566/1000 [0m                      

                       Computation: 10805 steps/s (collection: 1.082s, learning 0.055s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -2.0309
                       Mean reward: 7.78
               Mean episode length: 871.08
Episode_Reward/track_lin_vel_xy_exp: 0.3224
Episode_Reward/track_ang_vel_z_exp: 0.2870
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0903
         Episode_Reward/dof_acc_l2: -0.0287
     Episode_Reward/action_rate_l2: -0.0164
      Episode_Reward/feet_air_time: -0.0233
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0324
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6967296
                    Iteration time: 1.14s
                      Time elapsed: 00:10:12
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 567/1000 [0m                      

                       Computation: 11329 steps/s (collection: 1.031s, learning 0.054s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -2.0274
                       Mean reward: 8.24
               Mean episode length: 872.65
Episode_Reward/track_lin_vel_xy_exp: 0.5277
Episode_Reward/track_ang_vel_z_exp: 0.3220
       Episode_Reward/lin_vel_z_l2: -0.0175
      Episode_Reward/ang_vel_xy_l2: -0.0271
     Episode_Reward/dof_torques_l2: -0.1211
         Episode_Reward/dof_acc_l2: -0.0561
     Episode_Reward/action_rate_l2: -0.0202
      Episode_Reward/feet_air_time: -0.0319
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0256
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.08s
                      Time elapsed: 00:10:13
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 568/1000 [0m                      

                       Computation: 11169 steps/s (collection: 1.052s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.0296
                       Mean reward: 8.36
               Mean episode length: 888.05
Episode_Reward/track_lin_vel_xy_exp: 0.3061
Episode_Reward/track_ang_vel_z_exp: 0.3135
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.1171
         Episode_Reward/dof_acc_l2: -0.0412
     Episode_Reward/action_rate_l2: -0.0203
      Episode_Reward/feet_air_time: -0.0280
 Episode_Reward/undesired_contacts: -0.0240
Episode_Reward/flat_orientation_l2: -0.0237
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6991872
                    Iteration time: 1.10s
                      Time elapsed: 00:10:14
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 569/1000 [0m                      

                       Computation: 11528 steps/s (collection: 1.021s, learning 0.045s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.0412
                       Mean reward: 8.12
               Mean episode length: 884.75
Episode_Reward/track_lin_vel_xy_exp: 0.3086
Episode_Reward/track_ang_vel_z_exp: 0.1583
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.0296
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0154
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 1.07s
                      Time elapsed: 00:10:15
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 570/1000 [0m                      

                       Computation: 11504 steps/s (collection: 1.023s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0068
                 Mean entropy loss: -2.0538
                       Mean reward: 7.76
               Mean episode length: 861.54
Episode_Reward/track_lin_vel_xy_exp: 0.3430
Episode_Reward/track_ang_vel_z_exp: 0.2231
       Episode_Reward/lin_vel_z_l2: -0.0087
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.0163
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0157
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7016448
                    Iteration time: 1.07s
                      Time elapsed: 00:10:17
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 571/1000 [0m                      

                       Computation: 10768 steps/s (collection: 1.091s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -2.0715
                       Mean reward: 8.56
               Mean episode length: 897.46
Episode_Reward/track_lin_vel_xy_exp: 0.3228
Episode_Reward/track_ang_vel_z_exp: 0.3357
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.1084
         Episode_Reward/dof_acc_l2: -0.0335
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0216
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 1.14s
                      Time elapsed: 00:10:18
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 572/1000 [0m                      

                       Computation: 11491 steps/s (collection: 1.023s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.0746
                       Mean reward: 8.56
               Mean episode length: 906.71
Episode_Reward/track_lin_vel_xy_exp: 0.2755
Episode_Reward/track_ang_vel_z_exp: 0.2973
       Episode_Reward/lin_vel_z_l2: -0.0103
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.0224
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0166
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7041024
                    Iteration time: 1.07s
                      Time elapsed: 00:10:19
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 573/1000 [0m                      

                       Computation: 10663 steps/s (collection: 1.098s, learning 0.054s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -2.0672
                       Mean reward: 8.35
               Mean episode length: 915.63
Episode_Reward/track_lin_vel_xy_exp: 0.3742
Episode_Reward/track_ang_vel_z_exp: 0.3619
       Episode_Reward/lin_vel_z_l2: -0.0103
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.1050
         Episode_Reward/dof_acc_l2: -0.0306
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0264
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0216
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 1.15s
                      Time elapsed: 00:10:20
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 574/1000 [0m                      

                       Computation: 11250 steps/s (collection: 1.045s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -2.0471
                       Mean reward: 8.08
               Mean episode length: 895.64
Episode_Reward/track_lin_vel_xy_exp: 0.4472
Episode_Reward/track_ang_vel_z_exp: 0.2637
       Episode_Reward/lin_vel_z_l2: -0.0095
      Episode_Reward/ang_vel_xy_l2: -0.0161
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.0216
     Episode_Reward/action_rate_l2: -0.0157
      Episode_Reward/feet_air_time: -0.0211
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0210
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7065600
                    Iteration time: 1.09s
                      Time elapsed: 00:10:21
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 575/1000 [0m                      

                       Computation: 10666 steps/s (collection: 1.102s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.0447
                       Mean reward: 8.89
               Mean episode length: 889.39
Episode_Reward/track_lin_vel_xy_exp: 0.4665
Episode_Reward/track_ang_vel_z_exp: 0.3459
       Episode_Reward/lin_vel_z_l2: -0.0155
      Episode_Reward/ang_vel_xy_l2: -0.0241
     Episode_Reward/dof_torques_l2: -0.1166
         Episode_Reward/dof_acc_l2: -0.0471
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0265
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.15s
                      Time elapsed: 00:10:22
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 576/1000 [0m                      

                       Computation: 11166 steps/s (collection: 1.048s, learning 0.053s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0109
                 Mean entropy loss: -2.0550
                       Mean reward: 8.97
               Mean episode length: 894.79
Episode_Reward/track_lin_vel_xy_exp: 0.3469
Episode_Reward/track_ang_vel_z_exp: 0.3124
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.1197
         Episode_Reward/dof_acc_l2: -0.0341
     Episode_Reward/action_rate_l2: -0.0193
      Episode_Reward/feet_air_time: -0.0309
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0233
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7090176
                    Iteration time: 1.10s
                      Time elapsed: 00:10:23
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 577/1000 [0m                      

                       Computation: 11141 steps/s (collection: 1.059s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.0587
                       Mean reward: 8.67
               Mean episode length: 880.62
Episode_Reward/track_lin_vel_xy_exp: 0.3746
Episode_Reward/track_ang_vel_z_exp: 0.2531
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.0957
         Episode_Reward/dof_acc_l2: -0.0322
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0190
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 1.10s
                      Time elapsed: 00:10:24
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 578/1000 [0m                      

                       Computation: 11150 steps/s (collection: 1.052s, learning 0.050s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0069
                 Mean entropy loss: -2.0568
                       Mean reward: 8.95
               Mean episode length: 875.55
Episode_Reward/track_lin_vel_xy_exp: 0.4295
Episode_Reward/track_ang_vel_z_exp: 0.3174
       Episode_Reward/lin_vel_z_l2: -0.0097
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.0886
         Episode_Reward/dof_acc_l2: -0.0177
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0173
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7114752
                    Iteration time: 1.10s
                      Time elapsed: 00:10:25
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 579/1000 [0m                      

                       Computation: 10641 steps/s (collection: 1.095s, learning 0.060s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.0602
                       Mean reward: 8.74
               Mean episode length: 864.81
Episode_Reward/track_lin_vel_xy_exp: 0.1657
Episode_Reward/track_ang_vel_z_exp: 0.2280
       Episode_Reward/lin_vel_z_l2: -0.0088
      Episode_Reward/ang_vel_xy_l2: -0.0154
     Episode_Reward/dof_torques_l2: -0.0598
         Episode_Reward/dof_acc_l2: -0.0101
     Episode_Reward/action_rate_l2: -0.0116
      Episode_Reward/feet_air_time: -0.0135
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0309
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 1.15s
                      Time elapsed: 00:10:27
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 580/1000 [0m                      

                       Computation: 11472 steps/s (collection: 1.024s, learning 0.047s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -2.0572
                       Mean reward: 8.99
               Mean episode length: 880.44
Episode_Reward/track_lin_vel_xy_exp: 0.3342
Episode_Reward/track_ang_vel_z_exp: 0.4297
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.1198
         Episode_Reward/dof_acc_l2: -0.0308
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0274
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7139328
                    Iteration time: 1.07s
                      Time elapsed: 00:10:28
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 581/1000 [0m                      

                       Computation: 10781 steps/s (collection: 1.093s, learning 0.046s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -2.0635
                       Mean reward: 8.24
               Mean episode length: 880.14
Episode_Reward/track_lin_vel_xy_exp: 0.2030
Episode_Reward/track_ang_vel_z_exp: 0.1891
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0153
     Episode_Reward/dof_torques_l2: -0.1017
         Episode_Reward/dof_acc_l2: -0.0238
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0195
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0191
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 1.14s
                      Time elapsed: 00:10:29
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 582/1000 [0m                      

                       Computation: 11006 steps/s (collection: 1.068s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.0771
                       Mean reward: 8.38
               Mean episode length: 894.67
Episode_Reward/track_lin_vel_xy_exp: 0.2215
Episode_Reward/track_ang_vel_z_exp: 0.2449
       Episode_Reward/lin_vel_z_l2: -0.0081
      Episode_Reward/ang_vel_xy_l2: -0.0148
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.0153
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0162
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0215
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7163904
                    Iteration time: 1.12s
                      Time elapsed: 00:10:30
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 583/1000 [0m                      

                       Computation: 10911 steps/s (collection: 1.082s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -2.0857
                       Mean reward: 8.21
               Mean episode length: 916.21
Episode_Reward/track_lin_vel_xy_exp: 0.4142
Episode_Reward/track_ang_vel_z_exp: 0.2524
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.1085
         Episode_Reward/dof_acc_l2: -0.0345
     Episode_Reward/action_rate_l2: -0.0186
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0178
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.13s
                      Time elapsed: 00:10:31
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 584/1000 [0m                      

                       Computation: 11138 steps/s (collection: 1.055s, learning 0.048s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.1112
                       Mean reward: 7.97
               Mean episode length: 905.12
Episode_Reward/track_lin_vel_xy_exp: 0.2983
Episode_Reward/track_ang_vel_z_exp: 0.2937
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.0934
         Episode_Reward/dof_acc_l2: -0.0236
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0154
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0273
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7188480
                    Iteration time: 1.10s
                      Time elapsed: 00:10:32
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 585/1000 [0m                      

                       Computation: 11499 steps/s (collection: 1.014s, learning 0.055s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -2.1284
                       Mean reward: 7.87
               Mean episode length: 906.61
Episode_Reward/track_lin_vel_xy_exp: 0.3250
Episode_Reward/track_ang_vel_z_exp: 0.3537
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.1090
         Episode_Reward/dof_acc_l2: -0.0334
     Episode_Reward/action_rate_l2: -0.0193
      Episode_Reward/feet_air_time: -0.0270
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0195
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 1.07s
                      Time elapsed: 00:10:33
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 586/1000 [0m                      

                       Computation: 11670 steps/s (collection: 1.008s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.1512
                       Mean reward: 7.95
               Mean episode length: 901.09
Episode_Reward/track_lin_vel_xy_exp: 0.3237
Episode_Reward/track_ang_vel_z_exp: 0.2718
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.1007
         Episode_Reward/dof_acc_l2: -0.0312
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0253
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0250
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7213056
                    Iteration time: 1.05s
                      Time elapsed: 00:10:34
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 587/1000 [0m                      

                       Computation: 11175 steps/s (collection: 1.056s, learning 0.044s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -2.1700
                       Mean reward: 7.60
               Mean episode length: 891.27
Episode_Reward/track_lin_vel_xy_exp: 0.2619
Episode_Reward/track_ang_vel_z_exp: 0.2673
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1067
         Episode_Reward/dof_acc_l2: -0.0381
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0224
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: -0.0320
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 1.10s
                      Time elapsed: 00:10:35
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 588/1000 [0m                      

                       Computation: 11230 steps/s (collection: 1.033s, learning 0.061s)
             Mean action noise std: 0.21
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -2.1743
                       Mean reward: 7.49
               Mean episode length: 880.26
Episode_Reward/track_lin_vel_xy_exp: 0.2175
Episode_Reward/track_ang_vel_z_exp: 0.1157
       Episode_Reward/lin_vel_z_l2: -0.0078
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.0849
         Episode_Reward/dof_acc_l2: -0.0185
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0188
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7237632
                    Iteration time: 1.09s
                      Time elapsed: 00:10:36
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 589/1000 [0m                      

                       Computation: 10917 steps/s (collection: 1.081s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -2.1843
                       Mean reward: 7.61
               Mean episode length: 889.15
Episode_Reward/track_lin_vel_xy_exp: 0.4167
Episode_Reward/track_ang_vel_z_exp: 0.2975
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.1030
         Episode_Reward/dof_acc_l2: -0.0301
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0220
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0187
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 1.13s
                      Time elapsed: 00:10:38
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 590/1000 [0m                      

                       Computation: 10953 steps/s (collection: 1.077s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -2.1954
                       Mean reward: 7.81
               Mean episode length: 892.49
Episode_Reward/track_lin_vel_xy_exp: 0.3270
Episode_Reward/track_ang_vel_z_exp: 0.3253
       Episode_Reward/lin_vel_z_l2: -0.0103
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.1057
         Episode_Reward/dof_acc_l2: -0.0242
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0191
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0197
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7262208
                    Iteration time: 1.12s
                      Time elapsed: 00:10:39
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 591/1000 [0m                      

                       Computation: 11490 steps/s (collection: 1.018s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -2.2062
                       Mean reward: 7.83
               Mean episode length: 891.77
Episode_Reward/track_lin_vel_xy_exp: 0.2404
Episode_Reward/track_ang_vel_z_exp: 0.2800
       Episode_Reward/lin_vel_z_l2: -0.0078
      Episode_Reward/ang_vel_xy_l2: -0.0161
     Episode_Reward/dof_torques_l2: -0.0751
         Episode_Reward/dof_acc_l2: -0.0116
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0170
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0260
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.07s
                      Time elapsed: 00:10:40
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 592/1000 [0m                      

                       Computation: 11596 steps/s (collection: 1.010s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.2071
                       Mean reward: 8.19
               Mean episode length: 884.44
Episode_Reward/track_lin_vel_xy_exp: 0.4177
Episode_Reward/track_ang_vel_z_exp: 0.3161
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.0939
         Episode_Reward/dof_acc_l2: -0.0196
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0192
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7286784
                    Iteration time: 1.06s
                      Time elapsed: 00:10:41
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 593/1000 [0m                      

                       Computation: 11339 steps/s (collection: 1.033s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -2.1965
                       Mean reward: 8.16
               Mean episode length: 873.66
Episode_Reward/track_lin_vel_xy_exp: 0.3437
Episode_Reward/track_ang_vel_z_exp: 0.3135
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.0926
         Episode_Reward/dof_acc_l2: -0.0295
     Episode_Reward/action_rate_l2: -0.0158
      Episode_Reward/feet_air_time: -0.0240
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0269
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 1.08s
                      Time elapsed: 00:10:42
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 594/1000 [0m                      

                       Computation: 10971 steps/s (collection: 1.058s, learning 0.062s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.2109
                       Mean reward: 8.49
               Mean episode length: 890.42
Episode_Reward/track_lin_vel_xy_exp: 0.3958
Episode_Reward/track_ang_vel_z_exp: 0.2889
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.1111
         Episode_Reward/dof_acc_l2: -0.0330
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0235
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0185
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7311360
                    Iteration time: 1.12s
                      Time elapsed: 00:10:43
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 595/1000 [0m                      

                       Computation: 11472 steps/s (collection: 1.023s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -2.2268
                       Mean reward: 8.35
               Mean episode length: 907.03
Episode_Reward/track_lin_vel_xy_exp: 0.2087
Episode_Reward/track_ang_vel_z_exp: 0.2339
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.0248
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0209
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: -0.0284
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 1.07s
                      Time elapsed: 00:10:44
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 596/1000 [0m                      

                       Computation: 11146 steps/s (collection: 1.046s, learning 0.056s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -2.2119
                       Mean reward: 8.98
               Mean episode length: 927.59
Episode_Reward/track_lin_vel_xy_exp: 0.5793
Episode_Reward/track_ang_vel_z_exp: 0.2849
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0225
     Episode_Reward/dof_torques_l2: -0.1202
         Episode_Reward/dof_acc_l2: -0.0470
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0231
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0178
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7335936
                    Iteration time: 1.10s
                      Time elapsed: 00:10:45
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 597/1000 [0m                      

                       Computation: 11292 steps/s (collection: 1.039s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -2.1989
                       Mean reward: 8.55
               Mean episode length: 914.05
Episode_Reward/track_lin_vel_xy_exp: 0.2305
Episode_Reward/track_ang_vel_z_exp: 0.2081
       Episode_Reward/lin_vel_z_l2: -0.0093
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.0176
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0169
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 1.09s
                      Time elapsed: 00:10:46
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 598/1000 [0m                      

                       Computation: 10898 steps/s (collection: 1.075s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -2.2016
                       Mean reward: 8.49
               Mean episode length: 915.86
Episode_Reward/track_lin_vel_xy_exp: 0.3278
Episode_Reward/track_ang_vel_z_exp: 0.2610
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.1037
         Episode_Reward/dof_acc_l2: -0.0305
     Episode_Reward/action_rate_l2: -0.0165
      Episode_Reward/feet_air_time: -0.0204
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0228
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7360512
                    Iteration time: 1.13s
                      Time elapsed: 00:10:47
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 599/1000 [0m                      

                       Computation: 11249 steps/s (collection: 1.047s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0066
                 Mean entropy loss: -2.2198
                       Mean reward: 8.52
               Mean episode length: 906.40
Episode_Reward/track_lin_vel_xy_exp: 0.3311
Episode_Reward/track_ang_vel_z_exp: 0.3448
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0141
     Episode_Reward/dof_torques_l2: -0.0865
         Episode_Reward/dof_acc_l2: -0.0175
     Episode_Reward/action_rate_l2: -0.0147
      Episode_Reward/feet_air_time: -0.0168
 Episode_Reward/undesired_contacts: -0.0067
Episode_Reward/flat_orientation_l2: -0.0231
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.09s
                      Time elapsed: 00:10:49
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 600/1000 [0m                      

                       Computation: 10903 steps/s (collection: 1.079s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.2386
                       Mean reward: 8.46
               Mean episode length: 914.93
Episode_Reward/track_lin_vel_xy_exp: 0.3024
Episode_Reward/track_ang_vel_z_exp: 0.4054
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.1081
         Episode_Reward/dof_acc_l2: -0.0322
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0299
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0191
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7385088
                    Iteration time: 1.13s
                      Time elapsed: 00:10:50
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 601/1000 [0m                      

                       Computation: 10843 steps/s (collection: 1.076s, learning 0.057s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -2.2558
                       Mean reward: 8.05
               Mean episode length: 924.66
Episode_Reward/track_lin_vel_xy_exp: 0.3190
Episode_Reward/track_ang_vel_z_exp: 0.2843
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0193
     Episode_Reward/dof_torques_l2: -0.1061
         Episode_Reward/dof_acc_l2: -0.0279
     Episode_Reward/action_rate_l2: -0.0186
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0547
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 1.13s
                      Time elapsed: 00:10:51
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 602/1000 [0m                      

                       Computation: 11680 steps/s (collection: 1.003s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0105
                 Mean entropy loss: -2.2571
                       Mean reward: 7.44
               Mean episode length: 902.07
Episode_Reward/track_lin_vel_xy_exp: 0.2567
Episode_Reward/track_ang_vel_z_exp: 0.1676
       Episode_Reward/lin_vel_z_l2: -0.0084
      Episode_Reward/ang_vel_xy_l2: -0.0153
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.0142
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0154
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0244
  Episode_Termination/base_contact: 0.8333
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7409664
                    Iteration time: 1.05s
                      Time elapsed: 00:10:52
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 603/1000 [0m                      

                       Computation: 11193 steps/s (collection: 1.054s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.2568
                       Mean reward: 7.21
               Mean episode length: 902.45
Episode_Reward/track_lin_vel_xy_exp: 0.2522
Episode_Reward/track_ang_vel_z_exp: 0.2752
       Episode_Reward/lin_vel_z_l2: -0.0101
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.1008
         Episode_Reward/dof_acc_l2: -0.0235
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0212
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0143
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 1.10s
                      Time elapsed: 00:10:53
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 604/1000 [0m                      

                       Computation: 11279 steps/s (collection: 1.045s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0048
                 Mean entropy loss: -2.2798
                       Mean reward: 7.15
               Mean episode length: 886.15
Episode_Reward/track_lin_vel_xy_exp: 0.3275
Episode_Reward/track_ang_vel_z_exp: 0.2048
       Episode_Reward/lin_vel_z_l2: -0.0105
      Episode_Reward/ang_vel_xy_l2: -0.0166
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.0231
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0207
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0221
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7434240
                    Iteration time: 1.09s
                      Time elapsed: 00:10:54
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 605/1000 [0m                      

                       Computation: 10937 steps/s (collection: 1.071s, learning 0.053s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -2.2808
                       Mean reward: 7.47
               Mean episode length: 879.39
Episode_Reward/track_lin_vel_xy_exp: 0.3583
Episode_Reward/track_ang_vel_z_exp: 0.3071
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.1071
         Episode_Reward/dof_acc_l2: -0.0344
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0283
 Episode_Reward/undesired_contacts: -0.0065
Episode_Reward/flat_orientation_l2: -0.0313
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 1.12s
                      Time elapsed: 00:10:55
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 606/1000 [0m                      

                       Computation: 10744 steps/s (collection: 1.090s, learning 0.054s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -2.2893
                       Mean reward: 7.06
               Mean episode length: 876.50
Episode_Reward/track_lin_vel_xy_exp: 0.4567
Episode_Reward/track_ang_vel_z_exp: 0.3812
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0216
     Episode_Reward/dof_torques_l2: -0.1274
         Episode_Reward/dof_acc_l2: -0.0403
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0318
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0257
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7458816
                    Iteration time: 1.14s
                      Time elapsed: 00:10:56
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 607/1000 [0m                      

                       Computation: 10855 steps/s (collection: 1.065s, learning 0.067s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.3281
                       Mean reward: 6.61
               Mean episode length: 868.45
Episode_Reward/track_lin_vel_xy_exp: 0.1858
Episode_Reward/track_ang_vel_z_exp: 0.2221
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0150
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.0219
     Episode_Reward/action_rate_l2: -0.0130
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0225
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.13s
                      Time elapsed: 00:10:57
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 608/1000 [0m                      

                       Computation: 11176 steps/s (collection: 1.044s, learning 0.056s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0060
                 Mean entropy loss: -2.3075
                       Mean reward: 6.39
               Mean episode length: 857.00
Episode_Reward/track_lin_vel_xy_exp: 0.1490
Episode_Reward/track_ang_vel_z_exp: 0.3200
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0200
     Episode_Reward/dof_torques_l2: -0.1015
         Episode_Reward/dof_acc_l2: -0.0381
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0309
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0271
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7483392
                    Iteration time: 1.10s
                      Time elapsed: 00:10:59
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 609/1000 [0m                      

                       Computation: 11476 steps/s (collection: 1.026s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -2.2967
                       Mean reward: 6.83
               Mean episode length: 858.91
Episode_Reward/track_lin_vel_xy_exp: 0.3740
Episode_Reward/track_ang_vel_z_exp: 0.1778
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0164
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.0198
     Episode_Reward/action_rate_l2: -0.0159
      Episode_Reward/feet_air_time: -0.0222
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: -0.0232
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 1.07s
                      Time elapsed: 00:11:00
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 610/1000 [0m                      

                       Computation: 11187 steps/s (collection: 1.045s, learning 0.054s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.3117
                       Mean reward: 7.26
               Mean episode length: 857.29
Episode_Reward/track_lin_vel_xy_exp: 0.4364
Episode_Reward/track_ang_vel_z_exp: 0.2435
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0953
         Episode_Reward/dof_acc_l2: -0.0208
     Episode_Reward/action_rate_l2: -0.0173
      Episode_Reward/feet_air_time: -0.0205
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0186
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7507968
                    Iteration time: 1.10s
                      Time elapsed: 00:11:01
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 611/1000 [0m                      

                       Computation: 10724 steps/s (collection: 1.082s, learning 0.064s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.3283
                       Mean reward: 7.82
               Mean episode length: 897.79
Episode_Reward/track_lin_vel_xy_exp: 0.2538
Episode_Reward/track_ang_vel_z_exp: 0.2969
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.0273
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0197
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: -0.0224
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 1.15s
                      Time elapsed: 00:11:02
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 612/1000 [0m                      

                       Computation: 11043 steps/s (collection: 1.066s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -2.3246
                       Mean reward: 8.05
               Mean episode length: 904.49
Episode_Reward/track_lin_vel_xy_exp: 0.6857
Episode_Reward/track_ang_vel_z_exp: 0.4232
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.1174
         Episode_Reward/dof_acc_l2: -0.0441
     Episode_Reward/action_rate_l2: -0.0194
      Episode_Reward/feet_air_time: -0.0373
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0210
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7532544
                    Iteration time: 1.11s
                      Time elapsed: 00:11:03
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 613/1000 [0m                      

                       Computation: 11149 steps/s (collection: 1.052s, learning 0.050s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0056
                 Mean entropy loss: -2.3294
                       Mean reward: 8.74
               Mean episode length: 928.19
Episode_Reward/track_lin_vel_xy_exp: 0.2701
Episode_Reward/track_ang_vel_z_exp: 0.3311
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0165
     Episode_Reward/dof_torques_l2: -0.0929
         Episode_Reward/dof_acc_l2: -0.0204
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0171
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0200
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 1.10s
                      Time elapsed: 00:11:04
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 614/1000 [0m                      

                       Computation: 11494 steps/s (collection: 1.018s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.3567
                       Mean reward: 9.28
               Mean episode length: 925.04
Episode_Reward/track_lin_vel_xy_exp: 0.3220
Episode_Reward/track_ang_vel_z_exp: 0.2614
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.0206
     Episode_Reward/action_rate_l2: -0.0145
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0200
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7557120
                    Iteration time: 1.07s
                      Time elapsed: 00:11:05
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 615/1000 [0m                      

                       Computation: 10385 steps/s (collection: 1.096s, learning 0.087s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.3635
                       Mean reward: 9.16
               Mean episode length: 925.70
Episode_Reward/track_lin_vel_xy_exp: 0.2694
Episode_Reward/track_ang_vel_z_exp: 0.3267
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.1110
         Episode_Reward/dof_acc_l2: -0.0281
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0224
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0169
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.18s
                      Time elapsed: 00:11:06
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 616/1000 [0m                      

                       Computation: 11409 steps/s (collection: 1.028s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -2.3722
                       Mean reward: 9.03
               Mean episode length: 935.52
Episode_Reward/track_lin_vel_xy_exp: 0.3908
Episode_Reward/track_ang_vel_z_exp: 0.3864
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.1074
         Episode_Reward/dof_acc_l2: -0.0256
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0244
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7581696
                    Iteration time: 1.08s
                      Time elapsed: 00:11:07
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 617/1000 [0m                      

                       Computation: 11081 steps/s (collection: 1.063s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.3672
                       Mean reward: 9.17
               Mean episode length: 926.00
Episode_Reward/track_lin_vel_xy_exp: 0.4709
Episode_Reward/track_ang_vel_z_exp: 0.2397
       Episode_Reward/lin_vel_z_l2: -0.0101
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.0230
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0220
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0193
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 1.11s
                      Time elapsed: 00:11:09
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 618/1000 [0m                      

                       Computation: 10762 steps/s (collection: 1.097s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0073
                 Mean entropy loss: -2.3742
                       Mean reward: 8.96
               Mean episode length: 927.02
Episode_Reward/track_lin_vel_xy_exp: 0.2453
Episode_Reward/track_ang_vel_z_exp: 0.3424
       Episode_Reward/lin_vel_z_l2: -0.0160
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.1058
         Episode_Reward/dof_acc_l2: -0.0423
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0306
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0262
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7606272
                    Iteration time: 1.14s
                      Time elapsed: 00:11:10
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 619/1000 [0m                      

                       Computation: 10694 steps/s (collection: 1.100s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.4054
                       Mean reward: 9.22
               Mean episode length: 934.24
Episode_Reward/track_lin_vel_xy_exp: 0.5802
Episode_Reward/track_ang_vel_z_exp: 0.2697
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.1040
         Episode_Reward/dof_acc_l2: -0.0321
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0251
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 1.15s
                      Time elapsed: 00:11:11
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 620/1000 [0m                      

                       Computation: 11746 steps/s (collection: 0.998s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.4034
                       Mean reward: 9.33
               Mean episode length: 945.45
Episode_Reward/track_lin_vel_xy_exp: 0.5623
Episode_Reward/track_ang_vel_z_exp: 0.2912
       Episode_Reward/lin_vel_z_l2: -0.0097
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.1100
         Episode_Reward/dof_acc_l2: -0.0338
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0296
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0194
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7630848
                    Iteration time: 1.05s
                      Time elapsed: 00:11:12
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 621/1000 [0m                      

                       Computation: 11508 steps/s (collection: 1.013s, learning 0.055s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.4148
                       Mean reward: 9.93
               Mean episode length: 944.94
Episode_Reward/track_lin_vel_xy_exp: 0.5030
Episode_Reward/track_ang_vel_z_exp: 0.2913
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.0269
     Episode_Reward/action_rate_l2: -0.0146
      Episode_Reward/feet_air_time: -0.0197
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0273
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 1.07s
                      Time elapsed: 00:11:13
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 622/1000 [0m                      

                       Computation: 11397 steps/s (collection: 1.027s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.4407
                       Mean reward: 10.23
               Mean episode length: 940.30
Episode_Reward/track_lin_vel_xy_exp: 0.4250
Episode_Reward/track_ang_vel_z_exp: 0.4262
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.1175
         Episode_Reward/dof_acc_l2: -0.0367
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0235
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0140
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7655424
                    Iteration time: 1.08s
                      Time elapsed: 00:11:14
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 623/1000 [0m                      

                       Computation: 10859 steps/s (collection: 1.077s, learning 0.055s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0062
                 Mean entropy loss: -2.4417
                       Mean reward: 9.91
               Mean episode length: 938.26
Episode_Reward/track_lin_vel_xy_exp: 0.3089
Episode_Reward/track_ang_vel_z_exp: 0.2996
       Episode_Reward/lin_vel_z_l2: -0.0110
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.1052
         Episode_Reward/dof_acc_l2: -0.0288
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0316
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.13s
                      Time elapsed: 00:11:15
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 624/1000 [0m                      

                       Computation: 11114 steps/s (collection: 1.059s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.4417
                       Mean reward: 9.33
               Mean episode length: 935.90
Episode_Reward/track_lin_vel_xy_exp: 0.2064
Episode_Reward/track_ang_vel_z_exp: 0.3747
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.1163
         Episode_Reward/dof_acc_l2: -0.0382
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0281
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0219
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7680000
                    Iteration time: 1.11s
                      Time elapsed: 00:11:16
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 625/1000 [0m                      

                       Computation: 11092 steps/s (collection: 1.052s, learning 0.056s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.4328
                       Mean reward: 9.61
               Mean episode length: 928.96
Episode_Reward/track_lin_vel_xy_exp: 0.2658
Episode_Reward/track_ang_vel_z_exp: 0.3618
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.1016
         Episode_Reward/dof_acc_l2: -0.0275
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0198
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0239
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 1.11s
                      Time elapsed: 00:11:17
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 626/1000 [0m                      

                       Computation: 10945 steps/s (collection: 1.049s, learning 0.073s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.4357
                       Mean reward: 9.42
               Mean episode length: 922.68
Episode_Reward/track_lin_vel_xy_exp: 0.3599
Episode_Reward/track_ang_vel_z_exp: 0.3366
       Episode_Reward/lin_vel_z_l2: -0.0173
      Episode_Reward/ang_vel_xy_l2: -0.0249
     Episode_Reward/dof_torques_l2: -0.1304
         Episode_Reward/dof_acc_l2: -0.0683
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0396
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7704576
                    Iteration time: 1.12s
                      Time elapsed: 00:11:18
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 627/1000 [0m                      

                       Computation: 10665 steps/s (collection: 1.101s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -2.4457
                       Mean reward: 9.44
               Mean episode length: 920.70
Episode_Reward/track_lin_vel_xy_exp: 0.2730
Episode_Reward/track_ang_vel_z_exp: 0.3388
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0182
     Episode_Reward/dof_torques_l2: -0.0967
         Episode_Reward/dof_acc_l2: -0.0229
     Episode_Reward/action_rate_l2: -0.0159
      Episode_Reward/feet_air_time: -0.0261
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0252
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 1.15s
                      Time elapsed: 00:11:20
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 628/1000 [0m                      

                       Computation: 11602 steps/s (collection: 1.004s, learning 0.055s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -2.4401
                       Mean reward: 8.88
               Mean episode length: 884.62
Episode_Reward/track_lin_vel_xy_exp: 0.3620
Episode_Reward/track_ang_vel_z_exp: 0.2235
       Episode_Reward/lin_vel_z_l2: -0.0091
      Episode_Reward/ang_vel_xy_l2: -0.0167
     Episode_Reward/dof_torques_l2: -0.0801
         Episode_Reward/dof_acc_l2: -0.0198
     Episode_Reward/action_rate_l2: -0.0129
      Episode_Reward/feet_air_time: -0.0170
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0276
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7729152
                    Iteration time: 1.06s
                      Time elapsed: 00:11:21
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 629/1000 [0m                      

                       Computation: 11586 steps/s (collection: 1.006s, learning 0.055s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.4081
                       Mean reward: 8.49
               Mean episode length: 876.97
Episode_Reward/track_lin_vel_xy_exp: 0.4038
Episode_Reward/track_ang_vel_z_exp: 0.3654
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0200
     Episode_Reward/dof_torques_l2: -0.1084
         Episode_Reward/dof_acc_l2: -0.0330
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0279
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0165
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 1.06s
                      Time elapsed: 00:11:22
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 630/1000 [0m                      

                       Computation: 11363 steps/s (collection: 1.034s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.3955
                       Mean reward: 7.67
               Mean episode length: 852.48
Episode_Reward/track_lin_vel_xy_exp: 0.2789
Episode_Reward/track_ang_vel_z_exp: 0.2722
       Episode_Reward/lin_vel_z_l2: -0.0093
      Episode_Reward/ang_vel_xy_l2: -0.0156
     Episode_Reward/dof_torques_l2: -0.0816
         Episode_Reward/dof_acc_l2: -0.0230
     Episode_Reward/action_rate_l2: -0.0138
      Episode_Reward/feet_air_time: -0.0224
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7753728
                    Iteration time: 1.08s
                      Time elapsed: 00:11:23
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 631/1000 [0m                      

                       Computation: 11595 steps/s (collection: 1.016s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -2.3935
                       Mean reward: 8.04
               Mean episode length: 859.71
Episode_Reward/track_lin_vel_xy_exp: 0.4044
Episode_Reward/track_ang_vel_z_exp: 0.3169
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.1008
         Episode_Reward/dof_acc_l2: -0.0265
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0186
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.06s
                      Time elapsed: 00:11:24
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 632/1000 [0m                      

                       Computation: 11423 steps/s (collection: 1.028s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0054
                 Mean entropy loss: -2.3847
                       Mean reward: 8.35
               Mean episode length: 863.98
Episode_Reward/track_lin_vel_xy_exp: 0.3133
Episode_Reward/track_ang_vel_z_exp: 0.3660
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0243
     Episode_Reward/dof_torques_l2: -0.1168
         Episode_Reward/dof_acc_l2: -0.0433
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0259
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0237
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7778304
                    Iteration time: 1.08s
                      Time elapsed: 00:11:25
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 633/1000 [0m                      

                       Computation: 11331 steps/s (collection: 1.037s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -2.3842
                       Mean reward: 8.30
               Mean episode length: 861.72
Episode_Reward/track_lin_vel_xy_exp: 0.2693
Episode_Reward/track_ang_vel_z_exp: 0.2906
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0245
     Episode_Reward/dof_torques_l2: -0.1169
         Episode_Reward/dof_acc_l2: -0.0389
     Episode_Reward/action_rate_l2: -0.0173
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0186
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 1.08s
                      Time elapsed: 00:11:26
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 634/1000 [0m                      

                       Computation: 10514 steps/s (collection: 1.045s, learning 0.124s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -2.4078
                       Mean reward: 8.81
               Mean episode length: 862.72
Episode_Reward/track_lin_vel_xy_exp: 0.4762
Episode_Reward/track_ang_vel_z_exp: 0.3496
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.1144
         Episode_Reward/dof_acc_l2: -0.0377
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0286
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0192
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7802880
                    Iteration time: 1.17s
                      Time elapsed: 00:11:27
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 635/1000 [0m                      

                       Computation: 11708 steps/s (collection: 1.005s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -2.4228
                       Mean reward: 8.97
               Mean episode length: 886.19
Episode_Reward/track_lin_vel_xy_exp: 0.3509
Episode_Reward/track_ang_vel_z_exp: 0.3115
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.1063
         Episode_Reward/dof_acc_l2: -0.0262
     Episode_Reward/action_rate_l2: -0.0165
      Episode_Reward/feet_air_time: -0.0202
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0200
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 1.05s
                      Time elapsed: 00:11:28
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 636/1000 [0m                      

                       Computation: 11521 steps/s (collection: 1.020s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.4222
                       Mean reward: 9.04
               Mean episode length: 874.18
Episode_Reward/track_lin_vel_xy_exp: 0.3552
Episode_Reward/track_ang_vel_z_exp: 0.3120
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.0240
     Episode_Reward/action_rate_l2: -0.0146
      Episode_Reward/feet_air_time: -0.0206
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0273
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7827456
                    Iteration time: 1.07s
                      Time elapsed: 00:11:29
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 637/1000 [0m                      

                       Computation: 10803 steps/s (collection: 1.065s, learning 0.072s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.4334
                       Mean reward: 9.29
               Mean episode length: 890.35
Episode_Reward/track_lin_vel_xy_exp: 0.4556
Episode_Reward/track_ang_vel_z_exp: 0.2633
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0837
         Episode_Reward/dof_acc_l2: -0.0227
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0194
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0132
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 1.14s
                      Time elapsed: 00:11:30
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 638/1000 [0m                      

                       Computation: 11303 steps/s (collection: 1.035s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -2.4681
                       Mean reward: 9.35
               Mean episode length: 907.25
Episode_Reward/track_lin_vel_xy_exp: 0.2463
Episode_Reward/track_ang_vel_z_exp: 0.2836
       Episode_Reward/lin_vel_z_l2: -0.0115
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.1035
         Episode_Reward/dof_acc_l2: -0.0285
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0237
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0223
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7852032
                    Iteration time: 1.09s
                      Time elapsed: 00:11:32
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 639/1000 [0m                      

                       Computation: 11789 steps/s (collection: 0.997s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -2.4698
                       Mean reward: 9.65
               Mean episode length: 898.10
Episode_Reward/track_lin_vel_xy_exp: 0.3788
Episode_Reward/track_ang_vel_z_exp: 0.2970
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.1034
         Episode_Reward/dof_acc_l2: -0.0317
     Episode_Reward/action_rate_l2: -0.0154
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0195
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.04s
                      Time elapsed: 00:11:33
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 640/1000 [0m                      

                       Computation: 12671 steps/s (collection: 0.923s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -2.4661
                       Mean reward: 8.98
               Mean episode length: 896.60
Episode_Reward/track_lin_vel_xy_exp: 0.2483
Episode_Reward/track_ang_vel_z_exp: 0.2797
       Episode_Reward/lin_vel_z_l2: -0.0100
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.0971
         Episode_Reward/dof_acc_l2: -0.0203
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0164
 Episode_Reward/undesired_contacts: -0.0091
Episode_Reward/flat_orientation_l2: -0.0266
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7876608
                    Iteration time: 0.97s
                      Time elapsed: 00:11:34
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 641/1000 [0m                      

                       Computation: 12689 steps/s (collection: 0.922s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.4656
                       Mean reward: 8.89
               Mean episode length: 892.02
Episode_Reward/track_lin_vel_xy_exp: 0.4047
Episode_Reward/track_ang_vel_z_exp: 0.2945
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0212
     Episode_Reward/dof_torques_l2: -0.1028
         Episode_Reward/dof_acc_l2: -0.0311
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0243
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: -0.0204
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 0.97s
                      Time elapsed: 00:11:35
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 642/1000 [0m                      

                       Computation: 12605 steps/s (collection: 0.930s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.4497
                       Mean reward: 8.09
               Mean episode length: 875.45
Episode_Reward/track_lin_vel_xy_exp: 0.2561
Episode_Reward/track_ang_vel_z_exp: 0.2497
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.0975
         Episode_Reward/dof_acc_l2: -0.0329
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0271
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7901184
                    Iteration time: 0.97s
                      Time elapsed: 00:11:35
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 643/1000 [0m                      

                       Computation: 12550 steps/s (collection: 0.934s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -2.4617
                       Mean reward: 8.07
               Mean episode length: 889.61
Episode_Reward/track_lin_vel_xy_exp: 0.3390
Episode_Reward/track_ang_vel_z_exp: 0.2765
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.0973
         Episode_Reward/dof_acc_l2: -0.0297
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0167
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 0.98s
                      Time elapsed: 00:11:36
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 644/1000 [0m                      

                       Computation: 11536 steps/s (collection: 1.018s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -2.4689
                       Mean reward: 7.90
               Mean episode length: 882.33
Episode_Reward/track_lin_vel_xy_exp: 0.2871
Episode_Reward/track_ang_vel_z_exp: 0.2236
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0945
         Episode_Reward/dof_acc_l2: -0.0295
     Episode_Reward/action_rate_l2: -0.0137
      Episode_Reward/feet_air_time: -0.0220
 Episode_Reward/undesired_contacts: -0.0038
Episode_Reward/flat_orientation_l2: -0.0245
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7925760
                    Iteration time: 1.07s
                      Time elapsed: 00:11:38
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 645/1000 [0m                      

                       Computation: 12607 steps/s (collection: 0.929s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0054
                 Mean entropy loss: -2.4541
                       Mean reward: 7.93
               Mean episode length: 877.93
Episode_Reward/track_lin_vel_xy_exp: 0.3486
Episode_Reward/track_ang_vel_z_exp: 0.2978
       Episode_Reward/lin_vel_z_l2: -0.0094
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.0796
         Episode_Reward/dof_acc_l2: -0.0189
     Episode_Reward/action_rate_l2: -0.0129
      Episode_Reward/feet_air_time: -0.0147
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 0.97s
                      Time elapsed: 00:11:39
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 646/1000 [0m                      

                       Computation: 12190 steps/s (collection: 0.963s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -2.4481
                       Mean reward: 7.93
               Mean episode length: 877.42
Episode_Reward/track_lin_vel_xy_exp: 0.3726
Episode_Reward/track_ang_vel_z_exp: 0.2852
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.1001
         Episode_Reward/dof_acc_l2: -0.0273
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0135
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7950336
                    Iteration time: 1.01s
                      Time elapsed: 00:11:40
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 647/1000 [0m                      

                       Computation: 12179 steps/s (collection: 0.960s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.4392
                       Mean reward: 8.53
               Mean episode length: 868.94
Episode_Reward/track_lin_vel_xy_exp: 0.4151
Episode_Reward/track_ang_vel_z_exp: 0.3216
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.1065
         Episode_Reward/dof_acc_l2: -0.0414
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0265
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.01s
                      Time elapsed: 00:11:41
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 648/1000 [0m                      

                       Computation: 12174 steps/s (collection: 0.948s, learning 0.061s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.4309
                       Mean reward: 8.24
               Mean episode length: 864.71
Episode_Reward/track_lin_vel_xy_exp: 0.2712
Episode_Reward/track_ang_vel_z_exp: 0.1625
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.0982
         Episode_Reward/dof_acc_l2: -0.0293
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0205
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0265
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7974912
                    Iteration time: 1.01s
                      Time elapsed: 00:11:42
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 649/1000 [0m                      

                       Computation: 12308 steps/s (collection: 0.932s, learning 0.066s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -2.4380
                       Mean reward: 8.91
               Mean episode length: 884.01
Episode_Reward/track_lin_vel_xy_exp: 0.5145
Episode_Reward/track_ang_vel_z_exp: 0.3405
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0228
     Episode_Reward/dof_torques_l2: -0.1120
         Episode_Reward/dof_acc_l2: -0.0370
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0142
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 1.00s
                      Time elapsed: 00:11:43
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 650/1000 [0m                      

                       Computation: 12435 steps/s (collection: 0.943s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -2.4359
                       Mean reward: 9.51
               Mean episode length: 873.87
Episode_Reward/track_lin_vel_xy_exp: 0.4445
Episode_Reward/track_ang_vel_z_exp: 0.3551
       Episode_Reward/lin_vel_z_l2: -0.0136
      Episode_Reward/ang_vel_xy_l2: -0.0231
     Episode_Reward/dof_torques_l2: -0.1094
         Episode_Reward/dof_acc_l2: -0.0435
     Episode_Reward/action_rate_l2: -0.0173
      Episode_Reward/feet_air_time: -0.0287
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0200
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7999488
                    Iteration time: 0.99s
                      Time elapsed: 00:11:44
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 651/1000 [0m                      

                       Computation: 12517 steps/s (collection: 0.937s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -2.4622
                       Mean reward: 9.57
               Mean episode length: 864.03
Episode_Reward/track_lin_vel_xy_exp: 0.3727
Episode_Reward/track_ang_vel_z_exp: 0.3400
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0234
     Episode_Reward/dof_torques_l2: -0.1088
         Episode_Reward/dof_acc_l2: -0.0364
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0288
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0205
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 0.98s
                      Time elapsed: 00:11:45
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 652/1000 [0m                      

                       Computation: 12700 steps/s (collection: 0.919s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.4737
                       Mean reward: 10.12
               Mean episode length: 873.61
Episode_Reward/track_lin_vel_xy_exp: 0.4748
Episode_Reward/track_ang_vel_z_exp: 0.2857
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.1019
         Episode_Reward/dof_acc_l2: -0.0341
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8024064
                    Iteration time: 0.97s
                      Time elapsed: 00:11:45
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 653/1000 [0m                      

                       Computation: 11924 steps/s (collection: 0.978s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.4823
                       Mean reward: 9.22
               Mean episode length: 864.18
Episode_Reward/track_lin_vel_xy_exp: 0.2876
Episode_Reward/track_ang_vel_z_exp: 0.2420
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.1039
         Episode_Reward/dof_acc_l2: -0.0281
     Episode_Reward/action_rate_l2: -0.0154
      Episode_Reward/feet_air_time: -0.0227
 Episode_Reward/undesired_contacts: -0.0077
Episode_Reward/flat_orientation_l2: -0.0361
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 1.03s
                      Time elapsed: 00:11:47
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 654/1000 [0m                      

                       Computation: 12133 steps/s (collection: 0.968s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.4936
                       Mean reward: 9.34
               Mean episode length: 865.47
Episode_Reward/track_lin_vel_xy_exp: 0.2850
Episode_Reward/track_ang_vel_z_exp: 0.1892
       Episode_Reward/lin_vel_z_l2: -0.0089
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.0213
     Episode_Reward/action_rate_l2: -0.0134
      Episode_Reward/feet_air_time: -0.0186
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0289
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8048640
                    Iteration time: 1.01s
                      Time elapsed: 00:11:48
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 655/1000 [0m                      

                       Computation: 11578 steps/s (collection: 1.008s, learning 0.053s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.5010
                       Mean reward: 9.01
               Mean episode length: 872.74
Episode_Reward/track_lin_vel_xy_exp: 0.4309
Episode_Reward/track_ang_vel_z_exp: 0.3177
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0247
     Episode_Reward/dof_torques_l2: -0.1125
         Episode_Reward/dof_acc_l2: -0.0411
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0245
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.06s
                      Time elapsed: 00:11:49
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 656/1000 [0m                      

                       Computation: 11629 steps/s (collection: 1.000s, learning 0.056s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.4712
                       Mean reward: 8.59
               Mean episode length: 891.79
Episode_Reward/track_lin_vel_xy_exp: 0.2782
Episode_Reward/track_ang_vel_z_exp: 0.2964
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1066
         Episode_Reward/dof_acc_l2: -0.0304
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0202
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8073216
                    Iteration time: 1.06s
                      Time elapsed: 00:11:50
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 657/1000 [0m                      

                       Computation: 11720 steps/s (collection: 1.000s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.4627
                       Mean reward: 7.86
               Mean episode length: 887.96
Episode_Reward/track_lin_vel_xy_exp: 0.2916
Episode_Reward/track_ang_vel_z_exp: 0.3121
       Episode_Reward/lin_vel_z_l2: -0.0114
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.1150
         Episode_Reward/dof_acc_l2: -0.0303
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0198
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 1.05s
                      Time elapsed: 00:11:51
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 658/1000 [0m                      

                       Computation: 11820 steps/s (collection: 0.995s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.4737
                       Mean reward: 8.55
               Mean episode length: 895.16
Episode_Reward/track_lin_vel_xy_exp: 0.3459
Episode_Reward/track_ang_vel_z_exp: 0.2885
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.1006
         Episode_Reward/dof_acc_l2: -0.0282
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0233
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0190
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8097792
                    Iteration time: 1.04s
                      Time elapsed: 00:11:52
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 659/1000 [0m                      

                       Computation: 11959 steps/s (collection: 0.982s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -2.4478
                       Mean reward: 8.28
               Mean episode length: 868.94
Episode_Reward/track_lin_vel_xy_exp: 0.2565
Episode_Reward/track_ang_vel_z_exp: 0.2446
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.0222
     Episode_Reward/action_rate_l2: -0.0130
      Episode_Reward/feet_air_time: -0.0189
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: -0.0331
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 1.03s
                      Time elapsed: 00:11:53
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 660/1000 [0m                      

                       Computation: 11877 steps/s (collection: 0.972s, learning 0.063s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -2.4451
                       Mean reward: 7.81
               Mean episode length: 853.21
Episode_Reward/track_lin_vel_xy_exp: 0.2033
Episode_Reward/track_ang_vel_z_exp: 0.1932
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0160
     Episode_Reward/dof_torques_l2: -0.0865
         Episode_Reward/dof_acc_l2: -0.0253
     Episode_Reward/action_rate_l2: -0.0151
      Episode_Reward/feet_air_time: -0.0166
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8122368
                    Iteration time: 1.03s
                      Time elapsed: 00:11:54
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 661/1000 [0m                      

                       Computation: 11607 steps/s (collection: 1.013s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -2.4386
                       Mean reward: 8.30
               Mean episode length: 850.34
Episode_Reward/track_lin_vel_xy_exp: 0.4734
Episode_Reward/track_ang_vel_z_exp: 0.3284
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0202
     Episode_Reward/dof_torques_l2: -0.0960
         Episode_Reward/dof_acc_l2: -0.0248
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0218
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0229
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 1.06s
                      Time elapsed: 00:11:55
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 662/1000 [0m                      

                       Computation: 11671 steps/s (collection: 1.006s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -2.4438
                       Mean reward: 8.32
               Mean episode length: 842.03
Episode_Reward/track_lin_vel_xy_exp: 0.1762
Episode_Reward/track_ang_vel_z_exp: 0.3666
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0168
     Episode_Reward/dof_torques_l2: -0.0967
         Episode_Reward/dof_acc_l2: -0.0223
     Episode_Reward/action_rate_l2: -0.0155
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0224
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8146944
                    Iteration time: 1.05s
                      Time elapsed: 00:11:56
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 663/1000 [0m                      

                       Computation: 12135 steps/s (collection: 0.955s, learning 0.058s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -2.4452
                       Mean reward: 8.60
               Mean episode length: 825.54
Episode_Reward/track_lin_vel_xy_exp: 0.4282
Episode_Reward/track_ang_vel_z_exp: 0.2853
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.1117
         Episode_Reward/dof_acc_l2: -0.0343
     Episode_Reward/action_rate_l2: -0.0164
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0271
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.01s
                      Time elapsed: 00:11:57
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 664/1000 [0m                      

                       Computation: 11963 steps/s (collection: 0.982s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.4403
                       Mean reward: 8.81
               Mean episode length: 827.53
Episode_Reward/track_lin_vel_xy_exp: 0.3710
Episode_Reward/track_ang_vel_z_exp: 0.2863
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0273
     Episode_Reward/dof_torques_l2: -0.1136
         Episode_Reward/dof_acc_l2: -0.0369
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0271
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: -0.0261
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8171520
                    Iteration time: 1.03s
                      Time elapsed: 00:11:58
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 665/1000 [0m                      

                       Computation: 12105 steps/s (collection: 0.966s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.4425
                       Mean reward: 8.59
               Mean episode length: 816.97
Episode_Reward/track_lin_vel_xy_exp: 0.2872
Episode_Reward/track_ang_vel_z_exp: 0.3273
       Episode_Reward/lin_vel_z_l2: -0.0151
      Episode_Reward/ang_vel_xy_l2: -0.0222
     Episode_Reward/dof_torques_l2: -0.1078
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0265
 Episode_Reward/undesired_contacts: -0.0141
Episode_Reward/flat_orientation_l2: -0.0295
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 1.02s
                      Time elapsed: 00:11:59
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 666/1000 [0m                      

                       Computation: 11933 steps/s (collection: 0.976s, learning 0.053s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.4559
                       Mean reward: 8.54
               Mean episode length: 821.63
Episode_Reward/track_lin_vel_xy_exp: 0.1752
Episode_Reward/track_ang_vel_z_exp: 0.3732
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0835
         Episode_Reward/dof_acc_l2: -0.0228
     Episode_Reward/action_rate_l2: -0.0145
      Episode_Reward/feet_air_time: -0.0175
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0219
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8196096
                    Iteration time: 1.03s
                      Time elapsed: 00:12:00
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 667/1000 [0m                      

                       Computation: 11830 steps/s (collection: 0.995s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.4492
                       Mean reward: 8.86
               Mean episode length: 840.65
Episode_Reward/track_lin_vel_xy_exp: 0.4900
Episode_Reward/track_ang_vel_z_exp: 0.2415
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0154
     Episode_Reward/dof_torques_l2: -0.1067
         Episode_Reward/dof_acc_l2: -0.0261
     Episode_Reward/action_rate_l2: -0.0173
      Episode_Reward/feet_air_time: -0.0174
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0146
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 1.04s
                      Time elapsed: 00:12:01
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 668/1000 [0m                      

                       Computation: 11882 steps/s (collection: 0.988s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.4297
                       Mean reward: 9.09
               Mean episode length: 865.01
Episode_Reward/track_lin_vel_xy_exp: 0.2647
Episode_Reward/track_ang_vel_z_exp: 0.2796
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.0973
         Episode_Reward/dof_acc_l2: -0.0212
     Episode_Reward/action_rate_l2: -0.0151
      Episode_Reward/feet_air_time: -0.0181
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8220672
                    Iteration time: 1.03s
                      Time elapsed: 00:12:02
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 669/1000 [0m                      

                       Computation: 11628 steps/s (collection: 0.999s, learning 0.057s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -2.3998
                       Mean reward: 8.39
               Mean episode length: 834.67
Episode_Reward/track_lin_vel_xy_exp: 0.3797
Episode_Reward/track_ang_vel_z_exp: 0.2585
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0232
     Episode_Reward/dof_torques_l2: -0.1051
         Episode_Reward/dof_acc_l2: -0.0365
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0249
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: -0.0330
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 1.06s
                      Time elapsed: 00:12:03
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 670/1000 [0m                      

                       Computation: 10688 steps/s (collection: 1.074s, learning 0.075s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -2.4020
                       Mean reward: 8.19
               Mean episode length: 838.07
Episode_Reward/track_lin_vel_xy_exp: 0.3976
Episode_Reward/track_ang_vel_z_exp: 0.1888
       Episode_Reward/lin_vel_z_l2: -0.0152
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.1136
         Episode_Reward/dof_acc_l2: -0.0332
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0188
 Episode_Reward/undesired_contacts: -0.0070
Episode_Reward/flat_orientation_l2: -0.0212
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8245248
                    Iteration time: 1.15s
                      Time elapsed: 00:12:04
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 671/1000 [0m                      

                       Computation: 11609 steps/s (collection: 1.014s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.4126
                       Mean reward: 7.62
               Mean episode length: 844.33
Episode_Reward/track_lin_vel_xy_exp: 0.3029
Episode_Reward/track_ang_vel_z_exp: 0.2848
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.0972
         Episode_Reward/dof_acc_l2: -0.0217
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0250
 Episode_Reward/undesired_contacts: -0.0220
Episode_Reward/flat_orientation_l2: -0.0149
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.06s
                      Time elapsed: 00:12:05
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 672/1000 [0m                      

                       Computation: 11114 steps/s (collection: 1.054s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -2.4145
                       Mean reward: 7.37
               Mean episode length: 844.44
Episode_Reward/track_lin_vel_xy_exp: 0.2289
Episode_Reward/track_ang_vel_z_exp: 0.3104
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.0946
         Episode_Reward/dof_acc_l2: -0.0241
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0159
Episode_Reward/flat_orientation_l2: -0.0208
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8269824
                    Iteration time: 1.11s
                      Time elapsed: 00:12:06
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 673/1000 [0m                      

                       Computation: 10766 steps/s (collection: 1.041s, learning 0.101s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -2.4365
                       Mean reward: 7.39
               Mean episode length: 846.22
Episode_Reward/track_lin_vel_xy_exp: 0.3533
Episode_Reward/track_ang_vel_z_exp: 0.2305
       Episode_Reward/lin_vel_z_l2: -0.0113
      Episode_Reward/ang_vel_xy_l2: -0.0193
     Episode_Reward/dof_torques_l2: -0.1168
         Episode_Reward/dof_acc_l2: -0.0285
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0226
 Episode_Reward/undesired_contacts: -0.0036
Episode_Reward/flat_orientation_l2: -0.0225
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 1.14s
                      Time elapsed: 00:12:08
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 674/1000 [0m                      

                       Computation: 11549 steps/s (collection: 1.017s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -2.4623
                       Mean reward: 7.41
               Mean episode length: 852.89
Episode_Reward/track_lin_vel_xy_exp: 0.5427
Episode_Reward/track_ang_vel_z_exp: 0.3328
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.0218
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0280
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0125
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8294400
                    Iteration time: 1.06s
                      Time elapsed: 00:12:09
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 675/1000 [0m                      

                       Computation: 10864 steps/s (collection: 1.086s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.4794
                       Mean reward: 7.44
               Mean episode length: 870.53
Episode_Reward/track_lin_vel_xy_exp: 0.3687
Episode_Reward/track_ang_vel_z_exp: 0.3110
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1350
         Episode_Reward/dof_acc_l2: -0.0313
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0211
 Episode_Reward/undesired_contacts: -0.0664
Episode_Reward/flat_orientation_l2: -0.0152
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 1.13s
                      Time elapsed: 00:12:10
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 676/1000 [0m                      

                       Computation: 11066 steps/s (collection: 1.061s, learning 0.050s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -2.4792
                       Mean reward: 7.78
               Mean episode length: 898.02
Episode_Reward/track_lin_vel_xy_exp: 0.4992
Episode_Reward/track_ang_vel_z_exp: 0.3360
       Episode_Reward/lin_vel_z_l2: -0.0149
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.1232
         Episode_Reward/dof_acc_l2: -0.0418
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0335
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0159
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8318976
                    Iteration time: 1.11s
                      Time elapsed: 00:12:11
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 677/1000 [0m                      

                       Computation: 11259 steps/s (collection: 1.039s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.4819
                       Mean reward: 7.90
               Mean episode length: 907.72
Episode_Reward/track_lin_vel_xy_exp: 0.2613
Episode_Reward/track_ang_vel_z_exp: 0.2859
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.1043
         Episode_Reward/dof_acc_l2: -0.0198
     Episode_Reward/action_rate_l2: -0.0159
      Episode_Reward/feet_air_time: -0.0190
 Episode_Reward/undesired_contacts: -0.0031
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 1.09s
                      Time elapsed: 00:12:12
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 678/1000 [0m                      

                       Computation: 10831 steps/s (collection: 1.085s, learning 0.050s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -2.4932
                       Mean reward: 8.11
               Mean episode length: 914.84
Episode_Reward/track_lin_vel_xy_exp: 0.3610
Episode_Reward/track_ang_vel_z_exp: 0.3881
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.1024
         Episode_Reward/dof_acc_l2: -0.0347
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0239
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0132
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8343552
                    Iteration time: 1.13s
                      Time elapsed: 00:12:13
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 679/1000 [0m                      

                       Computation: 10683 steps/s (collection: 1.105s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.4945
                       Mean reward: 8.82
               Mean episode length: 928.84
Episode_Reward/track_lin_vel_xy_exp: 0.6523
Episode_Reward/track_ang_vel_z_exp: 0.4317
       Episode_Reward/lin_vel_z_l2: -0.0161
      Episode_Reward/ang_vel_xy_l2: -0.0266
     Episode_Reward/dof_torques_l2: -0.1434
         Episode_Reward/dof_acc_l2: -0.0552
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0344
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0176
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.15s
                      Time elapsed: 00:12:14
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 680/1000 [0m                      

                       Computation: 11407 steps/s (collection: 1.018s, learning 0.059s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -2.4903
                       Mean reward: 9.19
               Mean episode length: 927.40
Episode_Reward/track_lin_vel_xy_exp: 0.3848
Episode_Reward/track_ang_vel_z_exp: 0.2905
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.1002
         Episode_Reward/dof_acc_l2: -0.0257
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0198
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8368128
                    Iteration time: 1.08s
                      Time elapsed: 00:12:15
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 681/1000 [0m                      

                       Computation: 10929 steps/s (collection: 1.078s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0109
                 Mean entropy loss: -2.4996
                       Mean reward: 9.78
               Mean episode length: 940.20
Episode_Reward/track_lin_vel_xy_exp: 0.4882
Episode_Reward/track_ang_vel_z_exp: 0.3504
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0252
     Episode_Reward/dof_torques_l2: -0.1306
         Episode_Reward/dof_acc_l2: -0.0397
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0263
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 1.12s
                      Time elapsed: 00:12:16
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 682/1000 [0m                      

                       Computation: 11296 steps/s (collection: 1.041s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.5209
                       Mean reward: 9.96
               Mean episode length: 940.70
Episode_Reward/track_lin_vel_xy_exp: 0.4004
Episode_Reward/track_ang_vel_z_exp: 0.2890
       Episode_Reward/lin_vel_z_l2: -0.0122
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.1117
         Episode_Reward/dof_acc_l2: -0.0296
     Episode_Reward/action_rate_l2: -0.0179
      Episode_Reward/feet_air_time: -0.0223
 Episode_Reward/undesired_contacts: -0.0044
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8392704
                    Iteration time: 1.09s
                      Time elapsed: 00:12:18
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 683/1000 [0m                      

                       Computation: 11097 steps/s (collection: 1.060s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.5111
                       Mean reward: 9.79
               Mean episode length: 930.29
Episode_Reward/track_lin_vel_xy_exp: 0.2717
Episode_Reward/track_ang_vel_z_exp: 0.3246
       Episode_Reward/lin_vel_z_l2: -0.0107
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.1082
         Episode_Reward/dof_acc_l2: -0.0257
     Episode_Reward/action_rate_l2: -0.0157
      Episode_Reward/feet_air_time: -0.0221
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: -0.0224
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 1.11s
                      Time elapsed: 00:12:19
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 684/1000 [0m                      

                       Computation: 11395 steps/s (collection: 1.034s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -2.5321
                       Mean reward: 10.57
               Mean episode length: 926.37
Episode_Reward/track_lin_vel_xy_exp: 0.5060
Episode_Reward/track_ang_vel_z_exp: 0.3853
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.0977
         Episode_Reward/dof_acc_l2: -0.0238
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0137
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8417280
                    Iteration time: 1.08s
                      Time elapsed: 00:12:20
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 685/1000 [0m                      

                       Computation: 11631 steps/s (collection: 0.992s, learning 0.064s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -2.5432
                       Mean reward: 10.86
               Mean episode length: 935.26
Episode_Reward/track_lin_vel_xy_exp: 0.3249
Episode_Reward/track_ang_vel_z_exp: 0.3662
       Episode_Reward/lin_vel_z_l2: -0.0071
      Episode_Reward/ang_vel_xy_l2: -0.0127
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.0098
     Episode_Reward/action_rate_l2: -0.0158
      Episode_Reward/feet_air_time: -0.0093
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0070
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 1.06s
                      Time elapsed: 00:12:21
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 686/1000 [0m                      

                       Computation: 10566 steps/s (collection: 1.116s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -2.5529
                       Mean reward: 10.69
               Mean episode length: 929.98
Episode_Reward/track_lin_vel_xy_exp: 0.3989
Episode_Reward/track_ang_vel_z_exp: 0.3108
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.1189
         Episode_Reward/dof_acc_l2: -0.0344
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0269
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0190
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8441856
                    Iteration time: 1.16s
                      Time elapsed: 00:12:22
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 687/1000 [0m                      

                       Computation: 11477 steps/s (collection: 1.022s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -2.5649
                       Mean reward: 10.20
               Mean episode length: 924.86
Episode_Reward/track_lin_vel_xy_exp: 0.4455
Episode_Reward/track_ang_vel_z_exp: 0.3039
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1114
         Episode_Reward/dof_acc_l2: -0.0318
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0252
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0127
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.07s
                      Time elapsed: 00:12:23
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 688/1000 [0m                      

                       Computation: 11535 steps/s (collection: 1.021s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -2.5920
                       Mean reward: 10.51
               Mean episode length: 947.97
Episode_Reward/track_lin_vel_xy_exp: 0.4745
Episode_Reward/track_ang_vel_z_exp: 0.3612
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0223
     Episode_Reward/dof_torques_l2: -0.1280
         Episode_Reward/dof_acc_l2: -0.0342
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0251
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0243
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8466432
                    Iteration time: 1.07s
                      Time elapsed: 00:12:24
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 689/1000 [0m                      

                       Computation: 11680 steps/s (collection: 1.007s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0057
                 Mean entropy loss: -2.6206
                       Mean reward: 10.35
               Mean episode length: 936.82
Episode_Reward/track_lin_vel_xy_exp: 0.3009
Episode_Reward/track_ang_vel_z_exp: 0.3364
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.1045
         Episode_Reward/dof_acc_l2: -0.0338
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0228
 Episode_Reward/undesired_contacts: -0.0054
Episode_Reward/flat_orientation_l2: -0.0235
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 1.05s
                      Time elapsed: 00:12:25
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 690/1000 [0m                      

                       Computation: 10722 steps/s (collection: 1.093s, learning 0.053s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.6066
                       Mean reward: 10.21
               Mean episode length: 927.18
Episode_Reward/track_lin_vel_xy_exp: 0.3981
Episode_Reward/track_ang_vel_z_exp: 0.3259
       Episode_Reward/lin_vel_z_l2: -0.0098
      Episode_Reward/ang_vel_xy_l2: -0.0176
     Episode_Reward/dof_torques_l2: -0.0927
         Episode_Reward/dof_acc_l2: -0.0192
     Episode_Reward/action_rate_l2: -0.0147
      Episode_Reward/feet_air_time: -0.0201
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: -0.0246
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8491008
                    Iteration time: 1.15s
                      Time elapsed: 00:12:26
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 691/1000 [0m                      

                       Computation: 11092 steps/s (collection: 1.057s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -2.6058
                       Mean reward: 10.44
               Mean episode length: 917.80
Episode_Reward/track_lin_vel_xy_exp: 0.3881
Episode_Reward/track_ang_vel_z_exp: 0.3703
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0241
     Episode_Reward/dof_torques_l2: -0.1191
         Episode_Reward/dof_acc_l2: -0.0361
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0268
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 1.11s
                      Time elapsed: 00:12:27
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 692/1000 [0m                      

                       Computation: 10679 steps/s (collection: 1.092s, learning 0.059s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -2.6016
                       Mean reward: 9.58
               Mean episode length: 924.40
Episode_Reward/track_lin_vel_xy_exp: 0.4428
Episode_Reward/track_ang_vel_z_exp: 0.3593
       Episode_Reward/lin_vel_z_l2: -0.0160
      Episode_Reward/ang_vel_xy_l2: -0.0246
     Episode_Reward/dof_torques_l2: -0.1245
         Episode_Reward/dof_acc_l2: -0.0473
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0085
Episode_Reward/flat_orientation_l2: -0.0245
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8515584
                    Iteration time: 1.15s
                      Time elapsed: 00:12:29
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 693/1000 [0m                      

                       Computation: 10488 steps/s (collection: 1.113s, learning 0.058s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.5938
                       Mean reward: 9.57
               Mean episode length: 913.99
Episode_Reward/track_lin_vel_xy_exp: 0.5776
Episode_Reward/track_ang_vel_z_exp: 0.3307
       Episode_Reward/lin_vel_z_l2: -0.0150
      Episode_Reward/ang_vel_xy_l2: -0.0243
     Episode_Reward/dof_torques_l2: -0.1151
         Episode_Reward/dof_acc_l2: -0.0421
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0313
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0220
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 1.17s
                      Time elapsed: 00:12:30
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 694/1000 [0m                      

                       Computation: 11241 steps/s (collection: 1.043s, learning 0.050s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.5741
                       Mean reward: 9.73
               Mean episode length: 902.15
Episode_Reward/track_lin_vel_xy_exp: 0.5357
Episode_Reward/track_ang_vel_z_exp: 0.3194
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0234
     Episode_Reward/dof_torques_l2: -0.1123
         Episode_Reward/dof_acc_l2: -0.0360
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0288
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8540160
                    Iteration time: 1.09s
                      Time elapsed: 00:12:31
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 695/1000 [0m                      

                       Computation: 11326 steps/s (collection: 1.034s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -2.5663
                       Mean reward: 9.46
               Mean episode length: 875.19
Episode_Reward/track_lin_vel_xy_exp: 0.2114
Episode_Reward/track_ang_vel_z_exp: 0.2195
       Episode_Reward/lin_vel_z_l2: -0.0081
      Episode_Reward/ang_vel_xy_l2: -0.0124
     Episode_Reward/dof_torques_l2: -0.0706
         Episode_Reward/dof_acc_l2: -0.0231
     Episode_Reward/action_rate_l2: -0.0109
      Episode_Reward/feet_air_time: -0.0128
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.08s
                      Time elapsed: 00:12:32
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 696/1000 [0m                      

                       Computation: 11123 steps/s (collection: 1.059s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.5704
                       Mean reward: 9.54
               Mean episode length: 884.25
Episode_Reward/track_lin_vel_xy_exp: 0.4097
Episode_Reward/track_ang_vel_z_exp: 0.2877
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0248
     Episode_Reward/dof_torques_l2: -0.1096
         Episode_Reward/dof_acc_l2: -0.0394
     Episode_Reward/action_rate_l2: -0.0170
      Episode_Reward/feet_air_time: -0.0245
 Episode_Reward/undesired_contacts: -0.0170
Episode_Reward/flat_orientation_l2: -0.0217
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8564736
                    Iteration time: 1.10s
                      Time elapsed: 00:12:33
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 697/1000 [0m                      

                       Computation: 11382 steps/s (collection: 1.035s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.5708
                       Mean reward: 8.93
               Mean episode length: 883.88
Episode_Reward/track_lin_vel_xy_exp: 0.2449
Episode_Reward/track_ang_vel_z_exp: 0.3138
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.1227
         Episode_Reward/dof_acc_l2: -0.0390
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0233
 Episode_Reward/undesired_contacts: -0.0111
Episode_Reward/flat_orientation_l2: -0.0252
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 1.08s
                      Time elapsed: 00:12:34
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 698/1000 [0m                      

                       Computation: 11252 steps/s (collection: 1.035s, learning 0.057s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.5808
                       Mean reward: 8.45
               Mean episode length: 883.25
Episode_Reward/track_lin_vel_xy_exp: 0.3401
Episode_Reward/track_ang_vel_z_exp: 0.2811
       Episode_Reward/lin_vel_z_l2: -0.0104
      Episode_Reward/ang_vel_xy_l2: -0.0169
     Episode_Reward/dof_torques_l2: -0.1009
         Episode_Reward/dof_acc_l2: -0.0221
     Episode_Reward/action_rate_l2: -0.0158
      Episode_Reward/feet_air_time: -0.0147
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8589312
                    Iteration time: 1.09s
                      Time elapsed: 00:12:35
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 699/1000 [0m                      

                       Computation: 11013 steps/s (collection: 1.065s, learning 0.050s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0058
                 Mean entropy loss: -2.5899
                       Mean reward: 8.34
               Mean episode length: 889.18
Episode_Reward/track_lin_vel_xy_exp: 0.3843
Episode_Reward/track_ang_vel_z_exp: 0.2731
       Episode_Reward/lin_vel_z_l2: -0.0105
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.0988
         Episode_Reward/dof_acc_l2: -0.0255
     Episode_Reward/action_rate_l2: -0.0140
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: -0.0131
Episode_Reward/flat_orientation_l2: -0.0241
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 1.12s
                      Time elapsed: 00:12:36
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 700/1000 [0m                      

                       Computation: 11081 steps/s (collection: 1.064s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -2.5907
                       Mean reward: 8.17
               Mean episode length: 897.43
Episode_Reward/track_lin_vel_xy_exp: 0.3654
Episode_Reward/track_ang_vel_z_exp: 0.3172
       Episode_Reward/lin_vel_z_l2: -0.0147
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.1221
         Episode_Reward/dof_acc_l2: -0.0361
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0276
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0176
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8613888
                    Iteration time: 1.11s
                      Time elapsed: 00:12:37
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 701/1000 [0m                      

                       Computation: 11025 steps/s (collection: 1.069s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -2.5982
                       Mean reward: 8.03
               Mean episode length: 901.25
Episode_Reward/track_lin_vel_xy_exp: 0.2687
Episode_Reward/track_ang_vel_z_exp: 0.2221
       Episode_Reward/lin_vel_z_l2: -0.0090
      Episode_Reward/ang_vel_xy_l2: -0.0164
     Episode_Reward/dof_torques_l2: -0.0939
         Episode_Reward/dof_acc_l2: -0.0167
     Episode_Reward/action_rate_l2: -0.0154
      Episode_Reward/feet_air_time: -0.0158
 Episode_Reward/undesired_contacts: -0.0080
Episode_Reward/flat_orientation_l2: -0.0266
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 1.11s
                      Time elapsed: 00:12:38
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 702/1000 [0m                      

                       Computation: 10971 steps/s (collection: 1.074s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.6081
                       Mean reward: 8.30
               Mean episode length: 919.31
Episode_Reward/track_lin_vel_xy_exp: 0.4870
Episode_Reward/track_ang_vel_z_exp: 0.2304
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.1032
         Episode_Reward/dof_acc_l2: -0.0337
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0221
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0210
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8638464
                    Iteration time: 1.12s
                      Time elapsed: 00:12:40
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 703/1000 [0m                      

                       Computation: 11166 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.5819
                       Mean reward: 8.82
               Mean episode length: 905.69
Episode_Reward/track_lin_vel_xy_exp: 0.4419
Episode_Reward/track_ang_vel_z_exp: 0.3130
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0257
     Episode_Reward/dof_torques_l2: -0.1168
         Episode_Reward/dof_acc_l2: -0.0532
     Episode_Reward/action_rate_l2: -0.0165
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: -0.0052
Episode_Reward/flat_orientation_l2: -0.0265
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.10s
                      Time elapsed: 00:12:41
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 704/1000 [0m                      

                       Computation: 11220 steps/s (collection: 1.049s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0115
                 Mean entropy loss: -2.5686
                       Mean reward: 8.35
               Mean episode length: 884.26
Episode_Reward/track_lin_vel_xy_exp: 0.0805
Episode_Reward/track_ang_vel_z_exp: 0.2067
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.0248
     Episode_Reward/action_rate_l2: -0.0127
      Episode_Reward/feet_air_time: -0.0143
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: -0.0313
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8663040
                    Iteration time: 1.10s
                      Time elapsed: 00:12:42
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 705/1000 [0m                      

                       Computation: 10793 steps/s (collection: 1.094s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.5659
                       Mean reward: 8.67
               Mean episode length: 893.46
Episode_Reward/track_lin_vel_xy_exp: 0.4448
Episode_Reward/track_ang_vel_z_exp: 0.3328
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0228
     Episode_Reward/dof_torques_l2: -0.1177
         Episode_Reward/dof_acc_l2: -0.0400
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0285
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0204
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 1.14s
                      Time elapsed: 00:12:43
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 706/1000 [0m                      

                       Computation: 10959 steps/s (collection: 1.073s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -2.5686
                       Mean reward: 8.52
               Mean episode length: 895.21
Episode_Reward/track_lin_vel_xy_exp: 0.4379
Episode_Reward/track_ang_vel_z_exp: 0.3086
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0269
     Episode_Reward/dof_torques_l2: -0.1146
         Episode_Reward/dof_acc_l2: -0.0375
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0241
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8687616
                    Iteration time: 1.12s
                      Time elapsed: 00:12:44
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 707/1000 [0m                      

                       Computation: 10871 steps/s (collection: 1.077s, learning 0.053s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -2.5739
                       Mean reward: 8.21
               Mean episode length: 886.96
Episode_Reward/track_lin_vel_xy_exp: 0.4519
Episode_Reward/track_ang_vel_z_exp: 0.3188
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.1021
         Episode_Reward/dof_acc_l2: -0.0268
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0266
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0166
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 1.13s
                      Time elapsed: 00:12:45
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 708/1000 [0m                      

                       Computation: 10844 steps/s (collection: 1.084s, learning 0.049s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -2.5558
                       Mean reward: 8.26
               Mean episode length: 890.42
Episode_Reward/track_lin_vel_xy_exp: 0.4692
Episode_Reward/track_ang_vel_z_exp: 0.2137
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.0930
         Episode_Reward/dof_acc_l2: -0.0304
     Episode_Reward/action_rate_l2: -0.0149
      Episode_Reward/feet_air_time: -0.0250
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0179
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8712192
                    Iteration time: 1.13s
                      Time elapsed: 00:12:46
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 709/1000 [0m                      

                       Computation: 11378 steps/s (collection: 1.017s, learning 0.063s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -2.5708
                       Mean reward: 8.65
               Mean episode length: 884.19
Episode_Reward/track_lin_vel_xy_exp: 0.3282
Episode_Reward/track_ang_vel_z_exp: 0.2825
       Episode_Reward/lin_vel_z_l2: -0.0092
      Episode_Reward/ang_vel_xy_l2: -0.0112
     Episode_Reward/dof_torques_l2: -0.1058
         Episode_Reward/dof_acc_l2: -0.0156
     Episode_Reward/action_rate_l2: -0.0122
      Episode_Reward/feet_air_time: -0.0159
 Episode_Reward/undesired_contacts: -0.0275
Episode_Reward/flat_orientation_l2: -0.0179
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 1.08s
                      Time elapsed: 00:12:47
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 710/1000 [0m                      

                       Computation: 11247 steps/s (collection: 1.047s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.5712
                       Mean reward: 8.43
               Mean episode length: 878.39
Episode_Reward/track_lin_vel_xy_exp: 0.4223
Episode_Reward/track_ang_vel_z_exp: 0.3041
       Episode_Reward/lin_vel_z_l2: -0.0205
      Episode_Reward/ang_vel_xy_l2: -0.0257
     Episode_Reward/dof_torques_l2: -0.1295
         Episode_Reward/dof_acc_l2: -0.0586
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0273
 Episode_Reward/undesired_contacts: -0.0111
Episode_Reward/flat_orientation_l2: -0.0246
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8736768
                    Iteration time: 1.09s
                      Time elapsed: 00:12:49
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 711/1000 [0m                      

                       Computation: 11204 steps/s (collection: 1.052s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -2.5658
                       Mean reward: 8.27
               Mean episode length: 898.17
Episode_Reward/track_lin_vel_xy_exp: 0.2639
Episode_Reward/track_ang_vel_z_exp: 0.2542
       Episode_Reward/lin_vel_z_l2: -0.0094
      Episode_Reward/ang_vel_xy_l2: -0.0159
     Episode_Reward/dof_torques_l2: -0.0957
         Episode_Reward/dof_acc_l2: -0.0242
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0181
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: -0.0132
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.10s
                      Time elapsed: 00:12:50
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 712/1000 [0m                      

                       Computation: 10445 steps/s (collection: 1.113s, learning 0.063s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -2.5881
                       Mean reward: 9.45
               Mean episode length: 912.01
Episode_Reward/track_lin_vel_xy_exp: 0.6313
Episode_Reward/track_ang_vel_z_exp: 0.2920
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.1165
         Episode_Reward/dof_acc_l2: -0.0281
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0149
Episode_Reward/flat_orientation_l2: -0.0101
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8761344
                    Iteration time: 1.18s
                      Time elapsed: 00:12:51
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 713/1000 [0m                      

                       Computation: 11298 steps/s (collection: 1.024s, learning 0.063s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -2.5986
                       Mean reward: 9.27
               Mean episode length: 915.99
Episode_Reward/track_lin_vel_xy_exp: 0.5765
Episode_Reward/track_ang_vel_z_exp: 0.3653
       Episode_Reward/lin_vel_z_l2: -0.0102
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.1002
         Episode_Reward/dof_acc_l2: -0.0270
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 1.09s
                      Time elapsed: 00:12:52
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 714/1000 [0m                      

                       Computation: 11191 steps/s (collection: 1.029s, learning 0.068s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.5978
                       Mean reward: 9.05
               Mean episode length: 911.08
Episode_Reward/track_lin_vel_xy_exp: 0.3176
Episode_Reward/track_ang_vel_z_exp: 0.2497
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0164
     Episode_Reward/dof_torques_l2: -0.1061
         Episode_Reward/dof_acc_l2: -0.0214
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0181
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8785920
                    Iteration time: 1.10s
                      Time elapsed: 00:12:53
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 715/1000 [0m                      

                       Computation: 10532 steps/s (collection: 1.117s, learning 0.050s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -2.6173
                       Mean reward: 9.19
               Mean episode length: 904.99
Episode_Reward/track_lin_vel_xy_exp: 0.5298
Episode_Reward/track_ang_vel_z_exp: 0.2349
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.0988
         Episode_Reward/dof_acc_l2: -0.0252
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0184
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 1.17s
                      Time elapsed: 00:12:54
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 716/1000 [0m                      

                       Computation: 10659 steps/s (collection: 1.078s, learning 0.075s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0105
                 Mean entropy loss: -2.6287
                       Mean reward: 8.92
               Mean episode length: 905.60
Episode_Reward/track_lin_vel_xy_exp: 0.0303
Episode_Reward/track_ang_vel_z_exp: 0.2450
       Episode_Reward/lin_vel_z_l2: -0.0067
      Episode_Reward/ang_vel_xy_l2: -0.0138
     Episode_Reward/dof_torques_l2: -0.0771
         Episode_Reward/dof_acc_l2: -0.0082
     Episode_Reward/action_rate_l2: -0.0107
      Episode_Reward/feet_air_time: -0.0071
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0316
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8810496
                    Iteration time: 1.15s
                      Time elapsed: 00:12:55
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 717/1000 [0m                      

                       Computation: 10803 steps/s (collection: 1.090s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0108
                 Mean entropy loss: -2.6230
                       Mean reward: 8.75
               Mean episode length: 906.26
Episode_Reward/track_lin_vel_xy_exp: 0.4921
Episode_Reward/track_ang_vel_z_exp: 0.2568
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0215
     Episode_Reward/dof_torques_l2: -0.1161
         Episode_Reward/dof_acc_l2: -0.0347
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0251
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 1.14s
                      Time elapsed: 00:12:56
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 718/1000 [0m                      

                       Computation: 10827 steps/s (collection: 1.089s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.6408
                       Mean reward: 8.46
               Mean episode length: 903.16
Episode_Reward/track_lin_vel_xy_exp: 0.3541
Episode_Reward/track_ang_vel_z_exp: 0.2166
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0223
     Episode_Reward/dof_torques_l2: -0.1100
         Episode_Reward/dof_acc_l2: -0.0541
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0246
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: -0.0235
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8835072
                    Iteration time: 1.13s
                      Time elapsed: 00:12:58
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 719/1000 [0m                      

                       Computation: 10793 steps/s (collection: 1.092s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.6460
                       Mean reward: 9.04
               Mean episode length: 912.05
Episode_Reward/track_lin_vel_xy_exp: 0.5234
Episode_Reward/track_ang_vel_z_exp: 0.3862
       Episode_Reward/lin_vel_z_l2: -0.0149
      Episode_Reward/ang_vel_xy_l2: -0.0259
     Episode_Reward/dof_torques_l2: -0.1393
         Episode_Reward/dof_acc_l2: -0.0491
     Episode_Reward/action_rate_l2: -0.0193
      Episode_Reward/feet_air_time: -0.0276
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.14s
                      Time elapsed: 00:12:59
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 720/1000 [0m                      

                       Computation: 10640 steps/s (collection: 1.111s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -2.6507
                       Mean reward: 9.38
               Mean episode length: 909.52
Episode_Reward/track_lin_vel_xy_exp: 0.4388
Episode_Reward/track_ang_vel_z_exp: 0.2531
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.0959
         Episode_Reward/dof_acc_l2: -0.0258
     Episode_Reward/action_rate_l2: -0.0157
      Episode_Reward/feet_air_time: -0.0216
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8859648
                    Iteration time: 1.15s
                      Time elapsed: 00:13:00
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 721/1000 [0m                      

                       Computation: 10896 steps/s (collection: 1.072s, learning 0.056s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -2.6595
                       Mean reward: 9.57
               Mean episode length: 893.34
Episode_Reward/track_lin_vel_xy_exp: 0.5459
Episode_Reward/track_ang_vel_z_exp: 0.2969
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.0985
         Episode_Reward/dof_acc_l2: -0.0316
     Episode_Reward/action_rate_l2: -0.0164
      Episode_Reward/feet_air_time: -0.0235
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0137
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 1.13s
                      Time elapsed: 00:13:01
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 722/1000 [0m                      

                       Computation: 10582 steps/s (collection: 1.111s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0098
                 Mean entropy loss: -2.6778
                       Mean reward: 9.99
               Mean episode length: 879.39
Episode_Reward/track_lin_vel_xy_exp: 0.5501
Episode_Reward/track_ang_vel_z_exp: 0.3566
       Episode_Reward/lin_vel_z_l2: -0.0171
      Episode_Reward/ang_vel_xy_l2: -0.0241
     Episode_Reward/dof_torques_l2: -0.1360
         Episode_Reward/dof_acc_l2: -0.0593
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0301
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0159
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8884224
                    Iteration time: 1.16s
                      Time elapsed: 00:13:02
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 723/1000 [0m                      

                       Computation: 11606 steps/s (collection: 1.004s, learning 0.055s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -2.6808
                       Mean reward: 10.92
               Mean episode length: 905.81
Episode_Reward/track_lin_vel_xy_exp: 0.5692
Episode_Reward/track_ang_vel_z_exp: 0.3868
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0247
     Episode_Reward/dof_torques_l2: -0.1433
         Episode_Reward/dof_acc_l2: -0.0448
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0290
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 1.06s
                      Time elapsed: 00:13:03
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 724/1000 [0m                      

                       Computation: 10892 steps/s (collection: 1.078s, learning 0.050s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.6986
                       Mean reward: 10.92
               Mean episode length: 882.60
Episode_Reward/track_lin_vel_xy_exp: 0.3672
Episode_Reward/track_ang_vel_z_exp: 0.2161
       Episode_Reward/lin_vel_z_l2: -0.0101
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.0204
     Episode_Reward/action_rate_l2: -0.0133
      Episode_Reward/feet_air_time: -0.0143
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0209
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8908800
                    Iteration time: 1.13s
                      Time elapsed: 00:13:04
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 725/1000 [0m                      

                       Computation: 10865 steps/s (collection: 1.079s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.7087
                       Mean reward: 10.94
               Mean episode length: 879.43
Episode_Reward/track_lin_vel_xy_exp: 0.5106
Episode_Reward/track_ang_vel_z_exp: 0.2490
       Episode_Reward/lin_vel_z_l2: -0.0109
      Episode_Reward/ang_vel_xy_l2: -0.0166
     Episode_Reward/dof_torques_l2: -0.0992
         Episode_Reward/dof_acc_l2: -0.0245
     Episode_Reward/action_rate_l2: -0.0154
      Episode_Reward/feet_air_time: -0.0174
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 1.13s
                      Time elapsed: 00:13:05
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 726/1000 [0m                      

                       Computation: 10913 steps/s (collection: 1.066s, learning 0.060s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -2.7048
                       Mean reward: 10.40
               Mean episode length: 870.19
Episode_Reward/track_lin_vel_xy_exp: 0.3418
Episode_Reward/track_ang_vel_z_exp: 0.2403
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0206
     Episode_Reward/dof_torques_l2: -0.1027
         Episode_Reward/dof_acc_l2: -0.0300
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0207
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: -0.0200
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8933376
                    Iteration time: 1.13s
                      Time elapsed: 00:13:07
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 727/1000 [0m                      

                       Computation: 11181 steps/s (collection: 1.042s, learning 0.057s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.7033
                       Mean reward: 10.06
               Mean episode length: 867.46
Episode_Reward/track_lin_vel_xy_exp: 0.3907
Episode_Reward/track_ang_vel_z_exp: 0.2583
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0234
     Episode_Reward/dof_torques_l2: -0.1093
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0265
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: -0.0179
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.10s
                      Time elapsed: 00:13:08
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 728/1000 [0m                      

                       Computation: 11390 steps/s (collection: 1.035s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0107
                 Mean entropy loss: -2.7032
                       Mean reward: 9.90
               Mean episode length: 879.52
Episode_Reward/track_lin_vel_xy_exp: 0.2301
Episode_Reward/track_ang_vel_z_exp: 0.1353
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0176
     Episode_Reward/dof_torques_l2: -0.0736
         Episode_Reward/dof_acc_l2: -0.0191
     Episode_Reward/action_rate_l2: -0.0122
      Episode_Reward/feet_air_time: -0.0144
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: -0.0248
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8957952
                    Iteration time: 1.08s
                      Time elapsed: 00:13:09
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 729/1000 [0m                      

                       Computation: 11332 steps/s (collection: 1.019s, learning 0.066s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.6980
                       Mean reward: 9.47
               Mean episode length: 883.65
Episode_Reward/track_lin_vel_xy_exp: 0.3484
Episode_Reward/track_ang_vel_z_exp: 0.2670
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0203
     Episode_Reward/dof_torques_l2: -0.1096
         Episode_Reward/dof_acc_l2: -0.0294
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0200
 Episode_Reward/undesired_contacts: -0.0084
Episode_Reward/flat_orientation_l2: -0.0188
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 1.08s
                      Time elapsed: 00:13:10
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 730/1000 [0m                      

                       Computation: 11039 steps/s (collection: 1.069s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -2.6890
                       Mean reward: 9.76
               Mean episode length: 891.50
Episode_Reward/track_lin_vel_xy_exp: 0.3885
Episode_Reward/track_ang_vel_z_exp: 0.3051
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.1119
         Episode_Reward/dof_acc_l2: -0.0276
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0204
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0170
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8982528
                    Iteration time: 1.11s
                      Time elapsed: 00:13:11
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 731/1000 [0m                      

                       Computation: 11538 steps/s (collection: 1.019s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -2.6967
                       Mean reward: 9.42
               Mean episode length: 880.59
Episode_Reward/track_lin_vel_xy_exp: 0.5767
Episode_Reward/track_ang_vel_z_exp: 0.3332
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0222
     Episode_Reward/dof_torques_l2: -0.1138
         Episode_Reward/dof_acc_l2: -0.0330
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0231
 Episode_Reward/undesired_contacts: -0.0023
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 1.06s
                      Time elapsed: 00:13:12
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 732/1000 [0m                      

                       Computation: 10926 steps/s (collection: 1.024s, learning 0.101s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.6937
                       Mean reward: 9.48
               Mean episode length: 880.37
Episode_Reward/track_lin_vel_xy_exp: 0.5564
Episode_Reward/track_ang_vel_z_exp: 0.3479
       Episode_Reward/lin_vel_z_l2: -0.0202
      Episode_Reward/ang_vel_xy_l2: -0.0298
     Episode_Reward/dof_torques_l2: -0.1369
         Episode_Reward/dof_acc_l2: -0.0584
     Episode_Reward/action_rate_l2: -0.0186
      Episode_Reward/feet_air_time: -0.0307
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0242
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9007104
                    Iteration time: 1.12s
                      Time elapsed: 00:13:13
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 733/1000 [0m                      

                       Computation: 10885 steps/s (collection: 1.077s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -2.7041
                       Mean reward: 9.73
               Mean episode length: 887.67
Episode_Reward/track_lin_vel_xy_exp: 0.4958
Episode_Reward/track_ang_vel_z_exp: 0.2691
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.1015
         Episode_Reward/dof_acc_l2: -0.0370
     Episode_Reward/action_rate_l2: -0.0151
      Episode_Reward/feet_air_time: -0.0255
 Episode_Reward/undesired_contacts: -0.0066
Episode_Reward/flat_orientation_l2: -0.0246
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 1.13s
                      Time elapsed: 00:13:14
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 734/1000 [0m                      

                       Computation: 10756 steps/s (collection: 1.098s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -2.7294
                       Mean reward: 10.03
               Mean episode length: 884.52
Episode_Reward/track_lin_vel_xy_exp: 0.5841
Episode_Reward/track_ang_vel_z_exp: 0.3982
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1220
         Episode_Reward/dof_acc_l2: -0.0350
     Episode_Reward/action_rate_l2: -0.0179
      Episode_Reward/feet_air_time: -0.0318
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: -0.0144
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9031680
                    Iteration time: 1.14s
                      Time elapsed: 00:13:15
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 735/1000 [0m                      

                       Computation: 11007 steps/s (collection: 1.070s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -2.7340
                       Mean reward: 9.97
               Mean episode length: 889.82
Episode_Reward/track_lin_vel_xy_exp: 0.5532
Episode_Reward/track_ang_vel_z_exp: 0.3015
       Episode_Reward/lin_vel_z_l2: -0.0175
      Episode_Reward/ang_vel_xy_l2: -0.0256
     Episode_Reward/dof_torques_l2: -0.1260
         Episode_Reward/dof_acc_l2: -0.0498
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0309
 Episode_Reward/undesired_contacts: -0.0076
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.12s
                      Time elapsed: 00:13:17
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 736/1000 [0m                      

                       Computation: 11556 steps/s (collection: 1.019s, learning 0.045s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.7252
                       Mean reward: 10.38
               Mean episode length: 903.24
Episode_Reward/track_lin_vel_xy_exp: 0.5237
Episode_Reward/track_ang_vel_z_exp: 0.2815
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1162
         Episode_Reward/dof_acc_l2: -0.0372
     Episode_Reward/action_rate_l2: -0.0179
      Episode_Reward/feet_air_time: -0.0260
 Episode_Reward/undesired_contacts: -0.0017
Episode_Reward/flat_orientation_l2: -0.0132
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9056256
                    Iteration time: 1.06s
                      Time elapsed: 00:13:18
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 737/1000 [0m                      

                       Computation: 11199 steps/s (collection: 1.040s, learning 0.057s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.7265
                       Mean reward: 9.80
               Mean episode length: 907.67
Episode_Reward/track_lin_vel_xy_exp: 0.3288
Episode_Reward/track_ang_vel_z_exp: 0.3229
       Episode_Reward/lin_vel_z_l2: -0.0161
      Episode_Reward/ang_vel_xy_l2: -0.0260
     Episode_Reward/dof_torques_l2: -0.1303
         Episode_Reward/dof_acc_l2: -0.0507
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0257
 Episode_Reward/undesired_contacts: -0.0090
Episode_Reward/flat_orientation_l2: -0.0217
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 1.10s
                      Time elapsed: 00:13:19
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 738/1000 [0m                      

                       Computation: 11355 steps/s (collection: 1.038s, learning 0.044s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.7277
                       Mean reward: 9.74
               Mean episode length: 916.49
Episode_Reward/track_lin_vel_xy_exp: 0.4085
Episode_Reward/track_ang_vel_z_exp: 0.2886
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0215
     Episode_Reward/dof_torques_l2: -0.1115
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0243
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: -0.0215
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9080832
                    Iteration time: 1.08s
                      Time elapsed: 00:13:20
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 739/1000 [0m                      

                       Computation: 11163 steps/s (collection: 1.049s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -2.7261
                       Mean reward: 9.96
               Mean episode length: 925.12
Episode_Reward/track_lin_vel_xy_exp: 0.4780
Episode_Reward/track_ang_vel_z_exp: 0.3044
       Episode_Reward/lin_vel_z_l2: -0.0137
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1178
         Episode_Reward/dof_acc_l2: -0.0401
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0276
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0204
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 1.10s
                      Time elapsed: 00:13:21
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 740/1000 [0m                      

                       Computation: 11338 steps/s (collection: 1.036s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.7338
                       Mean reward: 10.78
               Mean episode length: 921.53
Episode_Reward/track_lin_vel_xy_exp: 0.6350
Episode_Reward/track_ang_vel_z_exp: 0.3166
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1208
         Episode_Reward/dof_acc_l2: -0.0425
     Episode_Reward/action_rate_l2: -0.0179
      Episode_Reward/feet_air_time: -0.0268
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0176
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9105408
                    Iteration time: 1.08s
                      Time elapsed: 00:13:22
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 741/1000 [0m                      

                       Computation: 10959 steps/s (collection: 1.075s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.7177
                       Mean reward: 11.11
               Mean episode length: 921.32
Episode_Reward/track_lin_vel_xy_exp: 0.5774
Episode_Reward/track_ang_vel_z_exp: 0.3526
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1171
         Episode_Reward/dof_acc_l2: -0.0394
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0302
 Episode_Reward/undesired_contacts: -0.0074
Episode_Reward/flat_orientation_l2: -0.0187
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 1.12s
                      Time elapsed: 00:13:23
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 742/1000 [0m                      

                       Computation: 11497 steps/s (collection: 1.022s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -2.6824
                       Mean reward: 11.35
               Mean episode length: 926.68
Episode_Reward/track_lin_vel_xy_exp: 0.5668
Episode_Reward/track_ang_vel_z_exp: 0.3415
       Episode_Reward/lin_vel_z_l2: -0.0190
      Episode_Reward/ang_vel_xy_l2: -0.0344
     Episode_Reward/dof_torques_l2: -0.1368
         Episode_Reward/dof_acc_l2: -0.0785
     Episode_Reward/action_rate_l2: -0.0214
      Episode_Reward/feet_air_time: -0.0359
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0263
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 9129984
                    Iteration time: 1.07s
                      Time elapsed: 00:13:24
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 743/1000 [0m                      

                       Computation: 10779 steps/s (collection: 1.089s, learning 0.051s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -2.6926
                       Mean reward: 10.81
               Mean episode length: 915.43
Episode_Reward/track_lin_vel_xy_exp: 0.4967
Episode_Reward/track_ang_vel_z_exp: 0.2814
       Episode_Reward/lin_vel_z_l2: -0.0168
      Episode_Reward/ang_vel_xy_l2: -0.0216
     Episode_Reward/dof_torques_l2: -0.1316
         Episode_Reward/dof_acc_l2: -0.0361
     Episode_Reward/action_rate_l2: -0.0170
      Episode_Reward/feet_air_time: -0.0224
 Episode_Reward/undesired_contacts: -0.0055
Episode_Reward/flat_orientation_l2: -0.0290
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.14s
                      Time elapsed: 00:13:25
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 744/1000 [0m                      

                       Computation: 11007 steps/s (collection: 1.069s, learning 0.047s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.7033
                       Mean reward: 11.49
               Mean episode length: 919.43
Episode_Reward/track_lin_vel_xy_exp: 0.5612
Episode_Reward/track_ang_vel_z_exp: 0.3070
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1018
         Episode_Reward/dof_acc_l2: -0.0388
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0131
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9154560
                    Iteration time: 1.12s
                      Time elapsed: 00:13:26
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 745/1000 [0m                      

                       Computation: 11487 steps/s (collection: 1.021s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -2.7135
                       Mean reward: 11.70
               Mean episode length: 916.85
Episode_Reward/track_lin_vel_xy_exp: 0.4902
Episode_Reward/track_ang_vel_z_exp: 0.3011
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.1173
         Episode_Reward/dof_acc_l2: -0.0479
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0358
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0228
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 1.07s
                      Time elapsed: 00:13:27
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 746/1000 [0m                      

                       Computation: 10636 steps/s (collection: 1.110s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.7107
                       Mean reward: 10.34
               Mean episode length: 902.50
Episode_Reward/track_lin_vel_xy_exp: 0.3935
Episode_Reward/track_ang_vel_z_exp: 0.2195
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.1028
         Episode_Reward/dof_acc_l2: -0.0304
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0184
 Episode_Reward/undesired_contacts: -0.0040
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9179136
                    Iteration time: 1.16s
                      Time elapsed: 00:13:29
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 747/1000 [0m                      

                       Computation: 11444 steps/s (collection: 1.020s, learning 0.054s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -2.7071
                       Mean reward: 9.95
               Mean episode length: 901.85
Episode_Reward/track_lin_vel_xy_exp: 0.4177
Episode_Reward/track_ang_vel_z_exp: 0.3678
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.1097
         Episode_Reward/dof_acc_l2: -0.0366
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0263
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 1.07s
                      Time elapsed: 00:13:30
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 748/1000 [0m                      

                       Computation: 11123 steps/s (collection: 1.047s, learning 0.057s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0057
                 Mean entropy loss: -2.7153
                       Mean reward: 9.85
               Mean episode length: 905.25
Episode_Reward/track_lin_vel_xy_exp: 0.3553
Episode_Reward/track_ang_vel_z_exp: 0.3026
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.1028
         Episode_Reward/dof_acc_l2: -0.0326
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0215
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: -0.0220
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9203712
                    Iteration time: 1.10s
                      Time elapsed: 00:13:31
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 749/1000 [0m                      

                       Computation: 10745 steps/s (collection: 1.098s, learning 0.046s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.7239
                       Mean reward: 9.75
               Mean episode length: 895.18
Episode_Reward/track_lin_vel_xy_exp: 0.3639
Episode_Reward/track_ang_vel_z_exp: 0.2211
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.0349
     Episode_Reward/action_rate_l2: -0.0147
      Episode_Reward/feet_air_time: -0.0224
 Episode_Reward/undesired_contacts: -0.0097
Episode_Reward/flat_orientation_l2: -0.0269
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 1.14s
                      Time elapsed: 00:13:32
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 750/1000 [0m                      

                       Computation: 10739 steps/s (collection: 1.083s, learning 0.061s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -2.7268
                       Mean reward: 10.15
               Mean episode length: 873.36
Episode_Reward/track_lin_vel_xy_exp: 0.4422
Episode_Reward/track_ang_vel_z_exp: 0.3140
       Episode_Reward/lin_vel_z_l2: -0.0121
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.1106
         Episode_Reward/dof_acc_l2: -0.0320
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0229
 Episode_Reward/undesired_contacts: -0.0143
Episode_Reward/flat_orientation_l2: -0.0382
  Episode_Termination/base_contact: 0.8333
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9228288
                    Iteration time: 1.14s
                      Time elapsed: 00:13:33
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 751/1000 [0m                      

                       Computation: 11186 steps/s (collection: 1.044s, learning 0.055s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -2.7319
                       Mean reward: 9.58
               Mean episode length: 876.14
Episode_Reward/track_lin_vel_xy_exp: 0.3354
Episode_Reward/track_ang_vel_z_exp: 0.3290
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1153
         Episode_Reward/dof_acc_l2: -0.0361
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0229
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: -0.0269
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.10s
                      Time elapsed: 00:13:34
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 752/1000 [0m                      

                       Computation: 11856 steps/s (collection: 0.985s, learning 0.052s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -2.7503
                       Mean reward: 8.88
               Mean episode length: 878.78
Episode_Reward/track_lin_vel_xy_exp: 0.3392
Episode_Reward/track_ang_vel_z_exp: 0.3044
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.1147
         Episode_Reward/dof_acc_l2: -0.0397
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0260
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0165
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 9252864
                    Iteration time: 1.04s
                      Time elapsed: 00:13:35
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 753/1000 [0m                      

                       Computation: 11688 steps/s (collection: 1.003s, learning 0.048s)
             Mean action noise std: 0.20
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.7753
                       Mean reward: 8.78
               Mean episode length: 876.27
Episode_Reward/track_lin_vel_xy_exp: 0.3887
Episode_Reward/track_ang_vel_z_exp: 0.3280
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1363
         Episode_Reward/dof_acc_l2: -0.0351
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0305
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 1.05s
                      Time elapsed: 00:13:36
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 754/1000 [0m                      

                       Computation: 10648 steps/s (collection: 1.091s, learning 0.063s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -2.7922
                       Mean reward: 9.06
               Mean episode length: 897.37
Episode_Reward/track_lin_vel_xy_exp: 0.4047
Episode_Reward/track_ang_vel_z_exp: 0.3308
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.1133
         Episode_Reward/dof_acc_l2: -0.0436
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0253
 Episode_Reward/undesired_contacts: -0.0088
Episode_Reward/flat_orientation_l2: -0.0154
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9277440
                    Iteration time: 1.15s
                      Time elapsed: 00:13:37
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 755/1000 [0m                      

                       Computation: 11527 steps/s (collection: 1.006s, learning 0.060s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -2.7984
                       Mean reward: 8.98
               Mean episode length: 881.19
Episode_Reward/track_lin_vel_xy_exp: 0.3407
Episode_Reward/track_ang_vel_z_exp: 0.2650
       Episode_Reward/lin_vel_z_l2: -0.0092
      Episode_Reward/ang_vel_xy_l2: -0.0164
     Episode_Reward/dof_torques_l2: -0.0692
         Episode_Reward/dof_acc_l2: -0.0207
     Episode_Reward/action_rate_l2: -0.0120
      Episode_Reward/feet_air_time: -0.0156
 Episode_Reward/undesired_contacts: -0.0022
Episode_Reward/flat_orientation_l2: -0.0193
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 1.07s
                      Time elapsed: 00:13:39
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 756/1000 [0m                      

                       Computation: 11239 steps/s (collection: 1.037s, learning 0.056s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0110
                 Mean entropy loss: -2.8108
                       Mean reward: 9.35
               Mean episode length: 888.79
Episode_Reward/track_lin_vel_xy_exp: 0.5829
Episode_Reward/track_ang_vel_z_exp: 0.3607
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.1238
         Episode_Reward/dof_acc_l2: -0.0390
     Episode_Reward/action_rate_l2: -0.0186
      Episode_Reward/feet_air_time: -0.0242
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0115
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9302016
                    Iteration time: 1.09s
                      Time elapsed: 00:13:40
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 757/1000 [0m                      

                       Computation: 11008 steps/s (collection: 1.069s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.8295
                       Mean reward: 9.82
               Mean episode length: 900.25
Episode_Reward/track_lin_vel_xy_exp: 0.5269
Episode_Reward/track_ang_vel_z_exp: 0.3092
       Episode_Reward/lin_vel_z_l2: -0.0178
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.1345
         Episode_Reward/dof_acc_l2: -0.0536
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0276
 Episode_Reward/undesired_contacts: -0.0085
Episode_Reward/flat_orientation_l2: -0.0216
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 1.12s
                      Time elapsed: 00:13:41
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 758/1000 [0m                      

                       Computation: 10864 steps/s (collection: 1.086s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -2.8270
                       Mean reward: 9.48
               Mean episode length: 874.00
Episode_Reward/track_lin_vel_xy_exp: 0.3859
Episode_Reward/track_ang_vel_z_exp: 0.2750
       Episode_Reward/lin_vel_z_l2: -0.0161
      Episode_Reward/ang_vel_xy_l2: -0.0223
     Episode_Reward/dof_torques_l2: -0.1193
         Episode_Reward/dof_acc_l2: -0.0571
     Episode_Reward/action_rate_l2: -0.0147
      Episode_Reward/feet_air_time: -0.0235
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: -0.0230
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9326592
                    Iteration time: 1.13s
                      Time elapsed: 00:13:42
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 759/1000 [0m                      

                       Computation: 10321 steps/s (collection: 1.146s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0098
                 Mean entropy loss: -2.8223
                       Mean reward: 9.59
               Mean episode length: 902.07
Episode_Reward/track_lin_vel_xy_exp: 0.3984
Episode_Reward/track_ang_vel_z_exp: 0.3823
       Episode_Reward/lin_vel_z_l2: -0.0183
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.1378
         Episode_Reward/dof_acc_l2: -0.0541
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0341
 Episode_Reward/undesired_contacts: -0.0028
Episode_Reward/flat_orientation_l2: -0.0274
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.19s
                      Time elapsed: 00:13:43
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 760/1000 [0m                      

                       Computation: 10843 steps/s (collection: 1.088s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -2.8403
                       Mean reward: 10.09
               Mean episode length: 902.07
Episode_Reward/track_lin_vel_xy_exp: 0.6339
Episode_Reward/track_ang_vel_z_exp: 0.3879
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0241
     Episode_Reward/dof_torques_l2: -0.1346
         Episode_Reward/dof_acc_l2: -0.0463
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0324
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9351168
                    Iteration time: 1.13s
                      Time elapsed: 00:13:44
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 761/1000 [0m                      

                       Computation: 10529 steps/s (collection: 1.093s, learning 0.074s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.8716
                       Mean reward: 10.04
               Mean episode length: 891.64
Episode_Reward/track_lin_vel_xy_exp: 0.3629
Episode_Reward/track_ang_vel_z_exp: 0.2827
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0223
     Episode_Reward/dof_torques_l2: -0.1068
         Episode_Reward/dof_acc_l2: -0.0327
     Episode_Reward/action_rate_l2: -0.0149
      Episode_Reward/feet_air_time: -0.0211
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0250
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 1.17s
                      Time elapsed: 00:13:45
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 762/1000 [0m                      

                       Computation: 10870 steps/s (collection: 1.061s, learning 0.070s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -2.8839
                       Mean reward: 10.84
               Mean episode length: 902.62
Episode_Reward/track_lin_vel_xy_exp: 0.5777
Episode_Reward/track_ang_vel_z_exp: 0.3495
       Episode_Reward/lin_vel_z_l2: -0.0201
      Episode_Reward/ang_vel_xy_l2: -0.0276
     Episode_Reward/dof_torques_l2: -0.1297
         Episode_Reward/dof_acc_l2: -0.0611
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0322
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0213
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9375744
                    Iteration time: 1.13s
                      Time elapsed: 00:13:46
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 763/1000 [0m                      

                       Computation: 10874 steps/s (collection: 1.082s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -2.9216
                       Mean reward: 10.16
               Mean episode length: 889.61
Episode_Reward/track_lin_vel_xy_exp: 0.3168
Episode_Reward/track_ang_vel_z_exp: 0.2668
       Episode_Reward/lin_vel_z_l2: -0.0138
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.1049
         Episode_Reward/dof_acc_l2: -0.0333
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0209
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: -0.0195
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 1.13s
                      Time elapsed: 00:13:48
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 764/1000 [0m                      

                       Computation: 10368 steps/s (collection: 1.140s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0074
                 Mean entropy loss: -2.9344
                       Mean reward: 10.11
               Mean episode length: 894.43
Episode_Reward/track_lin_vel_xy_exp: 0.5609
Episode_Reward/track_ang_vel_z_exp: 0.3786
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0225
     Episode_Reward/dof_torques_l2: -0.1281
         Episode_Reward/dof_acc_l2: -0.0427
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0263
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: -0.0109
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9400320
                    Iteration time: 1.19s
                      Time elapsed: 00:13:49
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 765/1000 [0m                      

                       Computation: 11354 steps/s (collection: 1.030s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.9303
                       Mean reward: 9.89
               Mean episode length: 908.38
Episode_Reward/track_lin_vel_xy_exp: 0.4451
Episode_Reward/track_ang_vel_z_exp: 0.3368
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.1329
         Episode_Reward/dof_acc_l2: -0.0414
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0243
 Episode_Reward/undesired_contacts: -0.0248
Episode_Reward/flat_orientation_l2: -0.0235
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 1.08s
                      Time elapsed: 00:13:50
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 766/1000 [0m                      

                       Computation: 10988 steps/s (collection: 1.072s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -2.9145
                       Mean reward: 9.81
               Mean episode length: 880.25
Episode_Reward/track_lin_vel_xy_exp: 0.2867
Episode_Reward/track_ang_vel_z_exp: 0.1771
       Episode_Reward/lin_vel_z_l2: -0.0112
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.0244
     Episode_Reward/action_rate_l2: -0.0130
      Episode_Reward/feet_air_time: -0.0178
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0221
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9424896
                    Iteration time: 1.12s
                      Time elapsed: 00:13:51
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 767/1000 [0m                      

                       Computation: 10524 steps/s (collection: 1.093s, learning 0.075s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -2.9051
                       Mean reward: 9.66
               Mean episode length: 865.93
Episode_Reward/track_lin_vel_xy_exp: 0.6103
Episode_Reward/track_ang_vel_z_exp: 0.3805
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0254
     Episode_Reward/dof_torques_l2: -0.1371
         Episode_Reward/dof_acc_l2: -0.0492
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0291
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0146
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.17s
                      Time elapsed: 00:13:52
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 768/1000 [0m                      

                       Computation: 10765 steps/s (collection: 1.080s, learning 0.061s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0114
                 Mean entropy loss: -2.8972
                       Mean reward: 9.36
               Mean episode length: 846.23
Episode_Reward/track_lin_vel_xy_exp: 0.5101
Episode_Reward/track_ang_vel_z_exp: 0.2654
       Episode_Reward/lin_vel_z_l2: -0.0136
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.1068
         Episode_Reward/dof_acc_l2: -0.0407
     Episode_Reward/action_rate_l2: -0.0158
      Episode_Reward/feet_air_time: -0.0284
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9449472
                    Iteration time: 1.14s
                      Time elapsed: 00:13:53
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 769/1000 [0m                      

                       Computation: 10691 steps/s (collection: 1.104s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.8924
                       Mean reward: 9.27
               Mean episode length: 836.83
Episode_Reward/track_lin_vel_xy_exp: 0.3283
Episode_Reward/track_ang_vel_z_exp: 0.2132
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.0985
         Episode_Reward/dof_acc_l2: -0.0314
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0243
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0269
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 1.15s
                      Time elapsed: 00:13:54
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 770/1000 [0m                      

                       Computation: 11282 steps/s (collection: 1.037s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0098
                 Mean entropy loss: -2.8859
                       Mean reward: 9.98
               Mean episode length: 852.28
Episode_Reward/track_lin_vel_xy_exp: 0.4671
Episode_Reward/track_ang_vel_z_exp: 0.3619
       Episode_Reward/lin_vel_z_l2: -0.0163
      Episode_Reward/ang_vel_xy_l2: -0.0229
     Episode_Reward/dof_torques_l2: -0.1203
         Episode_Reward/dof_acc_l2: -0.0407
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0305
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0274
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9474048
                    Iteration time: 1.09s
                      Time elapsed: 00:13:56
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 771/1000 [0m                      

                       Computation: 11304 steps/s (collection: 1.034s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -2.9027
                       Mean reward: 10.28
               Mean episode length: 860.80
Episode_Reward/track_lin_vel_xy_exp: 0.7308
Episode_Reward/track_ang_vel_z_exp: 0.4041
       Episode_Reward/lin_vel_z_l2: -0.0150
      Episode_Reward/ang_vel_xy_l2: -0.0267
     Episode_Reward/dof_torques_l2: -0.1371
         Episode_Reward/dof_acc_l2: -0.0569
     Episode_Reward/action_rate_l2: -0.0202
      Episode_Reward/feet_air_time: -0.0353
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0162
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 1.09s
                      Time elapsed: 00:13:57
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 772/1000 [0m                      

                       Computation: 11112 steps/s (collection: 1.061s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0064
                 Mean entropy loss: -2.9083
                       Mean reward: 10.31
               Mean episode length: 864.09
Episode_Reward/track_lin_vel_xy_exp: 0.2613
Episode_Reward/track_ang_vel_z_exp: 0.3267
       Episode_Reward/lin_vel_z_l2: -0.0096
      Episode_Reward/ang_vel_xy_l2: -0.0134
     Episode_Reward/dof_torques_l2: -0.0973
         Episode_Reward/dof_acc_l2: -0.0210
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0197
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0126
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9498624
                    Iteration time: 1.11s
                      Time elapsed: 00:13:58
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 773/1000 [0m                      

                       Computation: 11559 steps/s (collection: 1.010s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.9146
                       Mean reward: 10.15
               Mean episode length: 859.06
Episode_Reward/track_lin_vel_xy_exp: 0.5351
Episode_Reward/track_ang_vel_z_exp: 0.3491
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0179
     Episode_Reward/dof_torques_l2: -0.1221
         Episode_Reward/dof_acc_l2: -0.0381
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0299
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0149
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 1.06s
                      Time elapsed: 00:13:59
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 774/1000 [0m                      

                       Computation: 10952 steps/s (collection: 1.066s, learning 0.056s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -2.9100
                       Mean reward: 10.35
               Mean episode length: 893.10
Episode_Reward/track_lin_vel_xy_exp: 0.3803
Episode_Reward/track_ang_vel_z_exp: 0.3512
       Episode_Reward/lin_vel_z_l2: -0.0128
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.1249
         Episode_Reward/dof_acc_l2: -0.0449
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0263
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0139
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9523200
                    Iteration time: 1.12s
                      Time elapsed: 00:14:00
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 775/1000 [0m                      

                       Computation: 10923 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0111
                 Mean entropy loss: -2.9220
                       Mean reward: 10.46
               Mean episode length: 897.16
Episode_Reward/track_lin_vel_xy_exp: 0.6565
Episode_Reward/track_ang_vel_z_exp: 0.3942
       Episode_Reward/lin_vel_z_l2: -0.0176
      Episode_Reward/ang_vel_xy_l2: -0.0269
     Episode_Reward/dof_torques_l2: -0.1356
         Episode_Reward/dof_acc_l2: -0.0522
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0321
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: -0.0178
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.12s
                      Time elapsed: 00:14:01
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 776/1000 [0m                      

                       Computation: 11407 steps/s (collection: 1.027s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -2.9272
                       Mean reward: 10.39
               Mean episode length: 908.80
Episode_Reward/track_lin_vel_xy_exp: 0.2921
Episode_Reward/track_ang_vel_z_exp: 0.2329
       Episode_Reward/lin_vel_z_l2: -0.0132
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.0877
         Episode_Reward/dof_acc_l2: -0.0302
     Episode_Reward/action_rate_l2: -0.0120
      Episode_Reward/feet_air_time: -0.0183
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0295
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9547776
                    Iteration time: 1.08s
                      Time elapsed: 00:14:02
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 777/1000 [0m                      

                       Computation: 11342 steps/s (collection: 1.028s, learning 0.055s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -2.9285
                       Mean reward: 10.62
               Mean episode length: 921.13
Episode_Reward/track_lin_vel_xy_exp: 0.3468
Episode_Reward/track_ang_vel_z_exp: 0.3191
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0268
     Episode_Reward/dof_torques_l2: -0.1140
         Episode_Reward/dof_acc_l2: -0.0615
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0234
 Episode_Reward/undesired_contacts: -0.0059
Episode_Reward/flat_orientation_l2: -0.0246
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 1.08s
                      Time elapsed: 00:14:03
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 778/1000 [0m                      

                       Computation: 11144 steps/s (collection: 1.056s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -2.9099
                       Mean reward: 10.49
               Mean episode length: 918.87
Episode_Reward/track_lin_vel_xy_exp: 0.5607
Episode_Reward/track_ang_vel_z_exp: 0.3040
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0206
     Episode_Reward/dof_torques_l2: -0.1379
         Episode_Reward/dof_acc_l2: -0.0421
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0274
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0238
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9572352
                    Iteration time: 1.10s
                      Time elapsed: 00:14:04
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 779/1000 [0m                      

                       Computation: 10803 steps/s (collection: 1.088s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -2.9189
                       Mean reward: 10.41
               Mean episode length: 914.72
Episode_Reward/track_lin_vel_xy_exp: 0.3950
Episode_Reward/track_ang_vel_z_exp: 0.3051
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0240
     Episode_Reward/dof_torques_l2: -0.1093
         Episode_Reward/dof_acc_l2: -0.0432
     Episode_Reward/action_rate_l2: -0.0164
      Episode_Reward/feet_air_time: -0.0237
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: -0.0249
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 1.14s
                      Time elapsed: 00:14:05
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 780/1000 [0m                      

                       Computation: 10493 steps/s (collection: 1.116s, learning 0.055s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -2.9129
                       Mean reward: 11.10
               Mean episode length: 913.33
Episode_Reward/track_lin_vel_xy_exp: 0.6628
Episode_Reward/track_ang_vel_z_exp: 0.3608
       Episode_Reward/lin_vel_z_l2: -0.0137
      Episode_Reward/ang_vel_xy_l2: -0.0240
     Episode_Reward/dof_torques_l2: -0.1261
         Episode_Reward/dof_acc_l2: -0.0440
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0321
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: -0.0128
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9596928
                    Iteration time: 1.17s
                      Time elapsed: 00:14:07
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 781/1000 [0m                      

                       Computation: 11353 steps/s (collection: 1.035s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -2.8901
                       Mean reward: 11.26
               Mean episode length: 913.60
Episode_Reward/track_lin_vel_xy_exp: 0.5300
Episode_Reward/track_ang_vel_z_exp: 0.3785
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.1234
         Episode_Reward/dof_acc_l2: -0.0342
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0235
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0134
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 1.08s
                      Time elapsed: 00:14:08
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 782/1000 [0m                      

                       Computation: 11299 steps/s (collection: 1.037s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0110
                 Mean entropy loss: -2.9024
                       Mean reward: 11.35
               Mean episode length: 916.60
Episode_Reward/track_lin_vel_xy_exp: 0.5717
Episode_Reward/track_ang_vel_z_exp: 0.3026
       Episode_Reward/lin_vel_z_l2: -0.0111
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1080
         Episode_Reward/dof_acc_l2: -0.0338
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0228
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9621504
                    Iteration time: 1.09s
                      Time elapsed: 00:14:09
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 783/1000 [0m                      

                       Computation: 11673 steps/s (collection: 0.995s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -2.9074
                       Mean reward: 11.65
               Mean episode length: 930.08
Episode_Reward/track_lin_vel_xy_exp: 0.4640
Episode_Reward/track_ang_vel_z_exp: 0.3486
       Episode_Reward/lin_vel_z_l2: -0.0227
      Episode_Reward/ang_vel_xy_l2: -0.0347
     Episode_Reward/dof_torques_l2: -0.1578
         Episode_Reward/dof_acc_l2: -0.0846
     Episode_Reward/action_rate_l2: -0.0216
      Episode_Reward/feet_air_time: -0.0415
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: -0.0249
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.05s
                      Time elapsed: 00:14:10
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 784/1000 [0m                      

                       Computation: 11934 steps/s (collection: 0.980s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -2.9194
                       Mean reward: 12.07
               Mean episode length: 938.85
Episode_Reward/track_lin_vel_xy_exp: 0.4738
Episode_Reward/track_ang_vel_z_exp: 0.2708
       Episode_Reward/lin_vel_z_l2: -0.0152
      Episode_Reward/ang_vel_xy_l2: -0.0258
     Episode_Reward/dof_torques_l2: -0.1262
         Episode_Reward/dof_acc_l2: -0.0452
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0257
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0190
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9646080
                    Iteration time: 1.03s
                      Time elapsed: 00:14:11
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 785/1000 [0m                      

                       Computation: 11553 steps/s (collection: 1.015s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.9215
                       Mean reward: 11.96
               Mean episode length: 951.96
Episode_Reward/track_lin_vel_xy_exp: 0.5357
Episode_Reward/track_ang_vel_z_exp: 0.3106
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0262
     Episode_Reward/dof_torques_l2: -0.1305
         Episode_Reward/dof_acc_l2: -0.0491
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0292
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: -0.0216
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 1.06s
                      Time elapsed: 00:14:12
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 786/1000 [0m                      

                       Computation: 11511 steps/s (collection: 1.021s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -2.9302
                       Mean reward: 11.90
               Mean episode length: 937.33
Episode_Reward/track_lin_vel_xy_exp: 0.5757
Episode_Reward/track_ang_vel_z_exp: 0.2839
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0241
     Episode_Reward/dof_torques_l2: -0.1132
         Episode_Reward/dof_acc_l2: -0.0535
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0295
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9670656
                    Iteration time: 1.07s
                      Time elapsed: 00:14:13
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 787/1000 [0m                      

                       Computation: 10809 steps/s (collection: 1.085s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0105
                 Mean entropy loss: -2.9252
                       Mean reward: 11.40
               Mean episode length: 927.26
Episode_Reward/track_lin_vel_xy_exp: 0.2885
Episode_Reward/track_ang_vel_z_exp: 0.1901
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.0257
     Episode_Reward/action_rate_l2: -0.0129
      Episode_Reward/feet_air_time: -0.0169
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: -0.0206
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 1.14s
                      Time elapsed: 00:14:14
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 788/1000 [0m                      

                       Computation: 11193 steps/s (collection: 1.052s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0071
                 Mean entropy loss: -2.9060
                       Mean reward: 11.37
               Mean episode length: 924.65
Episode_Reward/track_lin_vel_xy_exp: 0.4879
Episode_Reward/track_ang_vel_z_exp: 0.3822
       Episode_Reward/lin_vel_z_l2: -0.0190
      Episode_Reward/ang_vel_xy_l2: -0.0257
     Episode_Reward/dof_torques_l2: -0.1356
         Episode_Reward/dof_acc_l2: -0.0535
     Episode_Reward/action_rate_l2: -0.0187
      Episode_Reward/feet_air_time: -0.0288
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9695232
                    Iteration time: 1.10s
                      Time elapsed: 00:14:15
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 789/1000 [0m                      

                       Computation: 11170 steps/s (collection: 1.055s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -2.9119
                       Mean reward: 10.83
               Mean episode length: 906.49
Episode_Reward/track_lin_vel_xy_exp: 0.5069
Episode_Reward/track_ang_vel_z_exp: 0.3228
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0248
     Episode_Reward/dof_torques_l2: -0.1376
         Episode_Reward/dof_acc_l2: -0.0548
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0290
 Episode_Reward/undesired_contacts: -0.0081
Episode_Reward/flat_orientation_l2: -0.0165
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 1.10s
                      Time elapsed: 00:14:16
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 790/1000 [0m                      

                       Computation: 11029 steps/s (collection: 1.064s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0109
                 Mean entropy loss: -2.9293
                       Mean reward: 10.08
               Mean episode length: 884.99
Episode_Reward/track_lin_vel_xy_exp: 0.3479
Episode_Reward/track_ang_vel_z_exp: 0.1824
       Episode_Reward/lin_vel_z_l2: -0.0117
      Episode_Reward/ang_vel_xy_l2: -0.0177
     Episode_Reward/dof_torques_l2: -0.0862
         Episode_Reward/dof_acc_l2: -0.0280
     Episode_Reward/action_rate_l2: -0.0149
      Episode_Reward/feet_air_time: -0.0212
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9719808
                    Iteration time: 1.11s
                      Time elapsed: 00:14:17
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 791/1000 [0m                      

                       Computation: 11675 steps/s (collection: 1.001s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0105
                 Mean entropy loss: -2.9552
                       Mean reward: 9.60
               Mean episode length: 871.50
Episode_Reward/track_lin_vel_xy_exp: 0.2744
Episode_Reward/track_ang_vel_z_exp: 0.3508
       Episode_Reward/lin_vel_z_l2: -0.0174
      Episode_Reward/ang_vel_xy_l2: -0.0220
     Episode_Reward/dof_torques_l2: -0.1280
         Episode_Reward/dof_acc_l2: -0.0342
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0240
 Episode_Reward/undesired_contacts: -0.0041
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.05s
                      Time elapsed: 00:14:18
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 792/1000 [0m                      

                       Computation: 10789 steps/s (collection: 1.089s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -3.0003
                       Mean reward: 9.21
               Mean episode length: 865.58
Episode_Reward/track_lin_vel_xy_exp: 0.2429
Episode_Reward/track_ang_vel_z_exp: 0.3148
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0155
     Episode_Reward/dof_torques_l2: -0.1314
         Episode_Reward/dof_acc_l2: -0.0267
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: -0.0257
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 9744384
                    Iteration time: 1.14s
                      Time elapsed: 00:14:20
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 793/1000 [0m                      

                       Computation: 11244 steps/s (collection: 1.034s, learning 0.059s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -3.0183
                       Mean reward: 9.29
               Mean episode length: 857.76
Episode_Reward/track_lin_vel_xy_exp: 0.4908
Episode_Reward/track_ang_vel_z_exp: 0.3212
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0214
     Episode_Reward/dof_torques_l2: -0.1041
         Episode_Reward/dof_acc_l2: -0.0383
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0255
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0154
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 1.09s
                      Time elapsed: 00:14:21
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 794/1000 [0m                      

                       Computation: 10950 steps/s (collection: 1.078s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -3.0346
                       Mean reward: 9.66
               Mean episode length: 870.74
Episode_Reward/track_lin_vel_xy_exp: 0.4177
Episode_Reward/track_ang_vel_z_exp: 0.2983
       Episode_Reward/lin_vel_z_l2: -0.0123
      Episode_Reward/ang_vel_xy_l2: -0.0188
     Episode_Reward/dof_torques_l2: -0.1085
         Episode_Reward/dof_acc_l2: -0.0378
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0243
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0130
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9768960
                    Iteration time: 1.12s
                      Time elapsed: 00:14:22
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 795/1000 [0m                      

                       Computation: 11433 steps/s (collection: 1.030s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.0401
                       Mean reward: 9.52
               Mean episode length: 859.81
Episode_Reward/track_lin_vel_xy_exp: 0.5047
Episode_Reward/track_ang_vel_z_exp: 0.2105
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.1198
         Episode_Reward/dof_acc_l2: -0.0524
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0196
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 1.07s
                      Time elapsed: 00:14:23
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 796/1000 [0m                      

                       Computation: 11268 steps/s (collection: 1.042s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -3.0251
                       Mean reward: 9.36
               Mean episode length: 853.49
Episode_Reward/track_lin_vel_xy_exp: 0.5163
Episode_Reward/track_ang_vel_z_exp: 0.3039
       Episode_Reward/lin_vel_z_l2: -0.0210
      Episode_Reward/ang_vel_xy_l2: -0.0254
     Episode_Reward/dof_torques_l2: -0.1250
         Episode_Reward/dof_acc_l2: -0.0753
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0282
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0225
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9793536
                    Iteration time: 1.09s
                      Time elapsed: 00:14:24
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 797/1000 [0m                      

                       Computation: 11215 steps/s (collection: 1.047s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.0049
                       Mean reward: 9.43
               Mean episode length: 841.28
Episode_Reward/track_lin_vel_xy_exp: 0.3695
Episode_Reward/track_ang_vel_z_exp: 0.2543
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.1004
         Episode_Reward/dof_acc_l2: -0.0268
     Episode_Reward/action_rate_l2: -0.0146
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 1.10s
                      Time elapsed: 00:14:25
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 798/1000 [0m                      

                       Computation: 11286 steps/s (collection: 1.039s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0065
                 Mean entropy loss: -3.0112
                       Mean reward: 9.61
               Mean episode length: 862.78
Episode_Reward/track_lin_vel_xy_exp: 0.4474
Episode_Reward/track_ang_vel_z_exp: 0.2848
       Episode_Reward/lin_vel_z_l2: -0.0122
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1292
         Episode_Reward/dof_acc_l2: -0.0461
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0312
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0191
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9818112
                    Iteration time: 1.09s
                      Time elapsed: 00:14:26
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 799/1000 [0m                      

                       Computation: 11337 steps/s (collection: 1.026s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.0070
                       Mean reward: 9.89
               Mean episode length: 872.88
Episode_Reward/track_lin_vel_xy_exp: 0.4429
Episode_Reward/track_ang_vel_z_exp: 0.2838
       Episode_Reward/lin_vel_z_l2: -0.0151
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.1305
         Episode_Reward/dof_acc_l2: -0.0389
     Episode_Reward/action_rate_l2: -0.0164
      Episode_Reward/feet_air_time: -0.0228
 Episode_Reward/undesired_contacts: -0.0206
Episode_Reward/flat_orientation_l2: -0.0177
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.08s
                      Time elapsed: 00:14:27
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 800/1000 [0m                      

                       Computation: 11022 steps/s (collection: 1.069s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -3.0002
                       Mean reward: 9.47
               Mean episode length: 871.31
Episode_Reward/track_lin_vel_xy_exp: 0.3007
Episode_Reward/track_ang_vel_z_exp: 0.2500
       Episode_Reward/lin_vel_z_l2: -0.0126
      Episode_Reward/ang_vel_xy_l2: -0.0190
     Episode_Reward/dof_torques_l2: -0.1462
         Episode_Reward/dof_acc_l2: -0.0282
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0185
 Episode_Reward/undesired_contacts: -0.0372
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9842688
                    Iteration time: 1.11s
                      Time elapsed: 00:14:28
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 801/1000 [0m                      

                       Computation: 10496 steps/s (collection: 1.118s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0061
                 Mean entropy loss: -2.9881
                       Mean reward: 9.36
               Mean episode length: 879.26
Episode_Reward/track_lin_vel_xy_exp: 0.5840
Episode_Reward/track_ang_vel_z_exp: 0.2798
       Episode_Reward/lin_vel_z_l2: -0.0190
      Episode_Reward/ang_vel_xy_l2: -0.0306
     Episode_Reward/dof_torques_l2: -0.1380
         Episode_Reward/dof_acc_l2: -0.0540
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0322
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0230
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 1.17s
                      Time elapsed: 00:14:30
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 802/1000 [0m                      

                       Computation: 10749 steps/s (collection: 1.085s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0063
                 Mean entropy loss: -2.9808
                       Mean reward: 8.77
               Mean episode length: 866.66
Episode_Reward/track_lin_vel_xy_exp: 0.3143
Episode_Reward/track_ang_vel_z_exp: 0.2427
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.1049
         Episode_Reward/dof_acc_l2: -0.0521
     Episode_Reward/action_rate_l2: -0.0140
      Episode_Reward/feet_air_time: -0.0240
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0274
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9867264
                    Iteration time: 1.14s
                      Time elapsed: 00:14:31
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 803/1000 [0m                      

                       Computation: 11267 steps/s (collection: 1.028s, learning 0.062s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -2.9869
                       Mean reward: 8.12
               Mean episode length: 847.43
Episode_Reward/track_lin_vel_xy_exp: 0.3085
Episode_Reward/track_ang_vel_z_exp: 0.1982
       Episode_Reward/lin_vel_z_l2: -0.0141
      Episode_Reward/ang_vel_xy_l2: -0.0237
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.0419
     Episode_Reward/action_rate_l2: -0.0124
      Episode_Reward/feet_air_time: -0.0202
 Episode_Reward/undesired_contacts: -0.0043
Episode_Reward/flat_orientation_l2: -0.0281
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 1.09s
                      Time elapsed: 00:14:32
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 804/1000 [0m                      

                       Computation: 11418 steps/s (collection: 1.027s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -2.9902
                       Mean reward: 8.36
               Mean episode length: 863.14
Episode_Reward/track_lin_vel_xy_exp: 0.5622
Episode_Reward/track_ang_vel_z_exp: 0.3019
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.1206
         Episode_Reward/dof_acc_l2: -0.0426
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0274
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0175
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9891840
                    Iteration time: 1.08s
                      Time elapsed: 00:14:33
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 805/1000 [0m                      

                       Computation: 10616 steps/s (collection: 1.108s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -2.9861
                       Mean reward: 8.75
               Mean episode length: 880.10
Episode_Reward/track_lin_vel_xy_exp: 0.5054
Episode_Reward/track_ang_vel_z_exp: 0.2853
       Episode_Reward/lin_vel_z_l2: -0.0106
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.1144
         Episode_Reward/dof_acc_l2: -0.0322
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0230
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0102
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 1.16s
                      Time elapsed: 00:14:34
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 806/1000 [0m                      

                       Computation: 10945 steps/s (collection: 1.069s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -2.9903
                       Mean reward: 8.73
               Mean episode length: 893.76
Episode_Reward/track_lin_vel_xy_exp: 0.6422
Episode_Reward/track_ang_vel_z_exp: 0.3485
       Episode_Reward/lin_vel_z_l2: -0.0171
      Episode_Reward/ang_vel_xy_l2: -0.0270
     Episode_Reward/dof_torques_l2: -0.1544
         Episode_Reward/dof_acc_l2: -0.0539
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0318
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: -0.0170
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9916416
                    Iteration time: 1.12s
                      Time elapsed: 00:14:35
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 807/1000 [0m                      

                       Computation: 11434 steps/s (collection: 1.027s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -3.0032
                       Mean reward: 9.33
               Mean episode length: 893.76
Episode_Reward/track_lin_vel_xy_exp: 0.6553
Episode_Reward/track_ang_vel_z_exp: 0.4022
       Episode_Reward/lin_vel_z_l2: -0.0140
      Episode_Reward/ang_vel_xy_l2: -0.0232
     Episode_Reward/dof_torques_l2: -0.1329
         Episode_Reward/dof_acc_l2: -0.0456
     Episode_Reward/action_rate_l2: -0.0186
      Episode_Reward/feet_air_time: -0.0294
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0097
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.07s
                      Time elapsed: 00:14:36
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 808/1000 [0m                      

                       Computation: 11041 steps/s (collection: 1.067s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -3.0060
                       Mean reward: 10.26
               Mean episode length: 904.06
Episode_Reward/track_lin_vel_xy_exp: 0.4553
Episode_Reward/track_ang_vel_z_exp: 0.2870
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.1251
         Episode_Reward/dof_acc_l2: -0.0346
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0201
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9940992
                    Iteration time: 1.11s
                      Time elapsed: 00:14:37
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 809/1000 [0m                      

                       Computation: 11349 steps/s (collection: 1.038s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.0068
                       Mean reward: 10.52
               Mean episode length: 936.18
Episode_Reward/track_lin_vel_xy_exp: 0.3902
Episode_Reward/track_ang_vel_z_exp: 0.2887
       Episode_Reward/lin_vel_z_l2: -0.0154
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1246
         Episode_Reward/dof_acc_l2: -0.0418
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0261
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: -0.0171
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 1.08s
                      Time elapsed: 00:14:38
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 810/1000 [0m                      

                       Computation: 11243 steps/s (collection: 1.042s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -3.0096
                       Mean reward: 11.29
               Mean episode length: 954.65
Episode_Reward/track_lin_vel_xy_exp: 0.4885
Episode_Reward/track_ang_vel_z_exp: 0.3613
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.1227
         Episode_Reward/dof_acc_l2: -0.0335
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0274
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0209
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9965568
                    Iteration time: 1.09s
                      Time elapsed: 00:14:40
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 811/1000 [0m                      

                       Computation: 10981 steps/s (collection: 1.064s, learning 0.055s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -3.0078
                       Mean reward: 11.04
               Mean episode length: 954.07
Episode_Reward/track_lin_vel_xy_exp: 0.3484
Episode_Reward/track_ang_vel_z_exp: 0.2127
       Episode_Reward/lin_vel_z_l2: -0.0174
      Episode_Reward/ang_vel_xy_l2: -0.0244
     Episode_Reward/dof_torques_l2: -0.1178
         Episode_Reward/dof_acc_l2: -0.0519
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0211
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0268
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 1.12s
                      Time elapsed: 00:14:41
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 812/1000 [0m                      

                       Computation: 10192 steps/s (collection: 1.124s, learning 0.082s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0052
                 Mean entropy loss: -3.0017
                       Mean reward: 11.19
               Mean episode length: 933.60
Episode_Reward/track_lin_vel_xy_exp: 0.5526
Episode_Reward/track_ang_vel_z_exp: 0.2867
       Episode_Reward/lin_vel_z_l2: -0.0150
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.1175
         Episode_Reward/dof_acc_l2: -0.0436
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9990144
                    Iteration time: 1.21s
                      Time elapsed: 00:14:42
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 813/1000 [0m                      

                       Computation: 11127 steps/s (collection: 1.059s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -3.0093
                       Mean reward: 10.88
               Mean episode length: 928.56
Episode_Reward/track_lin_vel_xy_exp: 0.4020
Episode_Reward/track_ang_vel_z_exp: 0.3437
       Episode_Reward/lin_vel_z_l2: -0.0149
      Episode_Reward/ang_vel_xy_l2: -0.0242
     Episode_Reward/dof_torques_l2: -0.1240
         Episode_Reward/dof_acc_l2: -0.0499
     Episode_Reward/action_rate_l2: -0.0167
      Episode_Reward/feet_air_time: -0.0286
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0222
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 1.10s
                      Time elapsed: 00:14:43
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 814/1000 [0m                      

                       Computation: 11671 steps/s (collection: 1.001s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.9988
                       Mean reward: 10.52
               Mean episode length: 916.99
Episode_Reward/track_lin_vel_xy_exp: 0.4096
Episode_Reward/track_ang_vel_z_exp: 0.2664
       Episode_Reward/lin_vel_z_l2: -0.0174
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.1195
         Episode_Reward/dof_acc_l2: -0.0504
     Episode_Reward/action_rate_l2: -0.0164
      Episode_Reward/feet_air_time: -0.0252
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0202
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10014720
                    Iteration time: 1.05s
                      Time elapsed: 00:14:44
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 815/1000 [0m                      

                       Computation: 10676 steps/s (collection: 1.091s, learning 0.060s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -2.9973
                       Mean reward: 10.26
               Mean episode length: 907.06
Episode_Reward/track_lin_vel_xy_exp: 0.4175
Episode_Reward/track_ang_vel_z_exp: 0.3021
       Episode_Reward/lin_vel_z_l2: -0.0185
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.1132
         Episode_Reward/dof_acc_l2: -0.0416
     Episode_Reward/action_rate_l2: -0.0157
      Episode_Reward/feet_air_time: -0.0230
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0230
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.15s
                      Time elapsed: 00:14:45
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 816/1000 [0m                      

                       Computation: 10794 steps/s (collection: 1.083s, learning 0.056s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -2.9849
                       Mean reward: 10.48
               Mean episode length: 905.68
Episode_Reward/track_lin_vel_xy_exp: 0.4809
Episode_Reward/track_ang_vel_z_exp: 0.3599
       Episode_Reward/lin_vel_z_l2: -0.0168
      Episode_Reward/ang_vel_xy_l2: -0.0222
     Episode_Reward/dof_torques_l2: -0.1408
         Episode_Reward/dof_acc_l2: -0.0528
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0298
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10039296
                    Iteration time: 1.14s
                      Time elapsed: 00:14:46
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 817/1000 [0m                      

                       Computation: 10914 steps/s (collection: 1.080s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0070
                 Mean entropy loss: -3.0022
                       Mean reward: 10.61
               Mean episode length: 906.52
Episode_Reward/track_lin_vel_xy_exp: 0.5720
Episode_Reward/track_ang_vel_z_exp: 0.3924
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0272
     Episode_Reward/dof_torques_l2: -0.1424
         Episode_Reward/dof_acc_l2: -0.0679
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0299
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0178
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 1.13s
                      Time elapsed: 00:14:47
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 818/1000 [0m                      

                       Computation: 11421 steps/s (collection: 1.030s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.0188
                       Mean reward: 10.54
               Mean episode length: 892.91
Episode_Reward/track_lin_vel_xy_exp: 0.3115
Episode_Reward/track_ang_vel_z_exp: 0.1919
       Episode_Reward/lin_vel_z_l2: -0.0108
      Episode_Reward/ang_vel_xy_l2: -0.0143
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.0279
     Episode_Reward/action_rate_l2: -0.0145
      Episode_Reward/feet_air_time: -0.0207
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10063872
                    Iteration time: 1.08s
                      Time elapsed: 00:14:48
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 819/1000 [0m                      

                       Computation: 11520 steps/s (collection: 1.016s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -3.0347
                       Mean reward: 10.78
               Mean episode length: 915.66
Episode_Reward/track_lin_vel_xy_exp: 0.4392
Episode_Reward/track_ang_vel_z_exp: 0.4132
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.1126
         Episode_Reward/dof_acc_l2: -0.0411
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0222
 Episode_Reward/undesired_contacts: -0.0011
Episode_Reward/flat_orientation_l2: -0.0186
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 1.07s
                      Time elapsed: 00:14:50
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 820/1000 [0m                      

                       Computation: 11435 steps/s (collection: 1.028s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.0411
                       Mean reward: 10.76
               Mean episode length: 926.29
Episode_Reward/track_lin_vel_xy_exp: 0.4263
Episode_Reward/track_ang_vel_z_exp: 0.3791
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.1195
         Episode_Reward/dof_acc_l2: -0.0391
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0169
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10088448
                    Iteration time: 1.07s
                      Time elapsed: 00:14:51
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 821/1000 [0m                      

                       Computation: 11231 steps/s (collection: 1.043s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.0558
                       Mean reward: 11.60
               Mean episode length: 963.13
Episode_Reward/track_lin_vel_xy_exp: 0.5561
Episode_Reward/track_ang_vel_z_exp: 0.3828
       Episode_Reward/lin_vel_z_l2: -0.0226
      Episode_Reward/ang_vel_xy_l2: -0.0256
     Episode_Reward/dof_torques_l2: -0.1458
         Episode_Reward/dof_acc_l2: -0.0564
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0311
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0112
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 1.09s
                      Time elapsed: 00:14:52
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 822/1000 [0m                      

                       Computation: 11141 steps/s (collection: 1.050s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.0607
                       Mean reward: 11.23
               Mean episode length: 945.88
Episode_Reward/track_lin_vel_xy_exp: 0.5191
Episode_Reward/track_ang_vel_z_exp: 0.2698
       Episode_Reward/lin_vel_z_l2: -0.0144
      Episode_Reward/ang_vel_xy_l2: -0.0222
     Episode_Reward/dof_torques_l2: -0.1210
         Episode_Reward/dof_acc_l2: -0.0460
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0219
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0159
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10113024
                    Iteration time: 1.10s
                      Time elapsed: 00:14:53
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 823/1000 [0m                      

                       Computation: 10937 steps/s (collection: 1.076s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -3.0650
                       Mean reward: 11.46
               Mean episode length: 931.33
Episode_Reward/track_lin_vel_xy_exp: 0.5456
Episode_Reward/track_ang_vel_z_exp: 0.2640
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0206
     Episode_Reward/dof_torques_l2: -0.1225
         Episode_Reward/dof_acc_l2: -0.0402
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0260
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0168
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.12s
                      Time elapsed: 00:14:54
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 824/1000 [0m                      

                       Computation: 11069 steps/s (collection: 1.062s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.0815
                       Mean reward: 10.87
               Mean episode length: 909.71
Episode_Reward/track_lin_vel_xy_exp: 0.4999
Episode_Reward/track_ang_vel_z_exp: 0.3325
       Episode_Reward/lin_vel_z_l2: -0.0188
      Episode_Reward/ang_vel_xy_l2: -0.0273
     Episode_Reward/dof_torques_l2: -0.1418
         Episode_Reward/dof_acc_l2: -0.0526
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0284
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0169
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10137600
                    Iteration time: 1.11s
                      Time elapsed: 00:14:55
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 825/1000 [0m                      

                       Computation: 11075 steps/s (collection: 1.057s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -3.0981
                       Mean reward: 10.91
               Mean episode length: 912.51
Episode_Reward/track_lin_vel_xy_exp: 0.4698
Episode_Reward/track_ang_vel_z_exp: 0.2307
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.1033
         Episode_Reward/dof_acc_l2: -0.0375
     Episode_Reward/action_rate_l2: -0.0141
      Episode_Reward/feet_air_time: -0.0236
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 1.11s
                      Time elapsed: 00:14:56
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 826/1000 [0m                      

                       Computation: 11319 steps/s (collection: 1.040s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -3.1120
                       Mean reward: 10.89
               Mean episode length: 914.66
Episode_Reward/track_lin_vel_xy_exp: 0.5068
Episode_Reward/track_ang_vel_z_exp: 0.3170
       Episode_Reward/lin_vel_z_l2: -0.0199
      Episode_Reward/ang_vel_xy_l2: -0.0253
     Episode_Reward/dof_torques_l2: -0.1470
         Episode_Reward/dof_acc_l2: -0.0484
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0152
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10162176
                    Iteration time: 1.09s
                      Time elapsed: 00:14:57
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 827/1000 [0m                      

                       Computation: 10793 steps/s (collection: 1.088s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0075
                 Mean entropy loss: -3.1226
                       Mean reward: 11.06
               Mean episode length: 913.38
Episode_Reward/track_lin_vel_xy_exp: 0.6839
Episode_Reward/track_ang_vel_z_exp: 0.2716
       Episode_Reward/lin_vel_z_l2: -0.0179
      Episode_Reward/ang_vel_xy_l2: -0.0245
     Episode_Reward/dof_torques_l2: -0.1407
         Episode_Reward/dof_acc_l2: -0.0669
     Episode_Reward/action_rate_l2: -0.0197
      Episode_Reward/feet_air_time: -0.0307
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 1.14s
                      Time elapsed: 00:14:58
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 828/1000 [0m                      

                       Computation: 11397 steps/s (collection: 1.031s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -3.1558
                       Mean reward: 11.06
               Mean episode length: 895.66
Episode_Reward/track_lin_vel_xy_exp: 0.6215
Episode_Reward/track_ang_vel_z_exp: 0.3582
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.1237
         Episode_Reward/dof_acc_l2: -0.0397
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0274
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0115
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 10186752
                    Iteration time: 1.08s
                      Time elapsed: 00:14:59
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 829/1000 [0m                      

                       Computation: 11000 steps/s (collection: 1.072s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -3.1609
                       Mean reward: 11.63
               Mean episode length: 938.68
Episode_Reward/track_lin_vel_xy_exp: 0.5631
Episode_Reward/track_ang_vel_z_exp: 0.3379
       Episode_Reward/lin_vel_z_l2: -0.0174
      Episode_Reward/ang_vel_xy_l2: -0.0275
     Episode_Reward/dof_torques_l2: -0.1400
         Episode_Reward/dof_acc_l2: -0.0560
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0284
 Episode_Reward/undesired_contacts: -0.0187
Episode_Reward/flat_orientation_l2: -0.0214
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 1.12s
                      Time elapsed: 00:15:01
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 830/1000 [0m                      

                       Computation: 11092 steps/s (collection: 1.064s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -3.1433
                       Mean reward: 11.15
               Mean episode length: 909.71
Episode_Reward/track_lin_vel_xy_exp: 0.3111
Episode_Reward/track_ang_vel_z_exp: 0.1926
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0156
     Episode_Reward/dof_torques_l2: -0.0746
         Episode_Reward/dof_acc_l2: -0.0275
     Episode_Reward/action_rate_l2: -0.0112
      Episode_Reward/feet_air_time: -0.0166
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10211328
                    Iteration time: 1.11s
                      Time elapsed: 00:15:02
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 831/1000 [0m                      

                       Computation: 11628 steps/s (collection: 1.004s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -3.1473
                       Mean reward: 11.70
               Mean episode length: 930.12
Episode_Reward/track_lin_vel_xy_exp: 0.5160
Episode_Reward/track_ang_vel_z_exp: 0.3425
       Episode_Reward/lin_vel_z_l2: -0.0188
      Episode_Reward/ang_vel_xy_l2: -0.0255
     Episode_Reward/dof_torques_l2: -0.1464
         Episode_Reward/dof_acc_l2: -0.0659
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0294
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0277
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.06s
                      Time elapsed: 00:15:03
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 832/1000 [0m                      

                       Computation: 11468 steps/s (collection: 1.028s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0098
                 Mean entropy loss: -3.1654
                       Mean reward: 11.84
               Mean episode length: 926.12
Episode_Reward/track_lin_vel_xy_exp: 0.4955
Episode_Reward/track_ang_vel_z_exp: 0.2903
       Episode_Reward/lin_vel_z_l2: -0.0150
      Episode_Reward/ang_vel_xy_l2: -0.0200
     Episode_Reward/dof_torques_l2: -0.1122
         Episode_Reward/dof_acc_l2: -0.0473
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0253
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0158
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10235904
                    Iteration time: 1.07s
                      Time elapsed: 00:15:04
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 833/1000 [0m                      

                       Computation: 11335 steps/s (collection: 1.035s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0077
                 Mean entropy loss: -3.1726
                       Mean reward: 11.82
               Mean episode length: 920.61
Episode_Reward/track_lin_vel_xy_exp: 0.6161
Episode_Reward/track_ang_vel_z_exp: 0.3547
       Episode_Reward/lin_vel_z_l2: -0.0165
      Episode_Reward/ang_vel_xy_l2: -0.0234
     Episode_Reward/dof_torques_l2: -0.1505
         Episode_Reward/dof_acc_l2: -0.0582
     Episode_Reward/action_rate_l2: -0.0194
      Episode_Reward/feet_air_time: -0.0295
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 1.08s
                      Time elapsed: 00:15:05
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 834/1000 [0m                      

                       Computation: 11084 steps/s (collection: 1.044s, learning 0.065s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -3.1748
                       Mean reward: 11.63
               Mean episode length: 911.36
Episode_Reward/track_lin_vel_xy_exp: 0.5259
Episode_Reward/track_ang_vel_z_exp: 0.2274
       Episode_Reward/lin_vel_z_l2: -0.0118
      Episode_Reward/ang_vel_xy_l2: -0.0161
     Episode_Reward/dof_torques_l2: -0.1192
         Episode_Reward/dof_acc_l2: -0.0320
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0207
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0091
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10260480
                    Iteration time: 1.11s
                      Time elapsed: 00:15:06
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 835/1000 [0m                      

                       Computation: 10811 steps/s (collection: 1.086s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0062
                 Mean entropy loss: -3.1827
                       Mean reward: 11.61
               Mean episode length: 907.01
Episode_Reward/track_lin_vel_xy_exp: 0.5933
Episode_Reward/track_ang_vel_z_exp: 0.3453
       Episode_Reward/lin_vel_z_l2: -0.0192
      Episode_Reward/ang_vel_xy_l2: -0.0231
     Episode_Reward/dof_torques_l2: -0.1352
         Episode_Reward/dof_acc_l2: -0.0601
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0303
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0228
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 1.14s
                      Time elapsed: 00:15:07
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 836/1000 [0m                      

                       Computation: 10565 steps/s (collection: 1.117s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -3.1960
                       Mean reward: 11.31
               Mean episode length: 907.35
Episode_Reward/track_lin_vel_xy_exp: 0.6725
Episode_Reward/track_ang_vel_z_exp: 0.3018
       Episode_Reward/lin_vel_z_l2: -0.0167
      Episode_Reward/ang_vel_xy_l2: -0.0218
     Episode_Reward/dof_torques_l2: -0.1338
         Episode_Reward/dof_acc_l2: -0.0429
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0240
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0181
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10285056
                    Iteration time: 1.16s
                      Time elapsed: 00:15:08
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 837/1000 [0m                      

                       Computation: 11119 steps/s (collection: 1.044s, learning 0.062s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -3.2050
                       Mean reward: 11.65
               Mean episode length: 907.35
Episode_Reward/track_lin_vel_xy_exp: 0.7393
Episode_Reward/track_ang_vel_z_exp: 0.3175
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.1341
         Episode_Reward/dof_acc_l2: -0.0416
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0297
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0130
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 1.11s
                      Time elapsed: 00:15:09
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 838/1000 [0m                      

                       Computation: 11283 steps/s (collection: 1.044s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -3.1845
                       Mean reward: 12.39
               Mean episode length: 922.71
Episode_Reward/track_lin_vel_xy_exp: 0.6671
Episode_Reward/track_ang_vel_z_exp: 0.3064
       Episode_Reward/lin_vel_z_l2: -0.0129
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.1109
         Episode_Reward/dof_acc_l2: -0.0368
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0143
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10309632
                    Iteration time: 1.09s
                      Time elapsed: 00:15:11
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 839/1000 [0m                      

                       Computation: 11286 steps/s (collection: 1.044s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0080
                 Mean entropy loss: -3.1647
                       Mean reward: 12.40
               Mean episode length: 932.18
Episode_Reward/track_lin_vel_xy_exp: 0.5829
Episode_Reward/track_ang_vel_z_exp: 0.3584
       Episode_Reward/lin_vel_z_l2: -0.0173
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.1267
         Episode_Reward/dof_acc_l2: -0.0468
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0257
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0197
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.09s
                      Time elapsed: 00:15:12
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 840/1000 [0m                      

                       Computation: 11410 steps/s (collection: 1.023s, learning 0.054s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.1691
                       Mean reward: 12.55
               Mean episode length: 941.67
Episode_Reward/track_lin_vel_xy_exp: 0.4994
Episode_Reward/track_ang_vel_z_exp: 0.2399
       Episode_Reward/lin_vel_z_l2: -0.0197
      Episode_Reward/ang_vel_xy_l2: -0.0222
     Episode_Reward/dof_torques_l2: -0.1156
         Episode_Reward/dof_acc_l2: -0.0522
     Episode_Reward/action_rate_l2: -0.0147
      Episode_Reward/feet_air_time: -0.0248
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0335
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10334208
                    Iteration time: 1.08s
                      Time elapsed: 00:15:13
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 841/1000 [0m                      

                       Computation: 11313 steps/s (collection: 1.033s, learning 0.054s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0116
                 Mean entropy loss: -3.1849
                       Mean reward: 12.29
               Mean episode length: 941.21
Episode_Reward/track_lin_vel_xy_exp: 0.4159
Episode_Reward/track_ang_vel_z_exp: 0.3178
       Episode_Reward/lin_vel_z_l2: -0.0116
      Episode_Reward/ang_vel_xy_l2: -0.0157
     Episode_Reward/dof_torques_l2: -0.1186
         Episode_Reward/dof_acc_l2: -0.0245
     Episode_Reward/action_rate_l2: -0.0151
      Episode_Reward/feet_air_time: -0.0191
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0191
  Episode_Termination/base_contact: 0.7083
      Episode_Termination/time_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 1.09s
                      Time elapsed: 00:15:14
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 842/1000 [0m                      

                       Computation: 10983 steps/s (collection: 1.075s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -3.1780
                       Mean reward: 12.04
               Mean episode length: 931.12
Episode_Reward/track_lin_vel_xy_exp: 0.4507
Episode_Reward/track_ang_vel_z_exp: 0.2990
       Episode_Reward/lin_vel_z_l2: -0.0219
      Episode_Reward/ang_vel_xy_l2: -0.0242
     Episode_Reward/dof_torques_l2: -0.1396
         Episode_Reward/dof_acc_l2: -0.0653
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0242
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0289
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10358784
                    Iteration time: 1.12s
                      Time elapsed: 00:15:15
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 843/1000 [0m                      

                       Computation: 11994 steps/s (collection: 0.977s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.1713
                       Mean reward: 11.91
               Mean episode length: 937.34
Episode_Reward/track_lin_vel_xy_exp: 0.4486
Episode_Reward/track_ang_vel_z_exp: 0.2358
       Episode_Reward/lin_vel_z_l2: -0.0165
      Episode_Reward/ang_vel_xy_l2: -0.0237
     Episode_Reward/dof_torques_l2: -0.1154
         Episode_Reward/dof_acc_l2: -0.0552
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0237
 Episode_Reward/undesired_contacts: -0.0082
Episode_Reward/flat_orientation_l2: -0.0336
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 1.02s
                      Time elapsed: 00:15:16
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 844/1000 [0m                      

                       Computation: 11384 steps/s (collection: 1.034s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.1572
                       Mean reward: 11.40
               Mean episode length: 937.34
Episode_Reward/track_lin_vel_xy_exp: 0.2939
Episode_Reward/track_ang_vel_z_exp: 0.3861
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0160
     Episode_Reward/dof_torques_l2: -0.1177
         Episode_Reward/dof_acc_l2: -0.0277
     Episode_Reward/action_rate_l2: -0.0165
      Episode_Reward/feet_air_time: -0.0203
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0160
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10383360
                    Iteration time: 1.08s
                      Time elapsed: 00:15:17
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 845/1000 [0m                      

                       Computation: 11145 steps/s (collection: 1.052s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -3.1639
                       Mean reward: 11.41
               Mean episode length: 937.77
Episode_Reward/track_lin_vel_xy_exp: 0.3709
Episode_Reward/track_ang_vel_z_exp: 0.3022
       Episode_Reward/lin_vel_z_l2: -0.0130
      Episode_Reward/ang_vel_xy_l2: -0.0186
     Episode_Reward/dof_torques_l2: -0.1236
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0242
 Episode_Reward/undesired_contacts: -0.0484
Episode_Reward/flat_orientation_l2: -0.0156
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 1.10s
                      Time elapsed: 00:15:18
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 846/1000 [0m                      

                       Computation: 11348 steps/s (collection: 1.034s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -3.1706
                       Mean reward: 11.18
               Mean episode length: 921.10
Episode_Reward/track_lin_vel_xy_exp: 0.6401
Episode_Reward/track_ang_vel_z_exp: 0.3570
       Episode_Reward/lin_vel_z_l2: -0.0165
      Episode_Reward/ang_vel_xy_l2: -0.0240
     Episode_Reward/dof_torques_l2: -0.1399
         Episode_Reward/dof_acc_l2: -0.0550
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0287
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10407936
                    Iteration time: 1.08s
                      Time elapsed: 00:15:19
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 847/1000 [0m                      

                       Computation: 11418 steps/s (collection: 1.030s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0108
                 Mean entropy loss: -3.1550
                       Mean reward: 11.29
               Mean episode length: 905.14
Episode_Reward/track_lin_vel_xy_exp: 0.6046
Episode_Reward/track_ang_vel_z_exp: 0.3623
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.1175
         Episode_Reward/dof_acc_l2: -0.0426
     Episode_Reward/action_rate_l2: -0.0154
      Episode_Reward/feet_air_time: -0.0288
 Episode_Reward/undesired_contacts: -0.0009
Episode_Reward/flat_orientation_l2: -0.0166
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.08s
                      Time elapsed: 00:15:20
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 848/1000 [0m                      

                       Computation: 11433 steps/s (collection: 1.027s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.1430
                       Mean reward: 11.12
               Mean episode length: 887.48
Episode_Reward/track_lin_vel_xy_exp: 0.4434
Episode_Reward/track_ang_vel_z_exp: 0.2503
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.1068
         Episode_Reward/dof_acc_l2: -0.0375
     Episode_Reward/action_rate_l2: -0.0135
      Episode_Reward/feet_air_time: -0.0213
 Episode_Reward/undesired_contacts: -0.0019
Episode_Reward/flat_orientation_l2: -0.0239
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10432512
                    Iteration time: 1.07s
                      Time elapsed: 00:15:21
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 849/1000 [0m                      

                       Computation: 11443 steps/s (collection: 1.026s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0105
                 Mean entropy loss: -3.1559
                       Mean reward: 11.81
               Mean episode length: 888.51
Episode_Reward/track_lin_vel_xy_exp: 0.7137
Episode_Reward/track_ang_vel_z_exp: 0.3854
       Episode_Reward/lin_vel_z_l2: -0.0166
      Episode_Reward/ang_vel_xy_l2: -0.0222
     Episode_Reward/dof_torques_l2: -0.1498
         Episode_Reward/dof_acc_l2: -0.0518
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0303
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0200
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 1.07s
                      Time elapsed: 00:15:22
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 850/1000 [0m                      

                       Computation: 11319 steps/s (collection: 1.040s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -3.1666
                       Mean reward: 12.45
               Mean episode length: 894.61
Episode_Reward/track_lin_vel_xy_exp: 0.5917
Episode_Reward/track_ang_vel_z_exp: 0.3034
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0200
     Episode_Reward/dof_torques_l2: -0.1094
         Episode_Reward/dof_acc_l2: -0.0301
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0226
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: -0.0168
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10457088
                    Iteration time: 1.09s
                      Time elapsed: 00:15:23
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 851/1000 [0m                      

                       Computation: 10834 steps/s (collection: 1.088s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -3.1675
                       Mean reward: 12.95
               Mean episode length: 889.17
Episode_Reward/track_lin_vel_xy_exp: 0.5748
Episode_Reward/track_ang_vel_z_exp: 0.3018
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0195
     Episode_Reward/dof_torques_l2: -0.1170
         Episode_Reward/dof_acc_l2: -0.0496
     Episode_Reward/action_rate_l2: -0.0158
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0210
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 1.13s
                      Time elapsed: 00:15:25
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 852/1000 [0m                      

                       Computation: 11006 steps/s (collection: 1.058s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0098
                 Mean entropy loss: -3.1795
                       Mean reward: 13.13
               Mean episode length: 898.20
Episode_Reward/track_lin_vel_xy_exp: 0.4803
Episode_Reward/track_ang_vel_z_exp: 0.2687
       Episode_Reward/lin_vel_z_l2: -0.0170
      Episode_Reward/ang_vel_xy_l2: -0.0184
     Episode_Reward/dof_torques_l2: -0.1304
         Episode_Reward/dof_acc_l2: -0.0519
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0210
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10481664
                    Iteration time: 1.12s
                      Time elapsed: 00:15:26
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 853/1000 [0m                      

                       Computation: 10833 steps/s (collection: 1.085s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -3.2117
                       Mean reward: 12.56
               Mean episode length: 888.42
Episode_Reward/track_lin_vel_xy_exp: 0.5690
Episode_Reward/track_ang_vel_z_exp: 0.2901
       Episode_Reward/lin_vel_z_l2: -0.0172
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.1260
         Episode_Reward/dof_acc_l2: -0.0429
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0256
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0186
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 1.13s
                      Time elapsed: 00:15:27
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 854/1000 [0m                      

                       Computation: 11062 steps/s (collection: 1.062s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -3.2306
                       Mean reward: 12.38
               Mean episode length: 893.73
Episode_Reward/track_lin_vel_xy_exp: 0.5090
Episode_Reward/track_ang_vel_z_exp: 0.3599
       Episode_Reward/lin_vel_z_l2: -0.0221
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.1426
         Episode_Reward/dof_acc_l2: -0.0578
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0268
 Episode_Reward/undesired_contacts: -0.0115
Episode_Reward/flat_orientation_l2: -0.0289
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10506240
                    Iteration time: 1.11s
                      Time elapsed: 00:15:28
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 855/1000 [0m                      

                       Computation: 10820 steps/s (collection: 1.089s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0115
                 Mean entropy loss: -3.2285
                       Mean reward: 12.86
               Mean episode length: 908.67
Episode_Reward/track_lin_vel_xy_exp: 0.7270
Episode_Reward/track_ang_vel_z_exp: 0.3881
       Episode_Reward/lin_vel_z_l2: -0.0165
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.1412
         Episode_Reward/dof_acc_l2: -0.0453
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0343
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0108
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.14s
                      Time elapsed: 00:15:29
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 856/1000 [0m                      

                       Computation: 11035 steps/s (collection: 1.069s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.2150
                       Mean reward: 13.03
               Mean episode length: 910.39
Episode_Reward/track_lin_vel_xy_exp: 0.6379
Episode_Reward/track_ang_vel_z_exp: 0.3650
       Episode_Reward/lin_vel_z_l2: -0.0139
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.1165
         Episode_Reward/dof_acc_l2: -0.0307
     Episode_Reward/action_rate_l2: -0.0158
      Episode_Reward/feet_air_time: -0.0289
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0098
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10530816
                    Iteration time: 1.11s
                      Time elapsed: 00:15:30
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 857/1000 [0m                      

                       Computation: 11191 steps/s (collection: 1.053s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0123
                 Mean entropy loss: -3.2189
                       Mean reward: 12.48
               Mean episode length: 900.32
Episode_Reward/track_lin_vel_xy_exp: 0.6162
Episode_Reward/track_ang_vel_z_exp: 0.3605
       Episode_Reward/lin_vel_z_l2: -0.0173
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.1514
         Episode_Reward/dof_acc_l2: -0.0591
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0322
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0172
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 1.10s
                      Time elapsed: 00:15:31
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 858/1000 [0m                      

                       Computation: 10972 steps/s (collection: 1.072s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -3.2304
                       Mean reward: 11.97
               Mean episode length: 909.92
Episode_Reward/track_lin_vel_xy_exp: 0.5466
Episode_Reward/track_ang_vel_z_exp: 0.2493
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1303
         Episode_Reward/dof_acc_l2: -0.0408
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0269
 Episode_Reward/undesired_contacts: -0.0014
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10555392
                    Iteration time: 1.12s
                      Time elapsed: 00:15:32
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 859/1000 [0m                      

                       Computation: 11549 steps/s (collection: 1.010s, learning 0.054s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -3.2308
                       Mean reward: 11.30
               Mean episode length: 891.97
Episode_Reward/track_lin_vel_xy_exp: 0.4078
Episode_Reward/track_ang_vel_z_exp: 0.2876
       Episode_Reward/lin_vel_z_l2: -0.0178
      Episode_Reward/ang_vel_xy_l2: -0.0234
     Episode_Reward/dof_torques_l2: -0.1267
         Episode_Reward/dof_acc_l2: -0.0489
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0253
 Episode_Reward/undesired_contacts: -0.0170
Episode_Reward/flat_orientation_l2: -0.0362
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 1.06s
                      Time elapsed: 00:15:34
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 860/1000 [0m                      

                       Computation: 10635 steps/s (collection: 1.107s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.2276
                       Mean reward: 11.30
               Mean episode length: 895.99
Episode_Reward/track_lin_vel_xy_exp: 0.4153
Episode_Reward/track_ang_vel_z_exp: 0.2844
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0159
     Episode_Reward/dof_torques_l2: -0.1026
         Episode_Reward/dof_acc_l2: -0.0414
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0285
 Episode_Reward/undesired_contacts: -0.0046
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10579968
                    Iteration time: 1.16s
                      Time elapsed: 00:15:35
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 861/1000 [0m                      

                       Computation: 10878 steps/s (collection: 1.085s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0108
                 Mean entropy loss: -3.2415
                       Mean reward: 10.56
               Mean episode length: 857.18
Episode_Reward/track_lin_vel_xy_exp: 0.3806
Episode_Reward/track_ang_vel_z_exp: 0.3354
       Episode_Reward/lin_vel_z_l2: -0.0138
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.1087
         Episode_Reward/dof_acc_l2: -0.0338
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0243
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0149
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 1.13s
                      Time elapsed: 00:15:36
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 862/1000 [0m                      

                       Computation: 10999 steps/s (collection: 1.072s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.2474
                       Mean reward: 10.34
               Mean episode length: 863.32
Episode_Reward/track_lin_vel_xy_exp: 0.5262
Episode_Reward/track_ang_vel_z_exp: 0.3054
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.1181
         Episode_Reward/dof_acc_l2: -0.0456
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0286
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10604544
                    Iteration time: 1.12s
                      Time elapsed: 00:15:37
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 863/1000 [0m                      

                       Computation: 10815 steps/s (collection: 1.086s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.2567
                       Mean reward: 9.71
               Mean episode length: 847.41
Episode_Reward/track_lin_vel_xy_exp: 0.3541
Episode_Reward/track_ang_vel_z_exp: 0.2596
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.1043
         Episode_Reward/dof_acc_l2: -0.0335
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0230
 Episode_Reward/undesired_contacts: -0.0025
Episode_Reward/flat_orientation_l2: -0.0254
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.14s
                      Time elapsed: 00:15:38
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 864/1000 [0m                      

                       Computation: 11599 steps/s (collection: 1.014s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.2531
                       Mean reward: 10.26
               Mean episode length: 859.31
Episode_Reward/track_lin_vel_xy_exp: 0.6565
Episode_Reward/track_ang_vel_z_exp: 0.2977
       Episode_Reward/lin_vel_z_l2: -0.0177
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1450
         Episode_Reward/dof_acc_l2: -0.0478
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0265
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0154
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10629120
                    Iteration time: 1.06s
                      Time elapsed: 00:15:39
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 865/1000 [0m                      

                       Computation: 11066 steps/s (collection: 1.065s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0111
                 Mean entropy loss: -3.2528
                       Mean reward: 9.98
               Mean episode length: 851.32
Episode_Reward/track_lin_vel_xy_exp: 0.4061
Episode_Reward/track_ang_vel_z_exp: 0.2664
       Episode_Reward/lin_vel_z_l2: -0.0160
      Episode_Reward/ang_vel_xy_l2: -0.0216
     Episode_Reward/dof_torques_l2: -0.1097
         Episode_Reward/dof_acc_l2: -0.0508
     Episode_Reward/action_rate_l2: -0.0141
      Episode_Reward/feet_air_time: -0.0186
 Episode_Reward/undesired_contacts: -0.0080
Episode_Reward/flat_orientation_l2: -0.0300
  Episode_Termination/base_contact: 0.6667
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 1.11s
                      Time elapsed: 00:15:40
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 866/1000 [0m                      

                       Computation: 11315 steps/s (collection: 1.026s, learning 0.060s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.2718
                       Mean reward: 10.53
               Mean episode length: 860.24
Episode_Reward/track_lin_vel_xy_exp: 0.5334
Episode_Reward/track_ang_vel_z_exp: 0.3486
       Episode_Reward/lin_vel_z_l2: -0.0194
      Episode_Reward/ang_vel_xy_l2: -0.0218
     Episode_Reward/dof_torques_l2: -0.1351
         Episode_Reward/dof_acc_l2: -0.0565
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0290
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0182
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10653696
                    Iteration time: 1.09s
                      Time elapsed: 00:15:41
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 867/1000 [0m                      

                       Computation: 10615 steps/s (collection: 1.111s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0067
                 Mean entropy loss: -3.2699
                       Mean reward: 10.65
               Mean episode length: 868.20
Episode_Reward/track_lin_vel_xy_exp: 0.5305
Episode_Reward/track_ang_vel_z_exp: 0.3352
       Episode_Reward/lin_vel_z_l2: -0.0155
      Episode_Reward/ang_vel_xy_l2: -0.0200
     Episode_Reward/dof_torques_l2: -0.1233
         Episode_Reward/dof_acc_l2: -0.0439
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0256
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0167
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 1.16s
                      Time elapsed: 00:15:42
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 868/1000 [0m                      

                       Computation: 11074 steps/s (collection: 1.057s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.2572
                       Mean reward: 11.64
               Mean episode length: 895.84
Episode_Reward/track_lin_vel_xy_exp: 0.5832
Episode_Reward/track_ang_vel_z_exp: 0.3665
       Episode_Reward/lin_vel_z_l2: -0.0119
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.1338
         Episode_Reward/dof_acc_l2: -0.0349
     Episode_Reward/action_rate_l2: -0.0173
      Episode_Reward/feet_air_time: -0.0271
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0127
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10678272
                    Iteration time: 1.11s
                      Time elapsed: 00:15:44
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 869/1000 [0m                      

                       Computation: 10760 steps/s (collection: 1.097s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0117
                 Mean entropy loss: -3.2579
                       Mean reward: 11.50
               Mean episode length: 896.96
Episode_Reward/track_lin_vel_xy_exp: 0.4951
Episode_Reward/track_ang_vel_z_exp: 0.2861
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1100
         Episode_Reward/dof_acc_l2: -0.0393
     Episode_Reward/action_rate_l2: -0.0149
      Episode_Reward/feet_air_time: -0.0251
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0224
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 1.14s
                      Time elapsed: 00:15:45
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 870/1000 [0m                      

                       Computation: 11053 steps/s (collection: 1.062s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -3.2481
                       Mean reward: 11.54
               Mean episode length: 891.20
Episode_Reward/track_lin_vel_xy_exp: 0.3576
Episode_Reward/track_ang_vel_z_exp: 0.2844
       Episode_Reward/lin_vel_z_l2: -0.0122
      Episode_Reward/ang_vel_xy_l2: -0.0162
     Episode_Reward/dof_torques_l2: -0.0986
         Episode_Reward/dof_acc_l2: -0.0283
     Episode_Reward/action_rate_l2: -0.0135
      Episode_Reward/feet_air_time: -0.0189
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0181
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10702848
                    Iteration time: 1.11s
                      Time elapsed: 00:15:46
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 871/1000 [0m                      

                       Computation: 11292 steps/s (collection: 1.040s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -3.2492
                       Mean reward: 12.19
               Mean episode length: 896.90
Episode_Reward/track_lin_vel_xy_exp: 0.5825
Episode_Reward/track_ang_vel_z_exp: 0.3349
       Episode_Reward/lin_vel_z_l2: -0.0185
      Episode_Reward/ang_vel_xy_l2: -0.0197
     Episode_Reward/dof_torques_l2: -0.1283
         Episode_Reward/dof_acc_l2: -0.0454
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0291
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.09s
                      Time elapsed: 00:15:47
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 872/1000 [0m                      

                       Computation: 10701 steps/s (collection: 1.104s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.2365
                       Mean reward: 12.45
               Mean episode length: 912.46
Episode_Reward/track_lin_vel_xy_exp: 0.5054
Episode_Reward/track_ang_vel_z_exp: 0.2890
       Episode_Reward/lin_vel_z_l2: -0.0156
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.1239
         Episode_Reward/dof_acc_l2: -0.0359
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0262
 Episode_Reward/undesired_contacts: -0.0295
Episode_Reward/flat_orientation_l2: -0.0116
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10727424
                    Iteration time: 1.15s
                      Time elapsed: 00:15:48
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 873/1000 [0m                      

                       Computation: 10876 steps/s (collection: 1.081s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0098
                 Mean entropy loss: -3.2477
                       Mean reward: 12.17
               Mean episode length: 899.05
Episode_Reward/track_lin_vel_xy_exp: 0.5280
Episode_Reward/track_ang_vel_z_exp: 0.3059
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0158
     Episode_Reward/dof_torques_l2: -0.1155
         Episode_Reward/dof_acc_l2: -0.0353
     Episode_Reward/action_rate_l2: -0.0151
      Episode_Reward/feet_air_time: -0.0242
 Episode_Reward/undesired_contacts: -0.0142
Episode_Reward/flat_orientation_l2: -0.0132
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 1.13s
                      Time elapsed: 00:15:49
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 874/1000 [0m                      

                       Computation: 10893 steps/s (collection: 1.082s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -3.2567
                       Mean reward: 11.75
               Mean episode length: 883.65
Episode_Reward/track_lin_vel_xy_exp: 0.5482
Episode_Reward/track_ang_vel_z_exp: 0.3315
       Episode_Reward/lin_vel_z_l2: -0.0189
      Episode_Reward/ang_vel_xy_l2: -0.0259
     Episode_Reward/dof_torques_l2: -0.1375
         Episode_Reward/dof_acc_l2: -0.0539
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0289
 Episode_Reward/undesired_contacts: -0.0206
Episode_Reward/flat_orientation_l2: -0.0287
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10752000
                    Iteration time: 1.13s
                      Time elapsed: 00:15:50
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 875/1000 [0m                      

                       Computation: 11068 steps/s (collection: 1.066s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0079
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -3.2699
                       Mean reward: 11.21
               Mean episode length: 872.85
Episode_Reward/track_lin_vel_xy_exp: 0.4892
Episode_Reward/track_ang_vel_z_exp: 0.1995
       Episode_Reward/lin_vel_z_l2: -0.0241
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1309
         Episode_Reward/dof_acc_l2: -0.0415
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0224
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0268
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 1.11s
                      Time elapsed: 00:15:51
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 876/1000 [0m                      

                       Computation: 10646 steps/s (collection: 1.106s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0066
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -3.2519
                       Mean reward: 10.74
               Mean episode length: 867.32
Episode_Reward/track_lin_vel_xy_exp: 0.4662
Episode_Reward/track_ang_vel_z_exp: 0.2561
       Episode_Reward/lin_vel_z_l2: -0.0222
      Episode_Reward/ang_vel_xy_l2: -0.0246
     Episode_Reward/dof_torques_l2: -0.1349
         Episode_Reward/dof_acc_l2: -0.0590
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0277
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0267
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10776576
                    Iteration time: 1.15s
                      Time elapsed: 00:15:53
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 877/1000 [0m                      

                       Computation: 10732 steps/s (collection: 1.094s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0050
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.2375
                       Mean reward: 11.04
               Mean episode length: 870.93
Episode_Reward/track_lin_vel_xy_exp: 0.5764
Episode_Reward/track_ang_vel_z_exp: 0.3333
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1310
         Episode_Reward/dof_acc_l2: -0.0515
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0285
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0180
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 1.14s
                      Time elapsed: 00:15:54
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 878/1000 [0m                      

                       Computation: 10562 steps/s (collection: 1.113s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0073
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -3.2341
                       Mean reward: 10.56
               Mean episode length: 862.52
Episode_Reward/track_lin_vel_xy_exp: 0.5685
Episode_Reward/track_ang_vel_z_exp: 0.3118
       Episode_Reward/lin_vel_z_l2: -0.0173
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1582
         Episode_Reward/dof_acc_l2: -0.0598
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0317
 Episode_Reward/undesired_contacts: -0.0020
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10801152
                    Iteration time: 1.16s
                      Time elapsed: 00:15:55
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 879/1000 [0m                      

                       Computation: 10709 steps/s (collection: 1.102s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -3.2114
                       Mean reward: 10.37
               Mean episode length: 853.24
Episode_Reward/track_lin_vel_xy_exp: 0.4904
Episode_Reward/track_ang_vel_z_exp: 0.2459
       Episode_Reward/lin_vel_z_l2: -0.0131
      Episode_Reward/ang_vel_xy_l2: -0.0192
     Episode_Reward/dof_torques_l2: -0.1083
         Episode_Reward/dof_acc_l2: -0.0365
     Episode_Reward/action_rate_l2: -0.0135
      Episode_Reward/feet_air_time: -0.0212
 Episode_Reward/undesired_contacts: -0.0081
Episode_Reward/flat_orientation_l2: -0.0242
  Episode_Termination/base_contact: 0.5833
      Episode_Termination/time_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.15s
                      Time elapsed: 00:15:56
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 880/1000 [0m                      

                       Computation: 11147 steps/s (collection: 1.057s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.1936
                       Mean reward: 10.82
               Mean episode length: 870.59
Episode_Reward/track_lin_vel_xy_exp: 0.6832
Episode_Reward/track_ang_vel_z_exp: 0.3871
       Episode_Reward/lin_vel_z_l2: -0.0199
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.1434
         Episode_Reward/dof_acc_l2: -0.0583
     Episode_Reward/action_rate_l2: -0.0192
      Episode_Reward/feet_air_time: -0.0334
 Episode_Reward/undesired_contacts: -0.0096
Episode_Reward/flat_orientation_l2: -0.0157
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10825728
                    Iteration time: 1.10s
                      Time elapsed: 00:15:57
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 881/1000 [0m                      

                       Computation: 11431 steps/s (collection: 1.027s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -3.1824
                       Mean reward: 10.56
               Mean episode length: 873.66
Episode_Reward/track_lin_vel_xy_exp: 0.3340
Episode_Reward/track_ang_vel_z_exp: 0.1891
       Episode_Reward/lin_vel_z_l2: -0.0176
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.1171
         Episode_Reward/dof_acc_l2: -0.0437
     Episode_Reward/action_rate_l2: -0.0142
      Episode_Reward/feet_air_time: -0.0211
 Episode_Reward/undesired_contacts: -0.0048
Episode_Reward/flat_orientation_l2: -0.0335
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 1.07s
                      Time elapsed: 00:15:58
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 882/1000 [0m                      

                       Computation: 11619 steps/s (collection: 0.998s, learning 0.059s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.1838
                       Mean reward: 10.81
               Mean episode length: 883.70
Episode_Reward/track_lin_vel_xy_exp: 0.5954
Episode_Reward/track_ang_vel_z_exp: 0.3614
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.1297
         Episode_Reward/dof_acc_l2: -0.0369
     Episode_Reward/action_rate_l2: -0.0165
      Episode_Reward/feet_air_time: -0.0291
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0161
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10850304
                    Iteration time: 1.06s
                      Time elapsed: 00:15:59
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 883/1000 [0m                      

                       Computation: 10476 steps/s (collection: 1.125s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -3.1738
                       Mean reward: 10.77
               Mean episode length: 863.78
Episode_Reward/track_lin_vel_xy_exp: 0.4553
Episode_Reward/track_ang_vel_z_exp: 0.2818
       Episode_Reward/lin_vel_z_l2: -0.0160
      Episode_Reward/ang_vel_xy_l2: -0.0178
     Episode_Reward/dof_torques_l2: -0.1245
         Episode_Reward/dof_acc_l2: -0.0426
     Episode_Reward/action_rate_l2: -0.0141
      Episode_Reward/feet_air_time: -0.0261
 Episode_Reward/undesired_contacts: -0.0111
Episode_Reward/flat_orientation_l2: -0.0261
  Episode_Termination/base_contact: 1.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 1.17s
                      Time elapsed: 00:16:00
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 884/1000 [0m                      

                       Computation: 11091 steps/s (collection: 1.047s, learning 0.060s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -3.1599
                       Mean reward: 11.01
               Mean episode length: 880.95
Episode_Reward/track_lin_vel_xy_exp: 0.5770
Episode_Reward/track_ang_vel_z_exp: 0.2469
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0212
     Episode_Reward/dof_torques_l2: -0.1232
         Episode_Reward/dof_acc_l2: -0.0509
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0280
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0142
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10874880
                    Iteration time: 1.11s
                      Time elapsed: 00:16:02
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 885/1000 [0m                      

                       Computation: 11207 steps/s (collection: 1.053s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -3.1482
                       Mean reward: 11.54
               Mean episode length: 899.42
Episode_Reward/track_lin_vel_xy_exp: 0.6939
Episode_Reward/track_ang_vel_z_exp: 0.3524
       Episode_Reward/lin_vel_z_l2: -0.0188
      Episode_Reward/ang_vel_xy_l2: -0.0277
     Episode_Reward/dof_torques_l2: -0.1530
         Episode_Reward/dof_acc_l2: -0.0616
     Episode_Reward/action_rate_l2: -0.0190
      Episode_Reward/feet_air_time: -0.0342
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0195
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 1.10s
                      Time elapsed: 00:16:03
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 886/1000 [0m                      

                       Computation: 10977 steps/s (collection: 1.075s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0052
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -3.1386
                       Mean reward: 11.97
               Mean episode length: 904.16
Episode_Reward/track_lin_vel_xy_exp: 0.6844
Episode_Reward/track_ang_vel_z_exp: 0.3836
       Episode_Reward/lin_vel_z_l2: -0.0224
      Episode_Reward/ang_vel_xy_l2: -0.0340
     Episode_Reward/dof_torques_l2: -0.1602
         Episode_Reward/dof_acc_l2: -0.0710
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0305
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10899456
                    Iteration time: 1.12s
                      Time elapsed: 00:16:04
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 887/1000 [0m                      

                       Computation: 11642 steps/s (collection: 1.011s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -3.1257
                       Mean reward: 11.98
               Mean episode length: 895.18
Episode_Reward/track_lin_vel_xy_exp: 0.4522
Episode_Reward/track_ang_vel_z_exp: 0.2687
       Episode_Reward/lin_vel_z_l2: -0.0157
      Episode_Reward/ang_vel_xy_l2: -0.0210
     Episode_Reward/dof_torques_l2: -0.1180
         Episode_Reward/dof_acc_l2: -0.0450
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0239
 Episode_Reward/undesired_contacts: -0.0045
Episode_Reward/flat_orientation_l2: -0.0278
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.06s
                      Time elapsed: 00:16:05
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 888/1000 [0m                      

                       Computation: 11144 steps/s (collection: 1.055s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -3.1123
                       Mean reward: 11.72
               Mean episode length: 898.64
Episode_Reward/track_lin_vel_xy_exp: 0.6896
Episode_Reward/track_ang_vel_z_exp: 0.3494
       Episode_Reward/lin_vel_z_l2: -0.0190
      Episode_Reward/ang_vel_xy_l2: -0.0260
     Episode_Reward/dof_torques_l2: -0.1706
         Episode_Reward/dof_acc_l2: -0.0659
     Episode_Reward/action_rate_l2: -0.0196
      Episode_Reward/feet_air_time: -0.0320
 Episode_Reward/undesired_contacts: -0.0216
Episode_Reward/flat_orientation_l2: -0.0216
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10924032
                    Iteration time: 1.10s
                      Time elapsed: 00:16:06
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 889/1000 [0m                      

                       Computation: 10790 steps/s (collection: 1.087s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0107
                 Mean entropy loss: -3.1215
                       Mean reward: 11.84
               Mean episode length: 889.96
Episode_Reward/track_lin_vel_xy_exp: 0.4944
Episode_Reward/track_ang_vel_z_exp: 0.2615
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0176
     Episode_Reward/dof_torques_l2: -0.1150
         Episode_Reward/dof_acc_l2: -0.0406
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0241
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 1.14s
                      Time elapsed: 00:16:07
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 890/1000 [0m                      

                       Computation: 11145 steps/s (collection: 1.041s, learning 0.062s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -3.1268
                       Mean reward: 11.52
               Mean episode length: 889.16
Episode_Reward/track_lin_vel_xy_exp: 0.5177
Episode_Reward/track_ang_vel_z_exp: 0.2640
       Episode_Reward/lin_vel_z_l2: -0.0177
      Episode_Reward/ang_vel_xy_l2: -0.0194
     Episode_Reward/dof_torques_l2: -0.1193
         Episode_Reward/dof_acc_l2: -0.0394
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0242
 Episode_Reward/undesired_contacts: -0.0356
Episode_Reward/flat_orientation_l2: -0.0227
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10948608
                    Iteration time: 1.10s
                      Time elapsed: 00:16:08
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 891/1000 [0m                      

                       Computation: 11347 steps/s (collection: 1.024s, learning 0.059s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.1284
                       Mean reward: 11.79
               Mean episode length: 903.10
Episode_Reward/track_lin_vel_xy_exp: 0.5330
Episode_Reward/track_ang_vel_z_exp: 0.3323
       Episode_Reward/lin_vel_z_l2: -0.0219
      Episode_Reward/ang_vel_xy_l2: -0.0212
     Episode_Reward/dof_torques_l2: -0.1556
         Episode_Reward/dof_acc_l2: -0.0655
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0254
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: -0.0269
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 1.08s
                      Time elapsed: 00:16:09
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 892/1000 [0m                      

                       Computation: 11460 steps/s (collection: 1.025s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -3.1409
                       Mean reward: 11.09
               Mean episode length: 879.66
Episode_Reward/track_lin_vel_xy_exp: 0.5131
Episode_Reward/track_ang_vel_z_exp: 0.3692
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0215
     Episode_Reward/dof_torques_l2: -0.1417
         Episode_Reward/dof_acc_l2: -0.0455
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0256
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0168
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10973184
                    Iteration time: 1.07s
                      Time elapsed: 00:16:10
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 893/1000 [0m                      

                       Computation: 10853 steps/s (collection: 1.070s, learning 0.062s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.1639
                       Mean reward: 11.61
               Mean episode length: 892.49
Episode_Reward/track_lin_vel_xy_exp: 0.7169
Episode_Reward/track_ang_vel_z_exp: 0.3597
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1331
         Episode_Reward/dof_acc_l2: -0.0430
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0328
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0163
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 1.13s
                      Time elapsed: 00:16:11
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 894/1000 [0m                      

                       Computation: 11060 steps/s (collection: 1.068s, learning 0.043s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -3.1766
                       Mean reward: 11.81
               Mean episode length: 905.18
Episode_Reward/track_lin_vel_xy_exp: 0.7443
Episode_Reward/track_ang_vel_z_exp: 0.3235
       Episode_Reward/lin_vel_z_l2: -0.0143
      Episode_Reward/ang_vel_xy_l2: -0.0231
     Episode_Reward/dof_torques_l2: -0.1318
         Episode_Reward/dof_acc_l2: -0.0480
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0244
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0114
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10997760
                    Iteration time: 1.11s
                      Time elapsed: 00:16:13
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 895/1000 [0m                      

                       Computation: 11518 steps/s (collection: 1.018s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -3.1879
                       Mean reward: 11.54
               Mean episode length: 885.07
Episode_Reward/track_lin_vel_xy_exp: 0.5163
Episode_Reward/track_ang_vel_z_exp: 0.2683
       Episode_Reward/lin_vel_z_l2: -0.0186
      Episode_Reward/ang_vel_xy_l2: -0.0180
     Episode_Reward/dof_torques_l2: -0.1167
         Episode_Reward/dof_acc_l2: -0.0429
     Episode_Reward/action_rate_l2: -0.0145
      Episode_Reward/feet_air_time: -0.0230
 Episode_Reward/undesired_contacts: -0.0050
Episode_Reward/flat_orientation_l2: -0.0264
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.07s
                      Time elapsed: 00:16:14
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 896/1000 [0m                      

                       Computation: 10618 steps/s (collection: 1.074s, learning 0.083s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -3.1979
                       Mean reward: 11.96
               Mean episode length: 896.43
Episode_Reward/track_lin_vel_xy_exp: 0.7468
Episode_Reward/track_ang_vel_z_exp: 0.3539
       Episode_Reward/lin_vel_z_l2: -0.0211
      Episode_Reward/ang_vel_xy_l2: -0.0302
     Episode_Reward/dof_torques_l2: -0.1504
         Episode_Reward/dof_acc_l2: -0.0621
     Episode_Reward/action_rate_l2: -0.0182
      Episode_Reward/feet_air_time: -0.0331
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0179
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11022336
                    Iteration time: 1.16s
                      Time elapsed: 00:16:15
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 897/1000 [0m                      

                       Computation: 11216 steps/s (collection: 1.033s, learning 0.063s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -3.1956
                       Mean reward: 11.92
               Mean episode length: 885.00
Episode_Reward/track_lin_vel_xy_exp: 0.7368
Episode_Reward/track_ang_vel_z_exp: 0.3549
       Episode_Reward/lin_vel_z_l2: -0.0169
      Episode_Reward/ang_vel_xy_l2: -0.0225
     Episode_Reward/dof_torques_l2: -0.1285
         Episode_Reward/dof_acc_l2: -0.0480
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0325
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0166
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 1.10s
                      Time elapsed: 00:16:16
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 898/1000 [0m                      

                       Computation: 11394 steps/s (collection: 1.031s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -3.1856
                       Mean reward: 12.56
               Mean episode length: 876.10
Episode_Reward/track_lin_vel_xy_exp: 0.5143
Episode_Reward/track_ang_vel_z_exp: 0.2432
       Episode_Reward/lin_vel_z_l2: -0.0151
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.1032
         Episode_Reward/dof_acc_l2: -0.0480
     Episode_Reward/action_rate_l2: -0.0132
      Episode_Reward/feet_air_time: -0.0248
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0175
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11046912
                    Iteration time: 1.08s
                      Time elapsed: 00:16:17
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 899/1000 [0m                      

                       Computation: 11066 steps/s (collection: 1.048s, learning 0.063s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -3.1889
                       Mean reward: 12.68
               Mean episode length: 879.39
Episode_Reward/track_lin_vel_xy_exp: 0.4493
Episode_Reward/track_ang_vel_z_exp: 0.2431
       Episode_Reward/lin_vel_z_l2: -0.0151
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.1177
         Episode_Reward/dof_acc_l2: -0.0395
     Episode_Reward/action_rate_l2: -0.0146
      Episode_Reward/feet_air_time: -0.0260
 Episode_Reward/undesired_contacts: -0.0255
Episode_Reward/flat_orientation_l2: -0.0235
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 1.11s
                      Time elapsed: 00:16:18
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 900/1000 [0m                      

                       Computation: 10824 steps/s (collection: 1.065s, learning 0.071s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -3.1973
                       Mean reward: 12.33
               Mean episode length: 865.85
Episode_Reward/track_lin_vel_xy_exp: 0.6314
Episode_Reward/track_ang_vel_z_exp: 0.3422
       Episode_Reward/lin_vel_z_l2: -0.0142
      Episode_Reward/ang_vel_xy_l2: -0.0187
     Episode_Reward/dof_torques_l2: -0.1189
         Episode_Reward/dof_acc_l2: -0.0400
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0313
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0134
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11071488
                    Iteration time: 1.14s
                      Time elapsed: 00:16:19
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 901/1000 [0m                      

                       Computation: 10636 steps/s (collection: 1.089s, learning 0.066s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0105
                 Mean entropy loss: -3.1992
                       Mean reward: 12.57
               Mean episode length: 875.88
Episode_Reward/track_lin_vel_xy_exp: 0.6890
Episode_Reward/track_ang_vel_z_exp: 0.3629
       Episode_Reward/lin_vel_z_l2: -0.0244
      Episode_Reward/ang_vel_xy_l2: -0.0267
     Episode_Reward/dof_torques_l2: -0.1507
         Episode_Reward/dof_acc_l2: -0.0622
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0299
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0204
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 1.16s
                      Time elapsed: 00:16:20
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 902/1000 [0m                      

                       Computation: 11097 steps/s (collection: 1.052s, learning 0.055s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.1995
                       Mean reward: 11.78
               Mean episode length: 848.77
Episode_Reward/track_lin_vel_xy_exp: 0.5970
Episode_Reward/track_ang_vel_z_exp: 0.3040
       Episode_Reward/lin_vel_z_l2: -0.0135
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.1044
         Episode_Reward/dof_acc_l2: -0.0332
     Episode_Reward/action_rate_l2: -0.0153
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.0016
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11096064
                    Iteration time: 1.11s
                      Time elapsed: 00:16:21
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 903/1000 [0m                      

                       Computation: 10542 steps/s (collection: 1.096s, learning 0.069s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.2085
                       Mean reward: 11.26
               Mean episode length: 846.20
Episode_Reward/track_lin_vel_xy_exp: 0.4690
Episode_Reward/track_ang_vel_z_exp: 0.2499
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0170
     Episode_Reward/dof_torques_l2: -0.1039
         Episode_Reward/dof_acc_l2: -0.0374
     Episode_Reward/action_rate_l2: -0.0138
      Episode_Reward/feet_air_time: -0.0255
 Episode_Reward/undesired_contacts: -0.0212
Episode_Reward/flat_orientation_l2: -0.0217
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.17s
                      Time elapsed: 00:16:23
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 904/1000 [0m                      

                       Computation: 10947 steps/s (collection: 1.076s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0105
                 Mean entropy loss: -3.2177
                       Mean reward: 10.78
               Mean episode length: 858.32
Episode_Reward/track_lin_vel_xy_exp: 0.4651
Episode_Reward/track_ang_vel_z_exp: 0.2937
       Episode_Reward/lin_vel_z_l2: -0.0187
      Episode_Reward/ang_vel_xy_l2: -0.0238
     Episode_Reward/dof_torques_l2: -0.1358
         Episode_Reward/dof_acc_l2: -0.0477
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0267
 Episode_Reward/undesired_contacts: -0.0074
Episode_Reward/flat_orientation_l2: -0.0154
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11120640
                    Iteration time: 1.12s
                      Time elapsed: 00:16:24
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 905/1000 [0m                      

                       Computation: 11079 steps/s (collection: 1.065s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0109
                 Mean entropy loss: -3.2293
                       Mean reward: 10.44
               Mean episode length: 852.76
Episode_Reward/track_lin_vel_xy_exp: 0.3623
Episode_Reward/track_ang_vel_z_exp: 0.2685
       Episode_Reward/lin_vel_z_l2: -0.0127
      Episode_Reward/ang_vel_xy_l2: -0.0163
     Episode_Reward/dof_torques_l2: -0.1305
         Episode_Reward/dof_acc_l2: -0.0338
     Episode_Reward/action_rate_l2: -0.0170
      Episode_Reward/feet_air_time: -0.0206
 Episode_Reward/undesired_contacts: -0.0184
Episode_Reward/flat_orientation_l2: -0.0164
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 1.11s
                      Time elapsed: 00:16:25
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 906/1000 [0m                      

                       Computation: 11376 steps/s (collection: 1.036s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.2323
                       Mean reward: 10.27
               Mean episode length: 863.80
Episode_Reward/track_lin_vel_xy_exp: 0.6678
Episode_Reward/track_ang_vel_z_exp: 0.3376
       Episode_Reward/lin_vel_z_l2: -0.0202
      Episode_Reward/ang_vel_xy_l2: -0.0256
     Episode_Reward/dof_torques_l2: -0.1457
         Episode_Reward/dof_acc_l2: -0.0465
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0292
 Episode_Reward/undesired_contacts: -0.0064
Episode_Reward/flat_orientation_l2: -0.0120
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11145216
                    Iteration time: 1.08s
                      Time elapsed: 00:16:26
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 907/1000 [0m                      

                       Computation: 11433 steps/s (collection: 1.029s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.2285
                       Mean reward: 9.74
               Mean episode length: 855.42
Episode_Reward/track_lin_vel_xy_exp: 0.5750
Episode_Reward/track_ang_vel_z_exp: 0.2938
       Episode_Reward/lin_vel_z_l2: -0.0234
      Episode_Reward/ang_vel_xy_l2: -0.0254
     Episode_Reward/dof_torques_l2: -0.1261
         Episode_Reward/dof_acc_l2: -0.0736
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0304
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0247
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 1.07s
                      Time elapsed: 00:16:27
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 908/1000 [0m                      

                       Computation: 11203 steps/s (collection: 1.050s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0108
                 Mean entropy loss: -3.2305
                       Mean reward: 10.06
               Mean episode length: 857.49
Episode_Reward/track_lin_vel_xy_exp: 0.3257
Episode_Reward/track_ang_vel_z_exp: 0.2672
       Episode_Reward/lin_vel_z_l2: -0.0209
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.1206
         Episode_Reward/dof_acc_l2: -0.0383
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0267
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: -0.0285
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11169792
                    Iteration time: 1.10s
                      Time elapsed: 00:16:28
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 909/1000 [0m                      

                       Computation: 11217 steps/s (collection: 1.051s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0107
                 Mean entropy loss: -3.2374
                       Mean reward: 9.98
               Mean episode length: 853.80
Episode_Reward/track_lin_vel_xy_exp: 0.4629
Episode_Reward/track_ang_vel_z_exp: 0.2934
       Episode_Reward/lin_vel_z_l2: -0.0265
      Episode_Reward/ang_vel_xy_l2: -0.0265
     Episode_Reward/dof_torques_l2: -0.1235
         Episode_Reward/dof_acc_l2: -0.0512
     Episode_Reward/action_rate_l2: -0.0151
      Episode_Reward/feet_air_time: -0.0241
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0327
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 1.10s
                      Time elapsed: 00:16:29
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 910/1000 [0m                      

                       Computation: 11166 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0108
                 Mean entropy loss: -3.2655
                       Mean reward: 9.74
               Mean episode length: 854.69
Episode_Reward/track_lin_vel_xy_exp: 0.5091
Episode_Reward/track_ang_vel_z_exp: 0.2735
       Episode_Reward/lin_vel_z_l2: -0.0174
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.1150
         Episode_Reward/dof_acc_l2: -0.0441
     Episode_Reward/action_rate_l2: -0.0146
      Episode_Reward/feet_air_time: -0.0238
 Episode_Reward/undesired_contacts: -0.0330
Episode_Reward/flat_orientation_l2: -0.0253
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11194368
                    Iteration time: 1.10s
                      Time elapsed: 00:16:30
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 911/1000 [0m                      

                       Computation: 11011 steps/s (collection: 1.050s, learning 0.066s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -3.2892
                       Mean reward: 10.00
               Mean episode length: 859.42
Episode_Reward/track_lin_vel_xy_exp: 0.4317
Episode_Reward/track_ang_vel_z_exp: 0.2206
       Episode_Reward/lin_vel_z_l2: -0.0176
      Episode_Reward/ang_vel_xy_l2: -0.0189
     Episode_Reward/dof_torques_l2: -0.1210
         Episode_Reward/dof_acc_l2: -0.0426
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0251
 Episode_Reward/undesired_contacts: -0.0294
Episode_Reward/flat_orientation_l2: -0.0187
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.12s
                      Time elapsed: 00:16:31
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 912/1000 [0m                      

                       Computation: 11188 steps/s (collection: 1.048s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0055
                 Mean entropy loss: -3.2941
                       Mean reward: 10.55
               Mean episode length: 854.13
Episode_Reward/track_lin_vel_xy_exp: 0.5668
Episode_Reward/track_ang_vel_z_exp: 0.3036
       Episode_Reward/lin_vel_z_l2: -0.0178
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.1237
         Episode_Reward/dof_acc_l2: -0.0473
     Episode_Reward/action_rate_l2: -0.0155
      Episode_Reward/feet_air_time: -0.0275
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11218944
                    Iteration time: 1.10s
                      Time elapsed: 00:16:33
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 913/1000 [0m                      

                       Computation: 10638 steps/s (collection: 1.106s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.2986
                       Mean reward: 10.33
               Mean episode length: 845.28
Episode_Reward/track_lin_vel_xy_exp: 0.6572
Episode_Reward/track_ang_vel_z_exp: 0.3517
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0253
     Episode_Reward/dof_torques_l2: -0.1637
         Episode_Reward/dof_acc_l2: -0.0728
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0351
 Episode_Reward/undesired_contacts: -0.0375
Episode_Reward/flat_orientation_l2: -0.0164
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 1.16s
                      Time elapsed: 00:16:34
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 914/1000 [0m                      

                       Computation: 11161 steps/s (collection: 1.057s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -3.3042
                       Mean reward: 10.90
               Mean episode length: 873.00
Episode_Reward/track_lin_vel_xy_exp: 0.7581
Episode_Reward/track_ang_vel_z_exp: 0.3323
       Episode_Reward/lin_vel_z_l2: -0.0203
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.1501
         Episode_Reward/dof_acc_l2: -0.0599
     Episode_Reward/action_rate_l2: -0.0194
      Episode_Reward/feet_air_time: -0.0333
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0141
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11243520
                    Iteration time: 1.10s
                      Time elapsed: 00:16:35
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 915/1000 [0m                      

                       Computation: 11145 steps/s (collection: 1.046s, learning 0.057s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -3.3121
                       Mean reward: 10.45
               Mean episode length: 865.86
Episode_Reward/track_lin_vel_xy_exp: 0.5149
Episode_Reward/track_ang_vel_z_exp: 0.3371
       Episode_Reward/lin_vel_z_l2: -0.0213
      Episode_Reward/ang_vel_xy_l2: -0.0277
     Episode_Reward/dof_torques_l2: -0.1479
         Episode_Reward/dof_acc_l2: -0.0669
     Episode_Reward/action_rate_l2: -0.0179
      Episode_Reward/feet_air_time: -0.0286
 Episode_Reward/undesired_contacts: -0.0026
Episode_Reward/flat_orientation_l2: -0.0291
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 1.10s
                      Time elapsed: 00:16:36
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 916/1000 [0m                      

                       Computation: 10783 steps/s (collection: 1.084s, learning 0.056s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -3.3368
                       Mean reward: 10.88
               Mean episode length: 881.21
Episode_Reward/track_lin_vel_xy_exp: 0.4748
Episode_Reward/track_ang_vel_z_exp: 0.3171
       Episode_Reward/lin_vel_z_l2: -0.0145
      Episode_Reward/ang_vel_xy_l2: -0.0171
     Episode_Reward/dof_torques_l2: -0.1230
         Episode_Reward/dof_acc_l2: -0.0354
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0254
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11268096
                    Iteration time: 1.14s
                      Time elapsed: 00:16:37
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 917/1000 [0m                      

                       Computation: 10804 steps/s (collection: 1.077s, learning 0.060s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -3.3376
                       Mean reward: 10.71
               Mean episode length: 875.85
Episode_Reward/track_lin_vel_xy_exp: 0.5974
Episode_Reward/track_ang_vel_z_exp: 0.3158
       Episode_Reward/lin_vel_z_l2: -0.0125
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.1168
         Episode_Reward/dof_acc_l2: -0.0366
     Episode_Reward/action_rate_l2: -0.0160
      Episode_Reward/feet_air_time: -0.0274
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0133
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 1.14s
                      Time elapsed: 00:16:38
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 918/1000 [0m                      

                       Computation: 10136 steps/s (collection: 1.154s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.3378
                       Mean reward: 11.52
               Mean episode length: 886.34
Episode_Reward/track_lin_vel_xy_exp: 0.5557
Episode_Reward/track_ang_vel_z_exp: 0.3207
       Episode_Reward/lin_vel_z_l2: -0.0188
      Episode_Reward/ang_vel_xy_l2: -0.0226
     Episode_Reward/dof_torques_l2: -0.1258
         Episode_Reward/dof_acc_l2: -0.0470
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0287
 Episode_Reward/undesired_contacts: -0.0013
Episode_Reward/flat_orientation_l2: -0.0229
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11292672
                    Iteration time: 1.21s
                      Time elapsed: 00:16:39
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 919/1000 [0m                      

                       Computation: 11142 steps/s (collection: 1.053s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -3.3486
                       Mean reward: 11.59
               Mean episode length: 890.77
Episode_Reward/track_lin_vel_xy_exp: 0.5188
Episode_Reward/track_ang_vel_z_exp: 0.3256
       Episode_Reward/lin_vel_z_l2: -0.0199
      Episode_Reward/ang_vel_xy_l2: -0.0218
     Episode_Reward/dof_torques_l2: -0.1425
         Episode_Reward/dof_acc_l2: -0.0504
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0341
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0141
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.10s
                      Time elapsed: 00:16:40
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 920/1000 [0m                      

                       Computation: 11129 steps/s (collection: 1.059s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0109
                 Mean entropy loss: -3.3545
                       Mean reward: 10.98
               Mean episode length: 872.58
Episode_Reward/track_lin_vel_xy_exp: 0.3618
Episode_Reward/track_ang_vel_z_exp: 0.2277
       Episode_Reward/lin_vel_z_l2: -0.0204
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.1436
         Episode_Reward/dof_acc_l2: -0.0562
     Episode_Reward/action_rate_l2: -0.0154
      Episode_Reward/feet_air_time: -0.0259
 Episode_Reward/undesired_contacts: -0.0262
Episode_Reward/flat_orientation_l2: -0.0270
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11317248
                    Iteration time: 1.10s
                      Time elapsed: 00:16:42
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 921/1000 [0m                      

                       Computation: 11516 steps/s (collection: 1.009s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -3.3669
                       Mean reward: 11.02
               Mean episode length: 865.25
Episode_Reward/track_lin_vel_xy_exp: 0.5401
Episode_Reward/track_ang_vel_z_exp: 0.3303
       Episode_Reward/lin_vel_z_l2: -0.0177
      Episode_Reward/ang_vel_xy_l2: -0.0209
     Episode_Reward/dof_torques_l2: -0.1260
         Episode_Reward/dof_acc_l2: -0.0479
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0283
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0143
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 1.07s
                      Time elapsed: 00:16:43
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 922/1000 [0m                      

                       Computation: 10784 steps/s (collection: 1.091s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -3.3540
                       Mean reward: 11.31
               Mean episode length: 870.70
Episode_Reward/track_lin_vel_xy_exp: 0.6216
Episode_Reward/track_ang_vel_z_exp: 0.2718
       Episode_Reward/lin_vel_z_l2: -0.0134
      Episode_Reward/ang_vel_xy_l2: -0.0176
     Episode_Reward/dof_torques_l2: -0.1035
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0141
      Episode_Reward/feet_air_time: -0.0232
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0170
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11341824
                    Iteration time: 1.14s
                      Time elapsed: 00:16:44
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 923/1000 [0m                      

                       Computation: 10979 steps/s (collection: 1.075s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0079
                 Mean entropy loss: -3.3384
                       Mean reward: 11.43
               Mean episode length: 868.87
Episode_Reward/track_lin_vel_xy_exp: 0.3190
Episode_Reward/track_ang_vel_z_exp: 0.2782
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0162
     Episode_Reward/dof_torques_l2: -0.0972
         Episode_Reward/dof_acc_l2: -0.0340
     Episode_Reward/action_rate_l2: -0.0133
      Episode_Reward/feet_air_time: -0.0174
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0253
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 1.12s
                      Time elapsed: 00:16:45
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 924/1000 [0m                      

                       Computation: 11001 steps/s (collection: 1.067s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.3307
                       Mean reward: 11.29
               Mean episode length: 859.18
Episode_Reward/track_lin_vel_xy_exp: 0.3346
Episode_Reward/track_ang_vel_z_exp: 0.1134
       Episode_Reward/lin_vel_z_l2: -0.0187
      Episode_Reward/ang_vel_xy_l2: -0.0191
     Episode_Reward/dof_torques_l2: -0.0922
         Episode_Reward/dof_acc_l2: -0.0321
     Episode_Reward/action_rate_l2: -0.0108
      Episode_Reward/feet_air_time: -0.0175
 Episode_Reward/undesired_contacts: -0.0051
Episode_Reward/flat_orientation_l2: -0.0374
  Episode_Termination/base_contact: 0.7500
      Episode_Termination/time_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11366400
                    Iteration time: 1.12s
                      Time elapsed: 00:16:46
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 925/1000 [0m                      

                       Computation: 10808 steps/s (collection: 1.087s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0085
                 Mean entropy loss: -3.3263
                       Mean reward: 11.87
               Mean episode length: 880.46
Episode_Reward/track_lin_vel_xy_exp: 0.7421
Episode_Reward/track_ang_vel_z_exp: 0.3364
       Episode_Reward/lin_vel_z_l2: -0.0152
      Episode_Reward/ang_vel_xy_l2: -0.0204
     Episode_Reward/dof_torques_l2: -0.1547
         Episode_Reward/dof_acc_l2: -0.0471
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0128
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 1.14s
                      Time elapsed: 00:16:47
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 926/1000 [0m                      

                       Computation: 11169 steps/s (collection: 1.055s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -3.3186
                       Mean reward: 12.30
               Mean episode length: 883.94
Episode_Reward/track_lin_vel_xy_exp: 0.7692
Episode_Reward/track_ang_vel_z_exp: 0.3488
       Episode_Reward/lin_vel_z_l2: -0.0187
      Episode_Reward/ang_vel_xy_l2: -0.0277
     Episode_Reward/dof_torques_l2: -0.1484
         Episode_Reward/dof_acc_l2: -0.0557
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0355
 Episode_Reward/undesired_contacts: -0.0029
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11390976
                    Iteration time: 1.10s
                      Time elapsed: 00:16:48
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 927/1000 [0m                      

                       Computation: 11013 steps/s (collection: 1.060s, learning 0.056s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.3052
                       Mean reward: 12.26
               Mean episode length: 873.83
Episode_Reward/track_lin_vel_xy_exp: 0.7065
Episode_Reward/track_ang_vel_z_exp: 0.3550
       Episode_Reward/lin_vel_z_l2: -0.0159
      Episode_Reward/ang_vel_xy_l2: -0.0261
     Episode_Reward/dof_torques_l2: -0.1374
         Episode_Reward/dof_acc_l2: -0.0442
     Episode_Reward/action_rate_l2: -0.0165
      Episode_Reward/feet_air_time: -0.0326
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0135
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.12s
                      Time elapsed: 00:16:49
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 928/1000 [0m                      

                       Computation: 10880 steps/s (collection: 1.084s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -3.2998
                       Mean reward: 12.53
               Mean episode length: 870.09
Episode_Reward/track_lin_vel_xy_exp: 0.5878
Episode_Reward/track_ang_vel_z_exp: 0.3011
       Episode_Reward/lin_vel_z_l2: -0.0200
      Episode_Reward/ang_vel_xy_l2: -0.0252
     Episode_Reward/dof_torques_l2: -0.1398
         Episode_Reward/dof_acc_l2: -0.0536
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0298
 Episode_Reward/undesired_contacts: -0.0062
Episode_Reward/flat_orientation_l2: -0.0290
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11415552
                    Iteration time: 1.13s
                      Time elapsed: 00:16:51
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 929/1000 [0m                      

                       Computation: 10969 steps/s (collection: 1.075s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -3.2832
                       Mean reward: 12.77
               Mean episode length: 885.19
Episode_Reward/track_lin_vel_xy_exp: 0.6991
Episode_Reward/track_ang_vel_z_exp: 0.3284
       Episode_Reward/lin_vel_z_l2: -0.0227
      Episode_Reward/ang_vel_xy_l2: -0.0246
     Episode_Reward/dof_torques_l2: -0.1439
         Episode_Reward/dof_acc_l2: -0.0504
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0348
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0144
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 1.12s
                      Time elapsed: 00:16:52
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 930/1000 [0m                      

                       Computation: 11140 steps/s (collection: 1.042s, learning 0.061s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0078
                 Mean entropy loss: -3.2806
                       Mean reward: 12.45
               Mean episode length: 901.27
Episode_Reward/track_lin_vel_xy_exp: 0.7308
Episode_Reward/track_ang_vel_z_exp: 0.3643
       Episode_Reward/lin_vel_z_l2: -0.0176
      Episode_Reward/ang_vel_xy_l2: -0.0257
     Episode_Reward/dof_torques_l2: -0.1397
         Episode_Reward/dof_acc_l2: -0.0602
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0339
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0153
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11440128
                    Iteration time: 1.10s
                      Time elapsed: 00:16:53
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 931/1000 [0m                      

                       Computation: 11081 steps/s (collection: 1.057s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -3.2786
                       Mean reward: 12.95
               Mean episode length: 914.71
Episode_Reward/track_lin_vel_xy_exp: 0.5968
Episode_Reward/track_ang_vel_z_exp: 0.3051
       Episode_Reward/lin_vel_z_l2: -0.0202
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.1315
         Episode_Reward/dof_acc_l2: -0.0464
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0324
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0200
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 1.11s
                      Time elapsed: 00:16:54
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 932/1000 [0m                      

                       Computation: 11774 steps/s (collection: 1.000s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -3.2907
                       Mean reward: 12.81
               Mean episode length: 899.82
Episode_Reward/track_lin_vel_xy_exp: 0.6468
Episode_Reward/track_ang_vel_z_exp: 0.3309
       Episode_Reward/lin_vel_z_l2: -0.0200
      Episode_Reward/ang_vel_xy_l2: -0.0275
     Episode_Reward/dof_torques_l2: -0.1323
         Episode_Reward/dof_acc_l2: -0.0612
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0339
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0242
  Episode_Termination/base_contact: 0.3333
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11464704
                    Iteration time: 1.04s
                      Time elapsed: 00:16:55
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 933/1000 [0m                      

                       Computation: 11210 steps/s (collection: 1.050s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0115
                 Mean entropy loss: -3.2917
                       Mean reward: 12.31
               Mean episode length: 871.19
Episode_Reward/track_lin_vel_xy_exp: 0.5143
Episode_Reward/track_ang_vel_z_exp: 0.2498
       Episode_Reward/lin_vel_z_l2: -0.0161
      Episode_Reward/ang_vel_xy_l2: -0.0175
     Episode_Reward/dof_torques_l2: -0.1081
         Episode_Reward/dof_acc_l2: -0.0463
     Episode_Reward/action_rate_l2: -0.0140
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.0058
Episode_Reward/flat_orientation_l2: -0.0184
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 1.10s
                      Time elapsed: 00:16:56
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 934/1000 [0m                      

                       Computation: 10670 steps/s (collection: 1.104s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0071
               Mean surrogate loss: -0.0113
                 Mean entropy loss: -3.2858
                       Mean reward: 12.40
               Mean episode length: 887.00
Episode_Reward/track_lin_vel_xy_exp: 0.7073
Episode_Reward/track_ang_vel_z_exp: 0.3360
       Episode_Reward/lin_vel_z_l2: -0.0161
      Episode_Reward/ang_vel_xy_l2: -0.0173
     Episode_Reward/dof_torques_l2: -0.1252
         Episode_Reward/dof_acc_l2: -0.0346
     Episode_Reward/action_rate_l2: -0.0162
      Episode_Reward/feet_air_time: -0.0309
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0093
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11489280
                    Iteration time: 1.15s
                      Time elapsed: 00:16:57
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 935/1000 [0m                      

                       Computation: 11264 steps/s (collection: 1.039s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0119
                 Mean entropy loss: -3.2984
                       Mean reward: 12.44
               Mean episode length: 872.78
Episode_Reward/track_lin_vel_xy_exp: 0.5593
Episode_Reward/track_ang_vel_z_exp: 0.2996
       Episode_Reward/lin_vel_z_l2: -0.0212
      Episode_Reward/ang_vel_xy_l2: -0.0246
     Episode_Reward/dof_torques_l2: -0.1244
         Episode_Reward/dof_acc_l2: -0.0502
     Episode_Reward/action_rate_l2: -0.0152
      Episode_Reward/feet_air_time: -0.0298
 Episode_Reward/undesired_contacts: -0.0018
Episode_Reward/flat_orientation_l2: -0.0246
  Episode_Termination/base_contact: 0.6250
      Episode_Termination/time_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.09s
                      Time elapsed: 00:16:58
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 936/1000 [0m                      

                       Computation: 10647 steps/s (collection: 1.101s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.3146
                       Mean reward: 12.31
               Mean episode length: 868.42
Episode_Reward/track_lin_vel_xy_exp: 0.4647
Episode_Reward/track_ang_vel_z_exp: 0.2450
       Episode_Reward/lin_vel_z_l2: -0.0216
      Episode_Reward/ang_vel_xy_l2: -0.0205
     Episode_Reward/dof_torques_l2: -0.1031
         Episode_Reward/dof_acc_l2: -0.0520
     Episode_Reward/action_rate_l2: -0.0141
      Episode_Reward/feet_air_time: -0.0262
 Episode_Reward/undesired_contacts: -0.0034
Episode_Reward/flat_orientation_l2: -0.0251
  Episode_Termination/base_contact: 0.5000
      Episode_Termination/time_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11513856
                    Iteration time: 1.15s
                      Time elapsed: 00:16:59
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 937/1000 [0m                      

                       Computation: 11060 steps/s (collection: 1.063s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0087
                 Mean entropy loss: -3.3042
                       Mean reward: 12.00
               Mean episode length: 836.11
Episode_Reward/track_lin_vel_xy_exp: 0.6597
Episode_Reward/track_ang_vel_z_exp: 0.3062
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0247
     Episode_Reward/dof_torques_l2: -0.1414
         Episode_Reward/dof_acc_l2: -0.0791
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0320
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0283
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 1.11s
                      Time elapsed: 00:17:00
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 938/1000 [0m                      

                       Computation: 10974 steps/s (collection: 1.068s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0107
                 Mean entropy loss: -3.3023
                       Mean reward: 11.69
               Mean episode length: 846.64
Episode_Reward/track_lin_vel_xy_exp: 0.4174
Episode_Reward/track_ang_vel_z_exp: 0.3559
       Episode_Reward/lin_vel_z_l2: -0.0229
      Episode_Reward/ang_vel_xy_l2: -0.0242
     Episode_Reward/dof_torques_l2: -0.1590
         Episode_Reward/dof_acc_l2: -0.0593
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0323
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11538432
                    Iteration time: 1.12s
                      Time elapsed: 00:17:02
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 939/1000 [0m                      

                       Computation: 10982 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0090
                 Mean entropy loss: -3.2930
                       Mean reward: 11.66
               Mean episode length: 864.77
Episode_Reward/track_lin_vel_xy_exp: 0.5106
Episode_Reward/track_ang_vel_z_exp: 0.2883
       Episode_Reward/lin_vel_z_l2: -0.0205
      Episode_Reward/ang_vel_xy_l2: -0.0174
     Episode_Reward/dof_torques_l2: -0.1497
         Episode_Reward/dof_acc_l2: -0.0390
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0236
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0094
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 1.12s
                      Time elapsed: 00:17:03
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 940/1000 [0m                      

                       Computation: 10802 steps/s (collection: 1.094s, learning 0.043s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0110
                 Mean entropy loss: -3.2945
                       Mean reward: 11.28
               Mean episode length: 853.38
Episode_Reward/track_lin_vel_xy_exp: 0.6018
Episode_Reward/track_ang_vel_z_exp: 0.3376
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0215
     Episode_Reward/dof_torques_l2: -0.1209
         Episode_Reward/dof_acc_l2: -0.0475
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0313
 Episode_Reward/undesired_contacts: -0.0032
Episode_Reward/flat_orientation_l2: -0.0184
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11563008
                    Iteration time: 1.14s
                      Time elapsed: 00:17:04
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 941/1000 [0m                      

                       Computation: 11147 steps/s (collection: 1.058s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -3.2906
                       Mean reward: 11.82
               Mean episode length: 862.69
Episode_Reward/track_lin_vel_xy_exp: 0.7716
Episode_Reward/track_ang_vel_z_exp: 0.3677
       Episode_Reward/lin_vel_z_l2: -0.0250
      Episode_Reward/ang_vel_xy_l2: -0.0254
     Episode_Reward/dof_torques_l2: -0.1464
         Episode_Reward/dof_acc_l2: -0.0655
     Episode_Reward/action_rate_l2: -0.0191
      Episode_Reward/feet_air_time: -0.0372
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0144
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 1.10s
                      Time elapsed: 00:17:05
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 942/1000 [0m                      

                       Computation: 11154 steps/s (collection: 1.048s, learning 0.054s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.2765
                       Mean reward: 11.72
               Mean episode length: 878.84
Episode_Reward/track_lin_vel_xy_exp: 0.4994
Episode_Reward/track_ang_vel_z_exp: 0.3322
       Episode_Reward/lin_vel_z_l2: -0.0151
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.1211
         Episode_Reward/dof_acc_l2: -0.0357
     Episode_Reward/action_rate_l2: -0.0159
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0027
Episode_Reward/flat_orientation_l2: -0.0138
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11587584
                    Iteration time: 1.10s
                      Time elapsed: 00:17:06
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 943/1000 [0m                      

                       Computation: 11565 steps/s (collection: 1.014s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0082
                 Mean entropy loss: -3.2745
                       Mean reward: 12.61
               Mean episode length: 909.34
Episode_Reward/track_lin_vel_xy_exp: 0.7745
Episode_Reward/track_ang_vel_z_exp: 0.3459
       Episode_Reward/lin_vel_z_l2: -0.0191
      Episode_Reward/ang_vel_xy_l2: -0.0238
     Episode_Reward/dof_torques_l2: -0.1427
         Episode_Reward/dof_acc_l2: -0.0499
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0358
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0139
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.06s
                      Time elapsed: 00:17:07
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 944/1000 [0m                      

                       Computation: 10775 steps/s (collection: 1.095s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -3.2822
                       Mean reward: 13.36
               Mean episode length: 941.65
Episode_Reward/track_lin_vel_xy_exp: 0.7719
Episode_Reward/track_ang_vel_z_exp: 0.3943
       Episode_Reward/lin_vel_z_l2: -0.0224
      Episode_Reward/ang_vel_xy_l2: -0.0297
     Episode_Reward/dof_torques_l2: -0.1505
         Episode_Reward/dof_acc_l2: -0.0696
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0333
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0210
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11612160
                    Iteration time: 1.14s
                      Time elapsed: 00:17:08
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 945/1000 [0m                      

                       Computation: 10485 steps/s (collection: 1.076s, learning 0.096s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -3.2937
                       Mean reward: 13.85
               Mean episode length: 924.83
Episode_Reward/track_lin_vel_xy_exp: 0.6126
Episode_Reward/track_ang_vel_z_exp: 0.2997
       Episode_Reward/lin_vel_z_l2: -0.0206
      Episode_Reward/ang_vel_xy_l2: -0.0250
     Episode_Reward/dof_torques_l2: -0.1360
         Episode_Reward/dof_acc_l2: -0.0572
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0283
 Episode_Reward/undesired_contacts: -0.0033
Episode_Reward/flat_orientation_l2: -0.0234
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 1.17s
                      Time elapsed: 00:17:09
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 946/1000 [0m                      

                       Computation: 11259 steps/s (collection: 1.046s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -3.3069
                       Mean reward: 14.19
               Mean episode length: 943.60
Episode_Reward/track_lin_vel_xy_exp: 0.6660
Episode_Reward/track_ang_vel_z_exp: 0.3217
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0263
     Episode_Reward/dof_torques_l2: -0.1408
         Episode_Reward/dof_acc_l2: -0.0727
     Episode_Reward/action_rate_l2: -0.0185
      Episode_Reward/feet_air_time: -0.0310
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0254
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11636736
                    Iteration time: 1.09s
                      Time elapsed: 00:17:11
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 947/1000 [0m                      

                       Computation: 10633 steps/s (collection: 1.110s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -3.3222
                       Mean reward: 13.66
               Mean episode length: 929.80
Episode_Reward/track_lin_vel_xy_exp: 0.6703
Episode_Reward/track_ang_vel_z_exp: 0.3028
       Episode_Reward/lin_vel_z_l2: -0.0165
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.1320
         Episode_Reward/dof_acc_l2: -0.0373
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0248
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0148
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 1.16s
                      Time elapsed: 00:17:12
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 948/1000 [0m                      

                       Computation: 11373 steps/s (collection: 1.019s, learning 0.062s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -3.3337
                       Mean reward: 14.12
               Mean episode length: 939.11
Episode_Reward/track_lin_vel_xy_exp: 0.6622
Episode_Reward/track_ang_vel_z_exp: 0.3321
       Episode_Reward/lin_vel_z_l2: -0.0236
      Episode_Reward/ang_vel_xy_l2: -0.0207
     Episode_Reward/dof_torques_l2: -0.1388
         Episode_Reward/dof_acc_l2: -0.0577
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0312
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0202
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11661312
                    Iteration time: 1.08s
                      Time elapsed: 00:17:13
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 949/1000 [0m                      

                       Computation: 11270 steps/s (collection: 1.043s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -3.3392
                       Mean reward: 13.31
               Mean episode length: 922.11
Episode_Reward/track_lin_vel_xy_exp: 0.5819
Episode_Reward/track_ang_vel_z_exp: 0.3270
       Episode_Reward/lin_vel_z_l2: -0.0225
      Episode_Reward/ang_vel_xy_l2: -0.0246
     Episode_Reward/dof_torques_l2: -0.1345
         Episode_Reward/dof_acc_l2: -0.0575
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0314
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: -0.0244
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 1.09s
                      Time elapsed: 00:17:14
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 950/1000 [0m                      

                       Computation: 11718 steps/s (collection: 1.001s, learning 0.047s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.3325
                       Mean reward: 12.57
               Mean episode length: 902.92
Episode_Reward/track_lin_vel_xy_exp: 0.6449
Episode_Reward/track_ang_vel_z_exp: 0.3020
       Episode_Reward/lin_vel_z_l2: -0.0196
      Episode_Reward/ang_vel_xy_l2: -0.0214
     Episode_Reward/dof_torques_l2: -0.1165
         Episode_Reward/dof_acc_l2: -0.0521
     Episode_Reward/action_rate_l2: -0.0165
      Episode_Reward/feet_air_time: -0.0288
 Episode_Reward/undesired_contacts: -0.0047
Episode_Reward/flat_orientation_l2: -0.0164
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11685888
                    Iteration time: 1.05s
                      Time elapsed: 00:17:15
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 951/1000 [0m                      

                       Computation: 11385 steps/s (collection: 1.027s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0099
                 Mean entropy loss: -3.3253
                       Mean reward: 12.78
               Mean episode length: 903.11
Episode_Reward/track_lin_vel_xy_exp: 0.7042
Episode_Reward/track_ang_vel_z_exp: 0.3512
       Episode_Reward/lin_vel_z_l2: -0.0259
      Episode_Reward/ang_vel_xy_l2: -0.0236
     Episode_Reward/dof_torques_l2: -0.1571
         Episode_Reward/dof_acc_l2: -0.0670
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0270
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.08s
                      Time elapsed: 00:17:16
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 952/1000 [0m                      

                       Computation: 11065 steps/s (collection: 1.058s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -3.3283
                       Mean reward: 12.49
               Mean episode length: 887.15
Episode_Reward/track_lin_vel_xy_exp: 0.5660
Episode_Reward/track_ang_vel_z_exp: 0.3340
       Episode_Reward/lin_vel_z_l2: -0.0201
      Episode_Reward/ang_vel_xy_l2: -0.0243
     Episode_Reward/dof_torques_l2: -0.1358
         Episode_Reward/dof_acc_l2: -0.0555
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0331
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0210
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11710464
                    Iteration time: 1.11s
                      Time elapsed: 00:17:17
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 953/1000 [0m                      

                       Computation: 11091 steps/s (collection: 1.058s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -3.3445
                       Mean reward: 11.67
               Mean episode length: 881.98
Episode_Reward/track_lin_vel_xy_exp: 0.5006
Episode_Reward/track_ang_vel_z_exp: 0.2983
       Episode_Reward/lin_vel_z_l2: -0.0203
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.1265
         Episode_Reward/dof_acc_l2: -0.0351
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0231
 Episode_Reward/undesired_contacts: -0.0012
Episode_Reward/flat_orientation_l2: -0.0181
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 1.11s
                      Time elapsed: 00:17:18
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 954/1000 [0m                      

                       Computation: 11310 steps/s (collection: 1.021s, learning 0.066s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.3733
                       Mean reward: 11.68
               Mean episode length: 895.91
Episode_Reward/track_lin_vel_xy_exp: 0.6059
Episode_Reward/track_ang_vel_z_exp: 0.3580
       Episode_Reward/lin_vel_z_l2: -0.0164
      Episode_Reward/ang_vel_xy_l2: -0.0228
     Episode_Reward/dof_torques_l2: -0.1227
         Episode_Reward/dof_acc_l2: -0.0483
     Episode_Reward/action_rate_l2: -0.0174
      Episode_Reward/feet_air_time: -0.0300
 Episode_Reward/undesired_contacts: -0.0010
Episode_Reward/flat_orientation_l2: -0.0156
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11735040
                    Iteration time: 1.09s
                      Time elapsed: 00:17:19
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 955/1000 [0m                      

                       Computation: 10717 steps/s (collection: 1.102s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0107
                 Mean entropy loss: -3.3697
                       Mean reward: 12.14
               Mean episode length: 903.11
Episode_Reward/track_lin_vel_xy_exp: 0.8208
Episode_Reward/track_ang_vel_z_exp: 0.3078
       Episode_Reward/lin_vel_z_l2: -0.0231
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1576
         Episode_Reward/dof_acc_l2: -0.0578
     Episode_Reward/action_rate_l2: -0.0195
      Episode_Reward/feet_air_time: -0.0352
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0184
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 1.15s
                      Time elapsed: 00:17:20
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 956/1000 [0m                      

                       Computation: 10550 steps/s (collection: 1.121s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -3.3783
                       Mean reward: 11.66
               Mean episode length: 891.26
Episode_Reward/track_lin_vel_xy_exp: 0.4029
Episode_Reward/track_ang_vel_z_exp: 0.2461
       Episode_Reward/lin_vel_z_l2: -0.0170
      Episode_Reward/ang_vel_xy_l2: -0.0162
     Episode_Reward/dof_torques_l2: -0.1155
         Episode_Reward/dof_acc_l2: -0.0394
     Episode_Reward/action_rate_l2: -0.0150
      Episode_Reward/feet_air_time: -0.0214
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0220
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11759616
                    Iteration time: 1.16s
                      Time elapsed: 00:17:22
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 957/1000 [0m                      

                       Computation: 10807 steps/s (collection: 1.093s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.3945
                       Mean reward: 11.77
               Mean episode length: 901.31
Episode_Reward/track_lin_vel_xy_exp: 0.5485
Episode_Reward/track_ang_vel_z_exp: 0.2064
       Episode_Reward/lin_vel_z_l2: -0.0148
      Episode_Reward/ang_vel_xy_l2: -0.0181
     Episode_Reward/dof_torques_l2: -0.1203
         Episode_Reward/dof_acc_l2: -0.0350
     Episode_Reward/action_rate_l2: -0.0166
      Episode_Reward/feet_air_time: -0.0244
 Episode_Reward/undesired_contacts: -0.0110
Episode_Reward/flat_orientation_l2: -0.0150
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 1.14s
                      Time elapsed: 00:17:23
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 958/1000 [0m                      

                       Computation: 11060 steps/s (collection: 1.066s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0083
                 Mean entropy loss: -3.3984
                       Mean reward: 11.88
               Mean episode length: 905.41
Episode_Reward/track_lin_vel_xy_exp: 0.5611
Episode_Reward/track_ang_vel_z_exp: 0.3200
       Episode_Reward/lin_vel_z_l2: -0.0257
      Episode_Reward/ang_vel_xy_l2: -0.0208
     Episode_Reward/dof_torques_l2: -0.1237
         Episode_Reward/dof_acc_l2: -0.0664
     Episode_Reward/action_rate_l2: -0.0170
      Episode_Reward/feet_air_time: -0.0247
 Episode_Reward/undesired_contacts: -0.0068
Episode_Reward/flat_orientation_l2: -0.0199
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11784192
                    Iteration time: 1.11s
                      Time elapsed: 00:17:24
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 959/1000 [0m                      

                       Computation: 10901 steps/s (collection: 1.082s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0073
                 Mean entropy loss: -3.3914
                       Mean reward: 12.17
               Mean episode length: 907.53
Episode_Reward/track_lin_vel_xy_exp: 0.7132
Episode_Reward/track_ang_vel_z_exp: 0.3328
       Episode_Reward/lin_vel_z_l2: -0.0293
      Episode_Reward/ang_vel_xy_l2: -0.0300
     Episode_Reward/dof_torques_l2: -0.1431
         Episode_Reward/dof_acc_l2: -0.0688
     Episode_Reward/action_rate_l2: -0.0186
      Episode_Reward/feet_air_time: -0.0335
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0206
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.13s
                      Time elapsed: 00:17:25
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 960/1000 [0m                      

                       Computation: 11081 steps/s (collection: 1.054s, learning 0.055s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0096
                 Mean entropy loss: -3.3817
                       Mean reward: 12.25
               Mean episode length: 909.17
Episode_Reward/track_lin_vel_xy_exp: 0.5765
Episode_Reward/track_ang_vel_z_exp: 0.3199
       Episode_Reward/lin_vel_z_l2: -0.0219
      Episode_Reward/ang_vel_xy_l2: -0.0254
     Episode_Reward/dof_torques_l2: -0.1446
         Episode_Reward/dof_acc_l2: -0.0494
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0291
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: -0.0179
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11808768
                    Iteration time: 1.11s
                      Time elapsed: 00:17:26
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 961/1000 [0m                      

                       Computation: 10449 steps/s (collection: 1.100s, learning 0.076s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.3696
                       Mean reward: 12.55
               Mean episode length: 895.79
Episode_Reward/track_lin_vel_xy_exp: 0.5244
Episode_Reward/track_ang_vel_z_exp: 0.2711
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0199
     Episode_Reward/dof_torques_l2: -0.1164
         Episode_Reward/dof_acc_l2: -0.0437
     Episode_Reward/action_rate_l2: -0.0136
      Episode_Reward/feet_air_time: -0.0278
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0188
  Episode_Termination/base_contact: 0.5417
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 1.18s
                      Time elapsed: 00:17:27
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 962/1000 [0m                      

                       Computation: 10952 steps/s (collection: 1.061s, learning 0.061s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0081
                 Mean entropy loss: -3.3561
                       Mean reward: 12.50
               Mean episode length: 883.58
Episode_Reward/track_lin_vel_xy_exp: 0.7143
Episode_Reward/track_ang_vel_z_exp: 0.3176
       Episode_Reward/lin_vel_z_l2: -0.0186
      Episode_Reward/ang_vel_xy_l2: -0.0220
     Episode_Reward/dof_torques_l2: -0.1346
         Episode_Reward/dof_acc_l2: -0.0495
     Episode_Reward/action_rate_l2: -0.0168
      Episode_Reward/feet_air_time: -0.0350
 Episode_Reward/undesired_contacts: -0.0037
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11833344
                    Iteration time: 1.12s
                      Time elapsed: 00:17:28
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 963/1000 [0m                      

                       Computation: 11433 steps/s (collection: 1.027s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.3389
                       Mean reward: 12.38
               Mean episode length: 883.58
Episode_Reward/track_lin_vel_xy_exp: 0.6109
Episode_Reward/track_ang_vel_z_exp: 0.2995
       Episode_Reward/lin_vel_z_l2: -0.0158
      Episode_Reward/ang_vel_xy_l2: -0.0196
     Episode_Reward/dof_torques_l2: -0.1270
         Episode_Reward/dof_acc_l2: -0.0425
     Episode_Reward/action_rate_l2: -0.0179
      Episode_Reward/feet_air_time: -0.0239
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0113
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 1.07s
                      Time elapsed: 00:17:29
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 964/1000 [0m                      

                       Computation: 11097 steps/s (collection: 1.058s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0107
                 Mean entropy loss: -3.3392
                       Mean reward: 12.29
               Mean episode length: 870.59
Episode_Reward/track_lin_vel_xy_exp: 0.6280
Episode_Reward/track_ang_vel_z_exp: 0.3543
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0233
     Episode_Reward/dof_torques_l2: -0.1381
         Episode_Reward/dof_acc_l2: -0.0632
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0298
 Episode_Reward/undesired_contacts: -0.0008
Episode_Reward/flat_orientation_l2: -0.0287
  Episode_Termination/base_contact: 0.2500
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11857920
                    Iteration time: 1.11s
                      Time elapsed: 00:17:31
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 965/1000 [0m                      

                       Computation: 11009 steps/s (collection: 1.067s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0072
                 Mean entropy loss: -3.3341
                       Mean reward: 12.86
               Mean episode length: 870.35
Episode_Reward/track_lin_vel_xy_exp: 0.7801
Episode_Reward/track_ang_vel_z_exp: 0.4106
       Episode_Reward/lin_vel_z_l2: -0.0162
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.1333
         Episode_Reward/dof_acc_l2: -0.0434
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0249
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0130
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 1.12s
                      Time elapsed: 00:17:32
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 966/1000 [0m                      

                       Computation: 11273 steps/s (collection: 1.044s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -3.3417
                       Mean reward: 12.76
               Mean episode length: 885.86
Episode_Reward/track_lin_vel_xy_exp: 0.4009
Episode_Reward/track_ang_vel_z_exp: 0.3986
       Episode_Reward/lin_vel_z_l2: -0.0207
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.1753
         Episode_Reward/dof_acc_l2: -0.0449
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0266
 Episode_Reward/undesired_contacts: -0.0305
Episode_Reward/flat_orientation_l2: -0.0209
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11882496
                    Iteration time: 1.09s
                      Time elapsed: 00:17:33
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 967/1000 [0m                      

                       Computation: 11004 steps/s (collection: 1.061s, learning 0.056s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0076
                 Mean entropy loss: -3.3542
                       Mean reward: 12.24
               Mean episode length: 890.48
Episode_Reward/track_lin_vel_xy_exp: 0.3821
Episode_Reward/track_ang_vel_z_exp: 0.2120
       Episode_Reward/lin_vel_z_l2: -0.0146
      Episode_Reward/ang_vel_xy_l2: -0.0183
     Episode_Reward/dof_torques_l2: -0.1151
         Episode_Reward/dof_acc_l2: -0.0333
     Episode_Reward/action_rate_l2: -0.0144
      Episode_Reward/feet_air_time: -0.0216
 Episode_Reward/undesired_contacts: -0.0015
Episode_Reward/flat_orientation_l2: -0.0168
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.12s
                      Time elapsed: 00:17:34
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 968/1000 [0m                      

                       Computation: 10729 steps/s (collection: 1.100s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -3.3568
                       Mean reward: 12.44
               Mean episode length: 893.13
Episode_Reward/track_lin_vel_xy_exp: 0.6729
Episode_Reward/track_ang_vel_z_exp: 0.3257
       Episode_Reward/lin_vel_z_l2: -0.0181
      Episode_Reward/ang_vel_xy_l2: -0.0230
     Episode_Reward/dof_torques_l2: -0.1325
         Episode_Reward/dof_acc_l2: -0.0489
     Episode_Reward/action_rate_l2: -0.0172
      Episode_Reward/feet_air_time: -0.0325
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11907072
                    Iteration time: 1.15s
                      Time elapsed: 00:17:35
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 969/1000 [0m                      

                       Computation: 11017 steps/s (collection: 1.038s, learning 0.078s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.3542
                       Mean reward: 12.08
               Mean episode length: 882.61
Episode_Reward/track_lin_vel_xy_exp: 0.6305
Episode_Reward/track_ang_vel_z_exp: 0.3039
       Episode_Reward/lin_vel_z_l2: -0.0207
      Episode_Reward/ang_vel_xy_l2: -0.0324
     Episode_Reward/dof_torques_l2: -0.1682
         Episode_Reward/dof_acc_l2: -0.0671
     Episode_Reward/action_rate_l2: -0.0178
      Episode_Reward/feet_air_time: -0.0332
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0278
  Episode_Termination/base_contact: 0.4583
      Episode_Termination/time_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 1.12s
                      Time elapsed: 00:17:36
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 970/1000 [0m                      

                       Computation: 10756 steps/s (collection: 1.067s, learning 0.076s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0101
                 Mean entropy loss: -3.3543
                       Mean reward: 11.76
               Mean episode length: 889.59
Episode_Reward/track_lin_vel_xy_exp: 0.4874
Episode_Reward/track_ang_vel_z_exp: 0.2342
       Episode_Reward/lin_vel_z_l2: -0.0196
      Episode_Reward/ang_vel_xy_l2: -0.0256
     Episode_Reward/dof_torques_l2: -0.1125
         Episode_Reward/dof_acc_l2: -0.0523
     Episode_Reward/action_rate_l2: -0.0155
      Episode_Reward/feet_air_time: -0.0284
 Episode_Reward/undesired_contacts: -0.0024
Episode_Reward/flat_orientation_l2: -0.0304
  Episode_Termination/base_contact: 0.4167
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11931648
                    Iteration time: 1.14s
                      Time elapsed: 00:17:37
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 971/1000 [0m                      

                       Computation: 11189 steps/s (collection: 1.041s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -3.3666
                       Mean reward: 11.50
               Mean episode length: 879.78
Episode_Reward/track_lin_vel_xy_exp: 0.6740
Episode_Reward/track_ang_vel_z_exp: 0.2992
       Episode_Reward/lin_vel_z_l2: -0.0247
      Episode_Reward/ang_vel_xy_l2: -0.0228
     Episode_Reward/dof_torques_l2: -0.1435
         Episode_Reward/dof_acc_l2: -0.0587
     Episode_Reward/action_rate_l2: -0.0176
      Episode_Reward/feet_air_time: -0.0287
 Episode_Reward/undesired_contacts: -0.0035
Episode_Reward/flat_orientation_l2: -0.0279
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 1.10s
                      Time elapsed: 00:17:38
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 972/1000 [0m                      

                       Computation: 11157 steps/s (collection: 1.055s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.3562
                       Mean reward: 11.93
               Mean episode length: 895.89
Episode_Reward/track_lin_vel_xy_exp: 0.7128
Episode_Reward/track_ang_vel_z_exp: 0.4087
       Episode_Reward/lin_vel_z_l2: -0.0249
      Episode_Reward/ang_vel_xy_l2: -0.0284
     Episode_Reward/dof_torques_l2: -0.1571
         Episode_Reward/dof_acc_l2: -0.0666
     Episode_Reward/action_rate_l2: -0.0199
      Episode_Reward/feet_air_time: -0.0386
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0185
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11956224
                    Iteration time: 1.10s
                      Time elapsed: 00:17:39
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 973/1000 [0m                      

                       Computation: 11297 steps/s (collection: 1.042s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0107
                 Mean entropy loss: -3.3605
                       Mean reward: 11.42
               Mean episode length: 884.61
Episode_Reward/track_lin_vel_xy_exp: 0.5471
Episode_Reward/track_ang_vel_z_exp: 0.2940
       Episode_Reward/lin_vel_z_l2: -0.0136
      Episode_Reward/ang_vel_xy_l2: -0.0167
     Episode_Reward/dof_torques_l2: -0.1110
         Episode_Reward/dof_acc_l2: -0.0289
     Episode_Reward/action_rate_l2: -0.0157
      Episode_Reward/feet_air_time: -0.0255
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0128
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 1.09s
                      Time elapsed: 00:17:41
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 974/1000 [0m                      

                       Computation: 10973 steps/s (collection: 1.074s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0109
                 Mean entropy loss: -3.3756
                       Mean reward: 12.35
               Mean episode length: 884.35
Episode_Reward/track_lin_vel_xy_exp: 0.6882
Episode_Reward/track_ang_vel_z_exp: 0.3005
       Episode_Reward/lin_vel_z_l2: -0.0185
      Episode_Reward/ang_vel_xy_l2: -0.0224
     Episode_Reward/dof_torques_l2: -0.1317
         Episode_Reward/dof_acc_l2: -0.0471
     Episode_Reward/action_rate_l2: -0.0163
      Episode_Reward/feet_air_time: -0.0317
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0152
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11980800
                    Iteration time: 1.12s
                      Time elapsed: 00:17:42
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 975/1000 [0m                      

                       Computation: 11081 steps/s (collection: 1.058s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0092
                 Mean entropy loss: -3.3869
                       Mean reward: 12.64
               Mean episode length: 897.18
Episode_Reward/track_lin_vel_xy_exp: 0.6007
Episode_Reward/track_ang_vel_z_exp: 0.3290
       Episode_Reward/lin_vel_z_l2: -0.0222
      Episode_Reward/ang_vel_xy_l2: -0.0248
     Episode_Reward/dof_torques_l2: -0.1562
         Episode_Reward/dof_acc_l2: -0.0540
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0311
 Episode_Reward/undesired_contacts: -0.0582
Episode_Reward/flat_orientation_l2: -0.0302
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.11s
                      Time elapsed: 00:17:43
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 976/1000 [0m                      

                       Computation: 11081 steps/s (collection: 1.059s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -3.4067
                       Mean reward: 13.16
               Mean episode length: 907.70
Episode_Reward/track_lin_vel_xy_exp: 0.7675
Episode_Reward/track_ang_vel_z_exp: 0.3628
       Episode_Reward/lin_vel_z_l2: -0.0272
      Episode_Reward/ang_vel_xy_l2: -0.0286
     Episode_Reward/dof_torques_l2: -0.1520
         Episode_Reward/dof_acc_l2: -0.0623
     Episode_Reward/action_rate_l2: -0.0202
      Episode_Reward/feet_air_time: -0.0313
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0185
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12005376
                    Iteration time: 1.11s
                      Time elapsed: 00:17:44
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 977/1000 [0m                      

                       Computation: 11171 steps/s (collection: 1.049s, learning 0.051s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -3.4173
                       Mean reward: 13.30
               Mean episode length: 911.31
Episode_Reward/track_lin_vel_xy_exp: 0.5811
Episode_Reward/track_ang_vel_z_exp: 0.2659
       Episode_Reward/lin_vel_z_l2: -0.0225
      Episode_Reward/ang_vel_xy_l2: -0.0218
     Episode_Reward/dof_torques_l2: -0.1419
         Episode_Reward/dof_acc_l2: -0.0565
     Episode_Reward/action_rate_l2: -0.0175
      Episode_Reward/feet_air_time: -0.0300
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0265
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 1.10s
                      Time elapsed: 00:17:45
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 978/1000 [0m                      

                       Computation: 10990 steps/s (collection: 1.072s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0047
               Mean surrogate loss: -0.0086
                 Mean entropy loss: -3.4224
                       Mean reward: 13.95
               Mean episode length: 929.27
Episode_Reward/track_lin_vel_xy_exp: 0.6278
Episode_Reward/track_ang_vel_z_exp: 0.2611
       Episode_Reward/lin_vel_z_l2: -0.0124
      Episode_Reward/ang_vel_xy_l2: -0.0161
     Episode_Reward/dof_torques_l2: -0.1166
         Episode_Reward/dof_acc_l2: -0.0293
     Episode_Reward/action_rate_l2: -0.0144
      Episode_Reward/feet_air_time: -0.0243
 Episode_Reward/undesired_contacts: -0.0053
Episode_Reward/flat_orientation_l2: -0.0201
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12029952
                    Iteration time: 1.12s
                      Time elapsed: 00:17:46
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 979/1000 [0m                      

                       Computation: 11069 steps/s (collection: 1.056s, learning 0.054s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0089
                 Mean entropy loss: -3.4086
                       Mean reward: 13.51
               Mean episode length: 907.24
Episode_Reward/track_lin_vel_xy_exp: 0.5746
Episode_Reward/track_ang_vel_z_exp: 0.2918
       Episode_Reward/lin_vel_z_l2: -0.0153
      Episode_Reward/ang_vel_xy_l2: -0.0206
     Episode_Reward/dof_torques_l2: -0.1338
         Episode_Reward/dof_acc_l2: -0.0431
     Episode_Reward/action_rate_l2: -0.0161
      Episode_Reward/feet_air_time: -0.0252
 Episode_Reward/undesired_contacts: -0.0042
Episode_Reward/flat_orientation_l2: -0.0212
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 1.11s
                      Time elapsed: 00:17:47
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 980/1000 [0m                      

                       Computation: 11429 steps/s (collection: 1.020s, learning 0.055s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0084
                 Mean entropy loss: -3.3919
                       Mean reward: 13.13
               Mean episode length: 895.27
Episode_Reward/track_lin_vel_xy_exp: 0.6092
Episode_Reward/track_ang_vel_z_exp: 0.3149
       Episode_Reward/lin_vel_z_l2: -0.0271
      Episode_Reward/ang_vel_xy_l2: -0.0247
     Episode_Reward/dof_torques_l2: -0.1475
         Episode_Reward/dof_acc_l2: -0.0647
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0279
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0197
  Episode_Termination/base_contact: 0.3750
      Episode_Termination/time_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12054528
                    Iteration time: 1.08s
                      Time elapsed: 00:17:48
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 981/1000 [0m                      

                       Computation: 10591 steps/s (collection: 1.112s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0061
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -3.3998
                       Mean reward: 13.08
               Mean episode length: 898.82
Episode_Reward/track_lin_vel_xy_exp: 0.5851
Episode_Reward/track_ang_vel_z_exp: 0.3470
       Episode_Reward/lin_vel_z_l2: -0.0190
      Episode_Reward/ang_vel_xy_l2: -0.0219
     Episode_Reward/dof_torques_l2: -0.1446
         Episode_Reward/dof_acc_l2: -0.0433
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0321
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0131
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 1.16s
                      Time elapsed: 00:17:49
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 982/1000 [0m                      

                       Computation: 11380 steps/s (collection: 1.027s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.3883
                       Mean reward: 12.75
               Mean episode length: 888.38
Episode_Reward/track_lin_vel_xy_exp: 0.5571
Episode_Reward/track_ang_vel_z_exp: 0.3022
       Episode_Reward/lin_vel_z_l2: -0.0241
      Episode_Reward/ang_vel_xy_l2: -0.0211
     Episode_Reward/dof_torques_l2: -0.1318
         Episode_Reward/dof_acc_l2: -0.0495
     Episode_Reward/action_rate_l2: -0.0156
      Episode_Reward/feet_air_time: -0.0282
 Episode_Reward/undesired_contacts: -0.0003
Episode_Reward/flat_orientation_l2: -0.0196
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12079104
                    Iteration time: 1.08s
                      Time elapsed: 00:17:51
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 983/1000 [0m                      

                       Computation: 11036 steps/s (collection: 1.061s, learning 0.052s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0103
                 Mean entropy loss: -3.3817
                       Mean reward: 13.15
               Mean episode length: 876.73
Episode_Reward/track_lin_vel_xy_exp: 0.7905
Episode_Reward/track_ang_vel_z_exp: 0.3637
       Episode_Reward/lin_vel_z_l2: -0.0245
      Episode_Reward/ang_vel_xy_l2: -0.0235
     Episode_Reward/dof_torques_l2: -0.1389
         Episode_Reward/dof_acc_l2: -0.0605
     Episode_Reward/action_rate_l2: -0.0184
      Episode_Reward/feet_air_time: -0.0351
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0189
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.11s
                      Time elapsed: 00:17:52
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 984/1000 [0m                      

                       Computation: 11169 steps/s (collection: 1.051s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.4016
                       Mean reward: 13.05
               Mean episode length: 876.73
Episode_Reward/track_lin_vel_xy_exp: 0.7894
Episode_Reward/track_ang_vel_z_exp: 0.3575
       Episode_Reward/lin_vel_z_l2: -0.0218
      Episode_Reward/ang_vel_xy_l2: -0.0277
     Episode_Reward/dof_torques_l2: -0.1549
         Episode_Reward/dof_acc_l2: -0.0521
     Episode_Reward/action_rate_l2: -0.0188
      Episode_Reward/feet_air_time: -0.0335
 Episode_Reward/undesired_contacts: -0.0001
Episode_Reward/flat_orientation_l2: -0.0123
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12103680
                    Iteration time: 1.10s
                      Time elapsed: 00:17:53
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 985/1000 [0m                      

                       Computation: 11127 steps/s (collection: 1.051s, learning 0.053s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0094
                 Mean entropy loss: -3.3826
                       Mean reward: 13.03
               Mean episode length: 882.91
Episode_Reward/track_lin_vel_xy_exp: 0.5653
Episode_Reward/track_ang_vel_z_exp: 0.2596
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0213
     Episode_Reward/dof_torques_l2: -0.1339
         Episode_Reward/dof_acc_l2: -0.0487
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0293
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0173
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 1.10s
                      Time elapsed: 00:17:54
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 986/1000 [0m                      

                       Computation: 11289 steps/s (collection: 1.039s, learning 0.049s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0104
                 Mean entropy loss: -3.3604
                       Mean reward: 12.96
               Mean episode length: 894.85
Episode_Reward/track_lin_vel_xy_exp: 0.5978
Episode_Reward/track_ang_vel_z_exp: 0.3438
       Episode_Reward/lin_vel_z_l2: -0.0219
      Episode_Reward/ang_vel_xy_l2: -0.0243
     Episode_Reward/dof_torques_l2: -0.1383
         Episode_Reward/dof_acc_l2: -0.0652
     Episode_Reward/action_rate_l2: -0.0181
      Episode_Reward/feet_air_time: -0.0322
 Episode_Reward/undesired_contacts: -0.0006
Episode_Reward/flat_orientation_l2: -0.0180
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12128256
                    Iteration time: 1.09s
                      Time elapsed: 00:17:55
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 987/1000 [0m                      

                       Computation: 10907 steps/s (collection: 1.081s, learning 0.046s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0034
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -3.3401
                       Mean reward: 13.73
               Mean episode length: 931.66
Episode_Reward/track_lin_vel_xy_exp: 0.6828
Episode_Reward/track_ang_vel_z_exp: 0.3703
       Episode_Reward/lin_vel_z_l2: -0.0194
      Episode_Reward/ang_vel_xy_l2: -0.0237
     Episode_Reward/dof_torques_l2: -0.1380
         Episode_Reward/dof_acc_l2: -0.0555
     Episode_Reward/action_rate_l2: -0.0183
      Episode_Reward/feet_air_time: -0.0330
 Episode_Reward/undesired_contacts: -0.0007
Episode_Reward/flat_orientation_l2: -0.0146
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 1.13s
                      Time elapsed: 00:17:56
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 988/1000 [0m                      

                       Computation: 11263 steps/s (collection: 1.046s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0093
                 Mean entropy loss: -3.3346
                       Mean reward: 14.43
               Mean episode length: 944.15
Episode_Reward/track_lin_vel_xy_exp: 0.7526
Episode_Reward/track_ang_vel_z_exp: 0.3879
       Episode_Reward/lin_vel_z_l2: -0.0250
      Episode_Reward/ang_vel_xy_l2: -0.0252
     Episode_Reward/dof_torques_l2: -0.1506
         Episode_Reward/dof_acc_l2: -0.0775
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0380
 Episode_Reward/undesired_contacts: -0.0000
Episode_Reward/flat_orientation_l2: -0.0155
  Episode_Termination/base_contact: 0.0417
      Episode_Termination/time_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12152832
                    Iteration time: 1.09s
                      Time elapsed: 00:17:57
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 989/1000 [0m                      

                       Computation: 11384 steps/s (collection: 1.035s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0112
                 Mean entropy loss: -3.3255
                       Mean reward: 14.46
               Mean episode length: 953.88
Episode_Reward/track_lin_vel_xy_exp: 0.7579
Episode_Reward/track_ang_vel_z_exp: 0.3992
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0259
     Episode_Reward/dof_torques_l2: -0.1773
         Episode_Reward/dof_acc_l2: -0.0821
     Episode_Reward/action_rate_l2: -0.0206
      Episode_Reward/feet_air_time: -0.0356
 Episode_Reward/undesired_contacts: -0.0078
Episode_Reward/flat_orientation_l2: -0.0238
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 1.08s
                      Time elapsed: 00:17:58
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 990/1000 [0m                      

                       Computation: 10435 steps/s (collection: 1.123s, learning 0.054s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0091
                 Mean entropy loss: -3.3260
                       Mean reward: 14.37
               Mean episode length: 938.16
Episode_Reward/track_lin_vel_xy_exp: 0.7172
Episode_Reward/track_ang_vel_z_exp: 0.3670
       Episode_Reward/lin_vel_z_l2: -0.0227
      Episode_Reward/ang_vel_xy_l2: -0.0218
     Episode_Reward/dof_torques_l2: -0.1429
         Episode_Reward/dof_acc_l2: -0.0492
     Episode_Reward/action_rate_l2: -0.0177
      Episode_Reward/feet_air_time: -0.0321
 Episode_Reward/undesired_contacts: -0.0021
Episode_Reward/flat_orientation_l2: -0.0170
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12177408
                    Iteration time: 1.18s
                      Time elapsed: 00:17:59
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 991/1000 [0m                      

                       Computation: 10748 steps/s (collection: 1.095s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0108
                 Mean entropy loss: -3.3256
                       Mean reward: 14.26
               Mean episode length: 927.01
Episode_Reward/track_lin_vel_xy_exp: 0.7216
Episode_Reward/track_ang_vel_z_exp: 0.3612
       Episode_Reward/lin_vel_z_l2: -0.0237
      Episode_Reward/ang_vel_xy_l2: -0.0324
     Episode_Reward/dof_torques_l2: -0.1521
         Episode_Reward/dof_acc_l2: -0.0753
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0390
 Episode_Reward/undesired_contacts: -0.0002
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.0833
      Episode_Termination/time_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.14s
                      Time elapsed: 00:18:01
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 992/1000 [0m                      

                       Computation: 11100 steps/s (collection: 1.059s, learning 0.048s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0106
                 Mean entropy loss: -3.3251
                       Mean reward: 14.57
               Mean episode length: 934.20
Episode_Reward/track_lin_vel_xy_exp: 0.6270
Episode_Reward/track_ang_vel_z_exp: 0.3658
       Episode_Reward/lin_vel_z_l2: -0.0182
      Episode_Reward/ang_vel_xy_l2: -0.0198
     Episode_Reward/dof_torques_l2: -0.1558
         Episode_Reward/dof_acc_l2: -0.0471
     Episode_Reward/action_rate_l2: -0.0171
      Episode_Reward/feet_air_time: -0.0279
 Episode_Reward/undesired_contacts: -0.0207
Episode_Reward/flat_orientation_l2: -0.0198
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12201984
                    Iteration time: 1.11s
                      Time elapsed: 00:18:02
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 993/1000 [0m                      

                       Computation: 11526 steps/s (collection: 1.016s, learning 0.050s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0097
                 Mean entropy loss: -3.3425
                       Mean reward: 14.80
               Mean episode length: 943.59
Episode_Reward/track_lin_vel_xy_exp: 0.7912
Episode_Reward/track_ang_vel_z_exp: 0.3585
       Episode_Reward/lin_vel_z_l2: -0.0270
      Episode_Reward/ang_vel_xy_l2: -0.0294
     Episode_Reward/dof_torques_l2: -0.1711
         Episode_Reward/dof_acc_l2: -0.0721
     Episode_Reward/action_rate_l2: -0.0202
      Episode_Reward/feet_air_time: -0.0376
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0203
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 1.07s
                      Time elapsed: 00:18:03
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 994/1000 [0m                      

                       Computation: 11002 steps/s (collection: 1.039s, learning 0.077s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0095
                 Mean entropy loss: -3.3601
                       Mean reward: 14.72
               Mean episode length: 949.19
Episode_Reward/track_lin_vel_xy_exp: 0.6998
Episode_Reward/track_ang_vel_z_exp: 0.3213
       Episode_Reward/lin_vel_z_l2: -0.0171
      Episode_Reward/ang_vel_xy_l2: -0.0217
     Episode_Reward/dof_torques_l2: -0.1279
         Episode_Reward/dof_acc_l2: -0.0455
     Episode_Reward/action_rate_l2: -0.0180
      Episode_Reward/feet_air_time: -0.0273
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0147
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12226560
                    Iteration time: 1.12s
                      Time elapsed: 00:18:04
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 995/1000 [0m                      

                       Computation: 11048 steps/s (collection: 1.054s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0114
                 Mean entropy loss: -3.3711
                       Mean reward: 14.27
               Mean episode length: 926.83
Episode_Reward/track_lin_vel_xy_exp: 0.6065
Episode_Reward/track_ang_vel_z_exp: 0.3400
       Episode_Reward/lin_vel_z_l2: -0.0188
      Episode_Reward/ang_vel_xy_l2: -0.0220
     Episode_Reward/dof_torques_l2: -0.1443
         Episode_Reward/dof_acc_l2: -0.0469
     Episode_Reward/action_rate_l2: -0.0170
      Episode_Reward/feet_air_time: -0.0301
 Episode_Reward/undesired_contacts: -0.0004
Episode_Reward/flat_orientation_l2: -0.0144
  Episode_Termination/base_contact: 0.2917
      Episode_Termination/time_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 1.11s
                      Time elapsed: 00:18:05
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 996/1000 [0m                      

                       Computation: 11076 steps/s (collection: 1.054s, learning 0.056s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0100
                 Mean entropy loss: -3.3657
                       Mean reward: 13.82
               Mean episode length: 923.40
Episode_Reward/track_lin_vel_xy_exp: 0.7514
Episode_Reward/track_ang_vel_z_exp: 0.3649
       Episode_Reward/lin_vel_z_l2: -0.0248
      Episode_Reward/ang_vel_xy_l2: -0.0281
     Episode_Reward/dof_torques_l2: -0.1492
         Episode_Reward/dof_acc_l2: -0.0666
     Episode_Reward/action_rate_l2: -0.0189
      Episode_Reward/feet_air_time: -0.0385
 Episode_Reward/undesired_contacts: -0.0152
Episode_Reward/flat_orientation_l2: -0.0234
  Episode_Termination/base_contact: 0.1250
      Episode_Termination/time_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12251136
                    Iteration time: 1.11s
                      Time elapsed: 00:18:06
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 997/1000 [0m                      

                       Computation: 10882 steps/s (collection: 1.072s, learning 0.058s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0088
                 Mean entropy loss: -3.3541
                       Mean reward: 12.99
               Mean episode length: 925.22
Episode_Reward/track_lin_vel_xy_exp: 0.4376
Episode_Reward/track_ang_vel_z_exp: 0.2381
       Episode_Reward/lin_vel_z_l2: -0.0120
      Episode_Reward/ang_vel_xy_l2: -0.0201
     Episode_Reward/dof_torques_l2: -0.1156
         Episode_Reward/dof_acc_l2: -0.0412
     Episode_Reward/action_rate_l2: -0.0149
      Episode_Reward/feet_air_time: -0.0272
 Episode_Reward/undesired_contacts: -0.0005
Episode_Reward/flat_orientation_l2: -0.0098
  Episode_Termination/base_contact: 0.2083
      Episode_Termination/time_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 1.13s
                      Time elapsed: 00:18:07
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 998/1000 [0m                      

                       Computation: 10942 steps/s (collection: 1.078s, learning 0.045s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0098
                 Mean entropy loss: -3.3627
                       Mean reward: 13.20
               Mean episode length: 925.22
Episode_Reward/track_lin_vel_xy_exp: 0.8207
Episode_Reward/track_ang_vel_z_exp: 0.3896
       Episode_Reward/lin_vel_z_l2: -0.0215
      Episode_Reward/ang_vel_xy_l2: -0.0247
     Episode_Reward/dof_torques_l2: -0.1511
         Episode_Reward/dof_acc_l2: -0.0668
     Episode_Reward/action_rate_l2: -0.0200
      Episode_Reward/feet_air_time: -0.0365
 Episode_Reward/undesired_contacts: 0.0000
Episode_Reward/flat_orientation_l2: -0.0183
  Episode_Termination/base_contact: 0.0000
      Episode_Termination/time_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12275712
                    Iteration time: 1.12s
                      Time elapsed: 00:18:08
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 999/1000 [0m                      

                       Computation: 11029 steps/s (collection: 1.070s, learning 0.044s)
             Mean action noise std: 0.19
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0102
                 Mean entropy loss: -3.3670
                       Mean reward: 12.91
               Mean episode length: 907.38
Episode_Reward/track_lin_vel_xy_exp: 0.6589
Episode_Reward/track_ang_vel_z_exp: 0.2774
       Episode_Reward/lin_vel_z_l2: -0.0187
      Episode_Reward/ang_vel_xy_l2: -0.0185
     Episode_Reward/dof_torques_l2: -0.1400
         Episode_Reward/dof_acc_l2: -0.0456
     Episode_Reward/action_rate_l2: -0.0169
      Episode_Reward/feet_air_time: -0.0330
 Episode_Reward/undesired_contacts: -0.0030
Episode_Reward/flat_orientation_l2: -0.0151
  Episode_Termination/base_contact: 0.1667
      Episode_Termination/time_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.11s
                      Time elapsed: 00:18:09
                               ETA: 00:00:01

[13.285s] Simulation App Startup Complete
[14.630s] [ext: omni.physx.fabric-107.3.26] startup
2025-12-05T08:21:20Z [20,326ms] [Warning] [omni.hydra] Mesh '/__Prototype_5758510811026756779/mesh_0' has corrupted data in primvar 'st': buffer size 702 doesn't match expected size 12828 in faceVarying primvars
2025-12-05T08:21:24Z [24,165ms] [Warning] [gpu.foundation.plugin] Invalid sync scope for buffer resource 'shared swapchain buffer'. Create resource with valid sync scope for lifetime tracking or use kResourceUsageFlagNoSyncScope.
2025-12-05T08:21:24Z [24,165ms] [Warning] [gpu.foundation.plugin] Invalid sync scope for buffer resource 'shared swapchain buffer'. Create resource with valid sync scope for lifetime tracking or use kResourceUsageFlagNoSyncScope.
2025-12-05T08:21:24Z [24,165ms] [Warning] [gpu.foundation.plugin] Invalid sync scope for buffer resource 'shared swapchain buffer'. Create resource with valid sync scope for lifetime tracking or use kResourceUsageFlagNoSyncScope.
2025-12-05T08:21:25Z [25,483ms] [Warning] [carb] Client gpu.foundation.plugin has acquired [gpu::unstable::IMemoryBudgetManagerFactory v0.1] 100 times. Consider accessing this interface with carb::getCachedInterface() (Performance warning)
2025-12-05T08:39:36Z [1,116,200ms] [Warning] [carb.windowing-glfw.plugin] GLFW initialization failed.
2025-12-05T08:39:36Z [1,116,200ms] [Warning] [carb] Failed to startup plugin carb.windowing-glfw.plugin (interfaces: [carb::windowing::IGLContext v1.0],[carb::windowing::IWindowing v1.5]) (impl: carb.windowing-glfw.plugin)
[1117.517s] Simulation App Shutting Down
